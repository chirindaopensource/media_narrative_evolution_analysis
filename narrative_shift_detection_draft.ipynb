{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0l7C-ir45z4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README.md\n",
        "\n",
        "# Narrative Shift Detection: A Hybrid DTM-LLM Approach\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/downloads/)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n",
        "[![Linting: flake8](https://img.shields.io/badge/linting-flake8-yellowgreen)](https://flake8.pycqa.org/)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type_checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%230C55A5.svg?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![scikit-learn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=flat&logo=scikit-learn&logoColor=white)](https://scikit-learn.org/)\n",
        "[![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=flat&logo=Matplotlib&logoColor=black)](https://matplotlib.org/)\n",
        "[![spaCy](https://img.shields.io/badge/spaCy-09A3D5?style=flat&logo=spacy&logoColor=white)](https://spacy.io/)\n",
        "[![Gensim](https://img.shields.io/badge/Gensim-FF6B35?style=flat&logoColor=white)](https://radimrehurek.com/gensim/)\n",
        "[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=flat&logo=PyTorch&logoColor=white)](https://pytorch.org/)\n",
        "[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-FFD21E?style=flat&logoColor=black)](https://huggingface.co/)\n",
        "[![Transformers](https://img.shields.io/badge/Transformers-FF6F00?style=flat&logoColor=white)](https://huggingface.co/transformers/)\n",
        "[![CUDA](https://img.shields.io/badge/CUDA-76B900?style=flat&logo=nvidia&logoColor=white)](https://developer.nvidia.com/cuda-toolkit)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "[![pytest](https://img.shields.io/badge/pytest-0A9EDC?style=flat&logo=pytest&logoColor=white)](https://pytest.org/)\n",
        "[![JSON Schema](https://img.shields.io/badge/JSON%20Schema-000000?style=flat&logo=json&logoColor=white)](https://json-schema.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-b31b1b?style=flat&logo=arxiv&logoColor=white)](https://arxiv.org/)\n",
        "[![DOI](https://img.shields.io/badge/DOI-10.000%2F000000-blue)](https://doi.org/)\n",
        "[![Research](https://img.shields.io/badge/Research-Computational%20Social%20Science-green)](https://github.com/)\n",
        "[![Methodology](https://img.shields.io/badge/Methodology-Hybrid%20DTM--LLM-orange)](https://github.com/)\n",
        "[![Memory Profiling](https://img.shields.io/badge/Memory-Profiling%20Enabled-red)](https://pypi.org/project/memory-profiler/)\n",
        "[![GPU Accelerated](https://img.shields.io/badge/GPU-Accelerated-76B900)](https://developer.nvidia.com/cuda-toolkit)\n",
        "[![Quantization](https://img.shields.io/badge/Model-Quantization%20Support-purple)](https://github.com/TimDettmers/bitsandbytes)\n",
        "[![Text Processing](https://img.shields.io/badge/Text-Processing-blue)](https://spacy.io/)\n",
        "[![Topic Modeling](https://img.shields.io/badge/Topic-Modeling-orange)](https://radimrehurek.com/gensim/)\n",
        "[![Change Detection](https://img.shields.io/badge/Change%20Point-Detection-red)](https://scipy.org/)\n",
        "[![Statistical Analysis](https://img.shields.io/badge/Statistical-Analysis-green)](https://scipy.org/)\n",
        "[![Bootstrap Methods](https://img.shields.io/badge/Bootstrap-Resampling-yellow)](https://scipy.org/)\n",
        "\n",
        "\n",
        "**Repository:** https://github.com/chirindaopensource/media_narrative_evolution_analysis\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "\n",
        "\n",
        "This repository contains an **independent** implementation of the research methodology from a 2025 paper which is entitled **\"Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and Large Language Models\"** by:\n",
        "\n",
        "* Kai-Robin Lange: Department of Statistics, TU Dortmund University, 44221 Dortmund, Germany.\n",
        "* Tobias Schmidt: Institute of Journalism, TU Dortmund University, 44221 Dortmund, Germany.\n",
        "* Matthias Reccius: Faculty of Management and Economics, Ruhr University Bochum, 44780 Bochum, Germany.\n",
        "* Henrik MÃ¼ller: Institute of Journalism, TU Dortmund University, 44221 Dortmund, Germany.\n",
        "* Michael Roos: Faculty of Management and Economics, Ruhr University Bochum, 44780 Bochum, Germany.\n",
        "* Carsten Jentsch: Department of Statistics, TU Dortmund University, 44221 Dortmund, Germany.\n",
        "\n",
        "The project provides a robust, end-to-end Python pipeline for identifying, analyzing, and understanding the evolution of narratives within large-scale longitudinal text corpora.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: run_narrative_shift_detection_pipeline](#key-callable-run_narrative_shift_detection_pipeline)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and Large Language Models.\" The core of this repository is the iPython Notebook `narrative_shift_detection_draft.ipynb`, which contains a comprehensive suite of functions to analyze narrative evolution in longitudinal text corpora.\n",
        "\n",
        "Analyzing how media narratives evolve over time is a critical task in computational social science, finance, and political economy. Traditional quantitative methods like topic modeling are scalable but often lack the semantic depth to understand complex narrative structures. Conversely, Large Language Models (LLMs) possess sophisticated language understanding but are computationally prohibitive to apply across entire large-scale corpora for continuous monitoring.\n",
        "\n",
        "This framework enables researchers to:\n",
        "\n",
        "- Detect significant narrative shifts in large text corpora\n",
        "- Identify the temporal moments when narratives change\n",
        "- Classify changes as \"content shifts\" or \"narrative shifts\" using the Narrative Policy Framework (NPF)\n",
        "- Provide computational efficiency through hybrid DTM-LLM architecture\n",
        "\n",
        "This codebase is intended for researchers and students in computational social science, journalism, political science, and related fields who require robust tools for quantitative narrative analysis.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in the theoretical constructs combining Dynamic Topic Models (DTMs) and Large Language Models (LLMs):\n",
        "\n",
        "**Dynamic Topic Modeling:** Utilizes Latent Dirichlet Allocation (LDA) with temporal coherence to track topic evolution over time. The pipeline implements:\n",
        "- Stable Topic Initialization to mitigate LDA stochasticity\n",
        "- Rolling Window LDA for temporal topic tracking\n",
        "- Statistical change point detection using bootstrap methods\n",
        "\n",
        "**Narrative Policy Framework (NPF):** Provides the theoretical foundation for classifying narrative changes into:\n",
        "- Content Shifts: Changes in topic focus or emphasis without fundamental narrative restructuring\n",
        "- Narrative Shifts: Deeper changes in how stories are framed, including character roles, plot structure, and moral positioning\n",
        "\n",
        "**Hybrid Architecture:** Combines the scalability of DTMs for corpus-wide analysis with the semantic depth of LLMs for targeted narrative interpretation, achieving both computational efficiency and analytical sophistication.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`narrative_shift_detection_draft.ipynb`) implements a full pipeline for narrative shift detection, including:\n",
        "\n",
        "- **Input Validation:** Rigorous checks for input data schema, parameter types, and value ranges\n",
        "- **Text Preprocessing:** Advanced text cleaning, tokenization, and lemmatization using spaCy\n",
        "- **Stable Topic Initialization:** LDAPrototype algorithm for consistent topic model initialization\n",
        "- **Dynamic Topic Evolution:** RollingLDA implementation for temporal topic tracking\n",
        "- **Statistical Change Point Detection:** Bootstrap-based hypothesis testing for significant topic shifts\n",
        "- **Document Filtering:** Intelligent selection of relevant documents for LLM analysis\n",
        "- **LLM-based Narrative Analysis:** Structured prompting for narrative shift classification\n",
        "- **Performance Evaluation:** Comprehensive metrics comparing LLM classifications to human annotations\n",
        "- **Visualization Suite:** Time series plots, topic evolution charts, and performance summaries\n",
        "- **Comprehensive Reporting:** Detailed documentation of pipeline runs for reproducibility\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the hybrid DTM-LLM methodology:\n",
        "\n",
        "1. **Stable Topic Initialization**: Mitigates LDA stochasticity by training multiple models on a warm-up corpus and selecting the most stable representative model as a prototype.\n",
        "\n",
        "2. **Dynamic Topic Evolution**: Models topic evolution using a rolling window approach where topic-word distributions from previous time steps inform current models, ensuring temporal coherence while allowing adaptation.\n",
        "\n",
        "3. **Statistical Change Point Detection**: Applies bootstrap-based hypothesis testing to time series of topic-word distributions, detecting statistically significant abrupt shifts and identifying key words driving changes.\n",
        "\n",
        "4. **Document Filtering**: Selects the most relevant documents for each detected change point based on topic relevance scores and temporal proximity.\n",
        "\n",
        "5. **LLM-based Narrative Analysis**: Uses carefully engineered prompts to instruct an LLM (Llama 3.1 8B) to analyze changes, classify them according to NPF, and provide structured explanations in JSON format.\n",
        "\n",
        "6. **Performance Evaluation**: Compares LLM classifications against human annotations using standard metrics (accuracy, precision, recall, F1-score).\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `narrative_shift_detection_draft.ipynb` notebook is structured as a logical pipeline with modular functions:\n",
        "\n",
        "**Input Processing and Validation:**\n",
        "- `validate_input_parameters`: Ensures all pipeline inputs are correctly structured\n",
        "- `cleanse_news_data`: Handles missing values and cleans raw text corpus\n",
        "- `preprocess_text_data`: Performs tokenization, lemmatization, and creates bag-of-words representation\n",
        "- `chunk_data_by_time`: Partitions corpus into discrete time chunks\n",
        "\n",
        "**Topic Modeling Pipeline:**\n",
        "- `train_lda_prototype`: Implements LDAPrototype algorithm for stable base topic model\n",
        "- `apply_rolling_lda`: Executes RollingLDA model for topic evolution tracking\n",
        "- `detect_topical_changes`: Implements bootstrap-based statistical test for change point detection\n",
        "\n",
        "**LLM Analysis Pipeline:**\n",
        "- `filter_documents_for_llm`: Selects most relevant documents for change point analysis\n",
        "- `setup_llm_model_and_tokenizer`: Loads and configures specified LLM and tokenizer\n",
        "- `construct_llm_prompt_for_narrative_analysis`: Engineers detailed structured prompts\n",
        "- `perform_llm_analysis_on_change_point`: Manages LLM inference and JSON output parsing\n",
        "\n",
        "**Evaluation and Reporting:**\n",
        "- `evaluate_llm_classification_performance`: Calculates performance metrics against human labels\n",
        "- `compile_analysis_results`: Aggregates all system, LLM, and human data\n",
        "- `plot_topic_evolution_and_changes`: Generates visualizations\n",
        "- `generate_pipeline_run_documentation`: Creates detailed run reports\n",
        "\n",
        "**Main Orchestrator:**\n",
        "- `run_narrative_shift_detection_pipeline`: Executes the entire pipeline in sequence\n",
        "\n",
        "## Key Callable: run_narrative_shift_detection_pipeline\n",
        "\n",
        "The central function in this project is `run_narrative_shift_detection_pipeline`. It orchestrates the entire analytical workflow.\n",
        "\n",
        "```python\n",
        "def run_narrative_shift_detection_pipeline(\n",
        "    # Parameters (i) to (vii) from the main problem description\n",
        "    news_article_data_frame_input: pd.DataFrame,\n",
        "    lda_prototype_params_input: Dict[str, Any],\n",
        "    rolling_lda_params_input: Dict[str, Any],\n",
        "    topical_changes_params_input: Dict[str, Any],\n",
        "    llm_interpretation_params_input: Dict[str, Any],\n",
        "    general_study_params_input: Dict[str, Any],\n",
        "    human_annotations_input_data: Dict[str, Dict[str, Any]],\n",
        "\n",
        "    # Detailed configuration parameters for individual pipeline steps\n",
        "    spacy_model_name_cfg: str = \"en_core_web_sm\",\n",
        "    custom_stopwords_cfg: Optional[List[str]] = None,\n",
        "    countvectorizer_min_df_cfg: int = 5,\n",
        "    countvectorizer_max_df_cfg: float = 0.95,\n",
        "\n",
        "    lda_iterations_prototype_cfg: int = 1000,\n",
        "    lda_alpha_prototype_cfg: str = 'symmetric',\n",
        "    lda_eta_prototype_cfg: Optional[Any] = None, # Gensim default (symmetric based on num_topics if None)\n",
        "    lda_passes_prototype_cfg: int = 10, # Added for completeness for train_lda_prototype\n",
        "\n",
        "    rolling_lda_iterations_warmup_cfg: int = 50,\n",
        "    rolling_lda_iterations_update_cfg: int = 20,\n",
        "    rolling_lda_passes_warmup_cfg: int = 10,\n",
        "    rolling_lda_passes_update_cfg: int = 1,\n",
        "    rolling_lda_alpha_cfg: str = 'symmetric',\n",
        "    rolling_lda_epsilon_eta_cfg: float = 1e-9,\n",
        "\n",
        "    tc_num_tokens_bootstrap_cfg: int = 10000,\n",
        "    tc_num_significant_loo_cfg: int = 10,\n",
        "    tc_epsilon_cfg: float = 1e-9,\n",
        "\n",
        "    llm_quantization_cfg: Optional[Dict[str, Any]] = None,\n",
        "    llm_auth_token_cfg: Optional[str] = None,\n",
        "    llm_trust_remote_code_cfg: bool = True, # Often needed for newer models\n",
        "    llm_use_cache_cfg: bool = True, # For setup_llm_model_and_tokenizer cache\n",
        "\n",
        "    llm_max_new_tokens_cfg: int = 3072,\n",
        "\n",
        "    eval_topic_matching_threshold_cfg: float = 0.1,\n",
        "    analysis_num_top_words_display_cfg: int = 5,\n",
        "    analysis_mapping_num_top_words_cfg: int = 10, # For compile_analysis_results mapping helper\n",
        "\n",
        "    viz_plots_per_row_cfg: int = 5,\n",
        "    viz_figure_title_cfg: str = \"Topic Evolution and Detected Narrative Shifts\",\n",
        "\n",
        "    output_directory_cfg: Optional[str] = None,\n",
        "\n",
        "    doc_run_notes_cfg: Optional[List[str]] = None,\n",
        "    doc_output_format_cfg: str = \"json\"\n",
        "\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire end-to-end narrative shift detection pipeline,\n",
        "    integrating all defined tasks from data validation to documentation.\n",
        "\n",
        "    This function manages the flow of data between modular components,\n",
        "    handles configuration, and implements saving of large artifacts to disk\n",
        "    if an output directory is specified.\n",
        "\n",
        "    Args:\n",
        "        news_article_data_frame_input: Raw news articles DataFrame (param i).\n",
        "        lda_prototype_params_input: Params for LDAPrototype (param ii).\n",
        "        rolling_lda_params_input: Params for RollingLDA (param iii).\n",
        "        topical_changes_params_input: Params for Topical Changes (param iv).\n",
        "        llm_interpretation_params_input: Params for LLM interpretation (param v).\n",
        "        general_study_params_input: General study parameters (param vi).\n",
        "        human_annotations_input_data: Pre-existing human annotations (param vii).\n",
        "        spacy_model_name_cfg: Name of spaCy model for preprocessing.\n",
        "        custom_stopwords_cfg: Custom stopwords for preprocessing.\n",
        "        countvectorizer_min_df_cfg: Min document frequency for CountVectorizer.\n",
        "        countvectorizer_max_df_cfg: Max document frequency for CountVectorizer.\n",
        "        lda_iterations_prototype_cfg: Iterations for LDA in LDAPrototype.\n",
        "        lda_alpha_prototype_cfg: Alpha for LDA in LDAPrototype.\n",
        "        lda_eta_prototype_cfg: Eta for LDA in LDAPrototype.\n",
        "        lda_passes_prototype_cfg: Passes for LDA in LDAPrototype.\n",
        "        rolling_lda_iterations_warmup_cfg: Iterations for RollingLDA warm-up.\n",
        "        rolling_lda_iterations_update_cfg: Iterations for RollingLDA updates.\n",
        "        rolling_lda_passes_warmup_cfg: Passes for RollingLDA warm-up.\n",
        "        rolling_lda_passes_update_cfg: Passes for RollingLDA updates.\n",
        "        rolling_lda_alpha_cfg: Alpha for RollingLDA.\n",
        "        rolling_lda_epsilon_eta_cfg: Epsilon for RollingLDA eta.\n",
        "        tc_num_tokens_bootstrap_cfg: N tokens for Topical Changes bootstrap.\n",
        "        tc_num_significant_loo_cfg: N LOO words for Topical Changes.\n",
        "        tc_epsilon_cfg: Epsilon for Topical Changes numerical stability.\n",
        "        llm_quantization_cfg: Quantization config for LLM setup.\n",
        "        llm_auth_token_cfg: Auth token for LLM setup.\n",
        "        llm_trust_remote_code_cfg: Trust remote code for LLM.\n",
        "        llm_use_cache_cfg: Whether to use internal cache in LLM setup.\n",
        "        llm_max_new_tokens_cfg: Max new tokens for LLM generation.\n",
        "        eval_topic_matching_threshold_cfg: Threshold for mapping system to human topics.\n",
        "        analysis_num_top_words_display_cfg: Num top words for topic display in analysis DF.\n",
        "        analysis_mapping_num_top_words_cfg: Num top words for topic matching in analysis mapping.\n",
        "        viz_plots_per_row_cfg: Plots per row in topic evolution visualization.\n",
        "        viz_figure_title_cfg: Title for the topic evolution figure.\n",
        "        output_directory_cfg (Optional[str]): Base directory to save all generated\n",
        "                                             artifacts. If None, artifacts are not saved.\n",
        "        doc_run_notes_cfg (Optional[List[str]]): User notes for the documentation.\n",
        "        doc_output_format_cfg (str): Format for run documentation ('json' or 'markdown').\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive dictionary containing key outputs from each major\n",
        "                        step of the pipeline. If `output_directory_cfg` is provided,\n",
        "                        this dictionary will contain paths to saved artifacts.\n",
        "    \"\"\"\n",
        "    # ... (implementation)\n",
        "```\n",
        "\n",
        "This function takes the raw data and configuration parameters, performs all analytical steps, and returns a comprehensive results dictionary. Refer to its docstring in the notebook for detailed parameter descriptions.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "**Python Requirements:**\n",
        "- Python 3.9 or higher (required for advanced typing features and library compatibility)\n",
        "\n",
        "**Core Dependencies:**\n",
        "- pandas: For data manipulation and DataFrame structures\n",
        "- numpy: For numerical operations and array manipulations\n",
        "- scipy: For statistical computations and bootstrap methods\n",
        "- scikit-learn: For machine learning utilities and metrics\n",
        "- matplotlib: For visualization and plotting\n",
        "- spacy: For natural language processing and text preprocessing\n",
        "- gensim: For topic modeling (LDA implementation)\n",
        "- torch: For PyTorch-based LLM operations\n",
        "- transformers: For Hugging Face model integration\n",
        "\n",
        "**Additional Requirements:**\n",
        "- CUDA-enabled GPU with â¥16GB VRAM (recommended for LLM inference)\n",
        "- Hugging Face Hub authentication for model access\n",
        "\n",
        "**Installation:**\n",
        "```sh\n",
        "pip install -r requirements.txt\n",
        "python -m spacy download en_core_web_sm\n",
        "huggingface-cli login  # For model access\n",
        "```\n",
        "\n",
        "## Installation\n",
        "\n",
        "1. **Clone the repository:**\n",
        "   ```sh\n",
        "   git clone https://github.com/chirindaopensource/media_narrative_evolution_analysis.git\n",
        "   cd media_narrative_evolution_analysis\n",
        "   ```\n",
        "\n",
        "2. **Install dependencies:**\n",
        "   ```sh\n",
        "   pip install -r requirements.txt\n",
        "   ```\n",
        "\n",
        "3. **Download spaCy model:**\n",
        "   ```sh\n",
        "   python -m spacy download en_core_web_sm\n",
        "   ```\n",
        "\n",
        "4. **Configure LLM access:**\n",
        "   ```sh\n",
        "   huggingface-cli login\n",
        "   ```\n",
        "\n",
        "## Data Structure\n",
        "\n",
        "The primary data input for the `run_narrative_shift_detection_pipeline` function is a pandas DataFrame with specific structure:\n",
        "\n",
        "**Type:** `pd.DataFrame`\n",
        "**Required Structure:**\n",
        "- **Index:** DatetimeIndex representing publication dates\n",
        "- **Columns:** Text columns containing news articles or documents\n",
        "- **Data Types:** String values for text content, datetime index for temporal analysis\n",
        "\n",
        "**Example DataFrame structure:**\n",
        "```python\n",
        "    mock_corpus_data = [\n",
        "        # --- Pre-shift period (2021) ---\n",
        "        {'article_id': 'A001', 'date': '2021-01-15', 'headline': 'Market Hits New High', 'full_text': 'The stock market reached a new peak today driven by strong financial sector performance.'},\n",
        "        {'article_id': 'A002', 'date': '2021-02-20', 'headline': 'Tech Innovations Drive Growth', 'full_text': 'New technology and software innovation are pushing the economy forward. The future of tech is bright.'},\n",
        "        {'article_id': 'A003', 'date': '2021-03-10', 'headline': 'Federal Reserve Policy', 'full_text': 'The federal reserve announced its new policy on interest rates, affecting the financial market.'},\n",
        "        {'article_id': 'A004', 'date': '2021-04-05', 'headline': 'Startup Ecosystem Thrives', 'full_text': 'The technology startup ecosystem sees record investment and innovation.'},\n",
        "        # ... Add more articles to ensure sufficient data for the 12-month warm-up ...\n",
        "        {'article_id': 'A005', 'date': '2021-05-15', 'headline': 'Quarterly Earnings Report', 'full_text': 'Major banks report strong quarterly earnings, boosting the market.'},\n",
        "        {'article_id': 'A006', 'date': '2021-06-20', 'headline': 'AI in Software Development', 'full_text': 'Artificial intelligence is a key technology for modern software.'},\n",
        "        {'article_id': 'A007', 'date': '2021-07-10', 'headline': 'Inflation Concerns Rise', 'full_text': 'Economists express concern over rising inflation and its impact on the market.'},\n",
        "        {'article_id': 'A008', 'date': '2021-08-05', 'headline': 'Cloud Computing Expands', 'full_text': 'The cloud computing technology sector continues its rapid expansion.'},\n",
        "        {'article_id': 'A009', 'date': '2021-09-15', 'headline': 'Bond Market Reacts', 'full_text': 'The bond market reacts to new financial data.'},\n",
        "        {'article_id': 'A010', 'date': '2021-10-20', 'headline': 'Next-Gen Tech Unveiled', 'full_text': 'A major technology firm unveils its next-generation hardware and software.'},\n",
        "        {'article_id': 'A011', 'date': '2021-11-10', 'headline': 'Global Trade Update', 'full_text': 'An update on global trade agreements and their effect on the financial market.'},\n",
        "        {'article_id': 'A012', 'date': '2021-12-05', 'headline': 'Year-End Tech Review', 'full_text': 'A review of the year in technology highlights major software and hardware achievements.'},\n",
        "        {'article_id': 'A013', 'date': '2022-01-15', 'headline': 'Market Opens Strong', 'full_text': 'The financial market opens the year with strong gains.'},\n",
        "        {'article_id': 'A014', 'date': '2022-02-20', 'headline': 'Software as a Service Grows', 'full_text': 'The software as a service technology model continues to show robust growth.'},\n",
        "\n",
        "        # --- Post-shift period (March 2022 onwards) ---\n",
        "        # The narrative around 'technology' now includes 'crisis', 'regulation', 'layoffs'.\n",
        "        {'article_id': 'A015', 'date': '2022-03-10', 'headline': 'Tech Bubble Concerns', 'full_text': 'Concerns of a technology bubble lead to a market crisis. Regulation is now being discussed.'},\n",
        "        {'article_id': 'A016', 'date': '2022-03-15', 'headline': 'Layoffs Hit Tech Sector', 'full_text': 'Major technology firms announce widespread layoffs amid the economic crisis. The software industry faces new regulation.'},\n",
        "        {'article_id': 'A017', 'date': '2022-04-05', 'headline': 'Financial Markets Tumble', 'full_text': 'Financial markets tumble as the technology sector crisis deepens. Investors are worried.'},\n",
        "        {'article_id': 'A018', 'date': '2022-04-20', 'headline': 'Government Scrutinizes Tech', 'full_text': 'The government begins to scrutinize big technology companies, proposing new regulation following the recent crisis and layoffs.'},\n",
        "    ]\n",
        "    # Create the pandas DataFrame from the mock data.\n",
        "    news_article_data_frame_input = pd.DataFrame(mock_corpus_data)\n",
        "    # Convert the 'date' column to datetime objects.\n",
        "    news_article_data_frame_input['date'] = pd.to_datetime(news_article_data_frame_input['date'])\n",
        "    # Set the 'date' column as the DataFrame's index, which is required by the pipeline.\n",
        "    news_article_data_frame_input = news_article_data_frame_input.set_index('date')\n",
        "    print(\"Mock DataFrame created successfully.\")\n",
        "```\n",
        "\n",
        "**Configuration Parameters:**\n",
        "- `lda_prototype_params`: LDA model parameters (K_topics, N_lda_runs)\n",
        "- `rolling_lda_params`: Rolling window parameters (w_warmup, m_memory, K_topics)\n",
        "- `topical_changes_params`: Change detection parameters (z_lookback, alpha_significance, B_bootstrap)\n",
        "- `llm_interpretation_params`: LLM configuration (model_name, temperature, N_docs_filter)\n",
        "- `general_study_params`: Study parameters (time_chunk_granularity, corpus_start_date, corpus_end_date)\n",
        "- `human_annotations_input_data`: Ground truth annotations for evaluation\n",
        "\n",
        "## Usage\n",
        "\n",
        "**Open and Run the Notebook:**\n",
        "1. Open `narrative_shift_detection_draft.ipynb` in Jupyter Notebook or JupyterLab\n",
        "2. Execute cells in order to define all functions and dependencies\n",
        "3. Examine the usage example in the notebook\n",
        "\n",
        "**Execute the Pipeline:**\n",
        "```python\n",
        "# Usage Example for reference\n",
        "# Assume all functions from the iPython notebook are defined and available in the scope,\n",
        "# Assume all the required Python modules have been imported.\n",
        "# This example will call `run_narrative_shift_detection_pipeline`.\n",
        "# from narrative_shift_detection_draft import run_narrative_shift_detection_pipeline\n",
        "\n",
        "def demonstrate_pipeline_execution() -> None:\n",
        "    \"\"\"\n",
        "    Provides a complete, runnable example of how to set up and execute the\n",
        "    narrative shift detection pipeline.\n",
        "\n",
        "    This function meticulously constructs all necessary input data structures and\n",
        "    configuration dictionaries, then invokes the main pipeline orchestrator.\n",
        "    It serves as a practical, implementation-grade blueprint for users of the\n",
        "    pipeline, demonstrating the precise format and structure required for each\n",
        "    input parameter.\n",
        "\n",
        "    The example uses a small, synthetic news corpus designed to have a plausible\n",
        "    narrative shift, allowing the pipeline to be tested end-to-end. It also\n",
        "    includes a mock human annotation to enable the evaluation stage.\n",
        "\n",
        "    Note:\n",
        "        This function assumes that the `run_narrative_shift_detection_pipeline`\n",
        "        and all its helper functions are defined and available in the current\n",
        "        Python environment. It also assumes that the required LLM (e.g.,\n",
        "        \"meta-llama/Meta-Llama-3.1-8B-Instruct\") is accessible, which may\n",
        "        require authentication and appropriate hardware (GPU). For this\n",
        "        demonstration, the LLM-dependent steps will be executed but may be\n",
        "        slow or require significant resources.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Define All Input Data Structures ---\n",
        "    # This section creates mock data that is structurally identical to the\n",
        "    # real-world data the pipeline is designed to process.\n",
        "\n",
        "    # Sub-step 1.a: Create a mock news article DataFrame (Parameter i)\n",
        "    # This DataFrame simulates a corpus with a narrative shift around a specific topic.\n",
        "    # The topic of 'technology' is stable until early 2022, after which it\n",
        "    # shifts to include terms related to 'crisis' and 'regulation'.\n",
        "    print(\"Step 1.a: Constructing mock news article DataFrame...\")\n",
        "    mock_corpus_data = [\n",
        "        # --- Pre-shift period (2021) ---\n",
        "        {'article_id': 'A001', 'date': '2021-01-15', 'headline': 'Market Hits New High', 'full_text': 'The stock market reached a new peak today driven by strong financial sector performance.'},\n",
        "        {'article_id': 'A002', 'date': '2021-02-20', 'headline': 'Tech Innovations Drive Growth', 'full_text': 'New technology and software innovation are pushing the economy forward. The future of tech is bright.'},\n",
        "        {'article_id': 'A003', 'date': '2021-03-10', 'headline': 'Federal Reserve Policy', 'full_text': 'The federal reserve announced its new policy on interest rates, affecting the financial market.'},\n",
        "        {'article_id': 'A004', 'date': '2021-04-05', 'headline': 'Startup Ecosystem Thrives', 'full_text': 'The technology startup ecosystem sees record investment and innovation.'},\n",
        "        # ... Add more articles to ensure sufficient data for the 12-month warm-up ...\n",
        "        {'article_id': 'A005', 'date': '2021-05-15', 'headline': 'Quarterly Earnings Report', 'full_text': 'Major banks report strong quarterly earnings, boosting the market.'},\n",
        "        {'article_id': 'A006', 'date': '2021-06-20', 'headline': 'AI in Software Development', 'full_text': 'Artificial intelligence is a key technology for modern software.'},\n",
        "        {'article_id': 'A007', 'date': '2021-07-10', 'headline': 'Inflation Concerns Rise', 'full_text': 'Economists express concern over rising inflation and its impact on the market.'},\n",
        "        {'article_id': 'A008', 'date': '2021-08-05', 'headline': 'Cloud Computing Expands', 'full_text': 'The cloud computing technology sector continues its rapid expansion.'},\n",
        "        {'article_id': 'A009', 'date': '2021-09-15', 'headline': 'Bond Market Reacts', 'full_text': 'The bond market reacts to new financial data.'},\n",
        "        {'article_id': 'A010', 'date': '2021-10-20', 'headline': 'Next-Gen Tech Unveiled', 'full_text': 'A major technology firm unveils its next-generation hardware and software.'},\n",
        "        {'article_id': 'A011', 'date': '2021-11-10', 'headline': 'Global Trade Update', 'full_text': 'An update on global trade agreements and their effect on the financial market.'},\n",
        "        {'article_id': 'A012', 'date': '2021-12-05', 'headline': 'Year-End Tech Review', 'full_text': 'A review of the year in technology highlights major software and hardware achievements.'},\n",
        "        {'article_id': 'A013', 'date': '2022-01-15', 'headline': 'Market Opens Strong', 'full_text': 'The financial market opens the year with strong gains.'},\n",
        "        {'article_id': 'A014', 'date': '2022-02-20', 'headline': 'Software as a Service Grows', 'full_text': 'The software as a service technology model continues to show robust growth.'},\n",
        "\n",
        "        # --- Post-shift period (March 2022 onwards) ---\n",
        "        # The narrative around 'technology' now includes 'crisis', 'regulation', 'layoffs'.\n",
        "        {'article_id': 'A015', 'date': '2022-03-10', 'headline': 'Tech Bubble Concerns', 'full_text': 'Concerns of a technology bubble lead to a market crisis. Regulation is now being discussed.'},\n",
        "        {'article_id': 'A016', 'date': '2022-03-15', 'headline': 'Layoffs Hit Tech Sector', 'full_text': 'Major technology firms announce widespread layoffs amid the economic crisis. The software industry faces new regulation.'},\n",
        "        {'article_id': 'A017', 'date': '2022-04-05', 'headline': 'Financial Markets Tumble', 'full_text': 'Financial markets tumble as the technology sector crisis deepens. Investors are worried.'},\n",
        "        {'article_id': 'A018', 'date': '2022-04-20', 'headline': 'Government Scrutinizes Tech', 'full_text': 'The government begins to scrutinize big technology companies, proposing new regulation following the recent crisis and layoffs.'},\n",
        "    ]\n",
        "    # Create the pandas DataFrame from the mock data.\n",
        "    news_article_data_frame_input = pd.DataFrame(mock_corpus_data)\n",
        "    # Convert the 'date' column to datetime objects.\n",
        "    news_article_data_frame_input['date'] = pd.to_datetime(news_article_data_frame_input['date'])\n",
        "    # Set the 'date' column as the DataFrame's index, which is required by the pipeline.\n",
        "    news_article_data_frame_input = news_article_data_frame_input.set_index('date')\n",
        "    print(\"Mock DataFrame created successfully.\")\n",
        "\n",
        "    # Sub-step 1.b: Define parameter dictionaries (Parameters ii-vi)\n",
        "    # These dictionaries configure the core algorithms of the pipeline.\n",
        "    # The values are taken directly from the paper's specified configuration.\n",
        "    print(\"Step 1.b: Defining algorithm parameter dictionaries...\")\n",
        "    # Parameters for LDAPrototype Selection.\n",
        "    lda_prototype_params_input = {\"K_topics\": 2, \"N_lda_runs\": 3} # Reduced for speed in example\n",
        "    # Parameters for RollingLDA Application.\n",
        "    rolling_lda_params_input = {\"w_warmup\": 12, \"m_memory\": 4, \"K_topics\": 2} # K_topics must match\n",
        "    # Parameters for Topical Change Detection.\n",
        "    topical_changes_params_input = {\"z_lookback\": 2, \"mixture_param_gamma\": 0.95, \"alpha_significance\": 0.01, \"B_bootstrap\": 100} # Reduced for speed\n",
        "    # Parameters for LLM-based Narrative Interpretation.\n",
        "    llm_interpretation_params_input = {\"llm_model_name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"llm_temperature\": 0.0, \"N_docs_filter\": 2}\n",
        "    # General Study Parameters.\n",
        "    general_study_params_input = {\"time_chunk_granularity\": \"monthly\", \"corpus_start_date\": \"2021-01-01\", \"corpus_end_date\": \"2022-12-31\"}\n",
        "    print(\"Parameter dictionaries defined.\")\n",
        "\n",
        "    # Sub-step 1.c: Create mock human-annotated change points (Parameter vii)\n",
        "    # This dictionary represents the ground truth against which the LLM's\n",
        "    # classification performance will be evaluated.\n",
        "    print(\"Step 1.c: Constructing mock human annotation data...\")\n",
        "    human_annotations_input_data = {\n",
        "        \"2022-03-15\": {\n",
        "            \"change_type\": \"narrative shift\",\n",
        "            \"topics\": [\"technology\", \"crisis\", \"regulation\", \"layoffs\"],\n",
        "            \"setting\": [\"Global technology sector\", \"Financial markets\"],\n",
        "            \"characters\": [\"Technology companies\", \"Investors\", \"Government regulators\"],\n",
        "            \"plot\": \"A previously booming technology sector faces an abrupt crisis, leading to layoffs and prompting calls for government regulation.\",\n",
        "            \"moral\": \"The moral is that unchecked growth in the technology sector is unsustainable and poses systemic risks, necessitating oversight.\"\n",
        "        }\n",
        "        # ... Add more annotations...\n",
        "    }\n",
        "    print(\"Mock human annotations created.\")\n",
        "\n",
        "    # --- Step 2: Define Detailed Pipeline Step Configurations ---\n",
        "    # These are the more granular settings passed to the orchestrator function.\n",
        "    print(\"\\nStep 2: Defining detailed pipeline configurations...\")\n",
        "    # Create a temporary directory for pipeline artifacts. This is a robust\n",
        "    # practice for examples as it ensures cleanup after execution.\n",
        "    with tempfile.TemporaryDirectory() as temp_dir:\n",
        "        # Set the output directory configuration to the created temporary directory.\n",
        "        output_directory_cfg = temp_dir\n",
        "        print(f\"Pipeline artifacts will be saved to temporary directory: {output_directory_cfg}\")\n",
        "\n",
        "        # --- Step 3: Execute the Pipeline ---\n",
        "        # This is the primary call to the main orchestrator function.\n",
        "        print(\"\\nStep 3: Executing the narrative shift detection pipeline...\")\n",
        "        # The `run_narrative_shift_detection_pipeline` function is assumed to be\n",
        "        # imported or defined in the current scope.\n",
        "        pipeline_outputs = run_narrative_shift_detection_pipeline(\n",
        "            # Pass all the defined input data structures (Parameters i-vii).\n",
        "            news_article_data_frame_input=news_article_data_frame_input,\n",
        "            lda_prototype_params_input=lda_prototype_params_input,\n",
        "            rolling_lda_params_input=rolling_lda_params_input,\n",
        "            topical_changes_params_input=topical_changes_params_input,\n",
        "            llm_interpretation_params_input=llm_interpretation_params_input,\n",
        "            general_study_params_input=general_study_params_input,\n",
        "            human_annotations_input_data=human_annotations_input_data,\n",
        "\n",
        "            # Pass all the detailed configuration settings.\n",
        "            spacy_model_name_cfg=\"en_core_web_sm\",\n",
        "            countvectorizer_min_df_cfg=1, # Lowered for small mock corpus\n",
        "            countvectorizer_max_df_cfg=0.95,\n",
        "            lda_iterations_prototype_cfg=200, # Reduced for speed\n",
        "            rolling_lda_iterations_warmup_cfg=100, # Reduced for speed\n",
        "            rolling_lda_iterations_update_cfg=50, # Reduced for speed\n",
        "            llm_quantization_cfg={\"load_in_8bit\": True}, # Use 8-bit quantization to reduce memory\n",
        "            llm_max_new_tokens_cfg=1024, # Sufficient for the expected JSON output\n",
        "            output_directory_cfg=output_directory_cfg,\n",
        "            doc_output_format_cfg=\"markdown\"\n",
        "        )\n",
        "        print(\"Pipeline execution finished.\")\n",
        "\n",
        "        # --- Step 4: Process and Display Pipeline Outputs ---\n",
        "        # This section demonstrates how to interpret the results returned by the pipeline.\n",
        "        print(\"\\n--- Pipeline Execution Summary ---\")\n",
        "        # Check the final status of the pipeline run.\n",
        "        pipeline_status = pipeline_outputs.get(\"pipeline_status\", \"Unknown\")\n",
        "        print(f\"Final Pipeline Status: {pipeline_status}\")\n",
        "\n",
        "        # If the pipeline failed, print the error message.\n",
        "        if pipeline_status == \"Failed\":\n",
        "            print(f\"Error Message: {pipeline_outputs.get('error_message')}\")\n",
        "            print(\"--- Error Traceback ---\")\n",
        "            print(pipeline_outputs.get('error_traceback'))\n",
        "        else:\n",
        "            # If the pipeline succeeded, print a summary of the key outputs.\n",
        "            # Use json.dumps for a clean, readable printout of the results dictionary.\n",
        "            # We create a copy to remove potentially large objects before printing.\n",
        "            summary_outputs = pipeline_outputs.copy()\n",
        "            # Remove keys that might contain very large data for a cleaner summary print.\n",
        "            summary_outputs.pop(\"parameters_and_configurations\", None)\n",
        "            summary_outputs.pop(\"compiled_analysis_dataframe_path\", None)\n",
        "\n",
        "            print(\"\\n--- Key Pipeline Outputs ---\")\n",
        "            # Pretty-print the summary dictionary.\n",
        "            print(json.dumps(summary_outputs, indent=2))\n",
        "\n",
        "            # Load and display the head of the final compiled analysis DataFrame if it was created.\n",
        "            analysis_df_path = pipeline_outputs.get(\"compiled_analysis_dataframe_path\")\n",
        "            if analysis_df_path and os.path.exists(analysis_df_path):\n",
        "                print(\"\\n--- Compiled Analysis DataFrame (Head) ---\")\n",
        "                # Read the saved CSV file into a pandas DataFrame.\n",
        "                results_df = pd.read_csv(analysis_df_path)\n",
        "                # Print the first few rows of the DataFrame.\n",
        "                print(results_df.head().to_string())\n",
        "\n",
        "            # Display the content of the generated documentation file.\n",
        "            doc_path = pipeline_outputs.get(\"documentation_file_path\")\n",
        "            if doc_path and os.path.exists(doc_path):\n",
        "                print(f\"\\n--- Generated Run Documentation (from {doc_path}) ---\")\n",
        "                # Open and read the content of the documentation file.\n",
        "                with open(doc_path, 'r', encoding='utf-8') as f:\n",
        "                    # Print the documentation content.\n",
        "                    print(f.read())\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    demonstrate_pipeline_execution()\n",
        "    print(\"Demonstration function `demonstrate_pipeline_execution` is defined.\")\n",
        "    print(\"To run the example, uncomment the call in the `if __name__ == '__main__':` block.\")\n",
        "    print(\"Ensure all dependencies are installed and you have access to the required LLM and hardware.\")\n",
        "```\n",
        "\n",
        "**Adapt for Real Data:**\n",
        "- Replace synthetic data with your own corpus following the input structure requirements\n",
        "- Adjust parameters based on your corpus characteristics and research questions\n",
        "- Ensure sufficient computational resources for LLM inference\n",
        "- Review data quality reports in the output for validation\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The `run_narrative_shift_detection_pipeline` function returns a comprehensive dictionary containing:\n",
        "\n",
        "**Core Results:**\n",
        "- `change_points_detected`: List of detected narrative shift points with timestamps and metadata\n",
        "- `llm_classifications`: Structured classifications of each change point (content vs. narrative shift)\n",
        "- `topic_evolution_data`: Time series data of topic distributions and evolution\n",
        "- `performance_metrics`: Evaluation results comparing LLM output to human annotations\n",
        "\n",
        "**Artifacts and Outputs:**\n",
        "- `compiled_analysis_dataframe_path`: Path to comprehensive results DataFrame\n",
        "- `visualizations_directory`: Directory containing generated plots and charts\n",
        "- `model_artifacts_directory`: Saved topic models and intermediate results\n",
        "- `pipeline_documentation_path`: Detailed run documentation for reproducibility\n",
        "\n",
        "**Metadata:**\n",
        "- `pipeline_execution_time`: Total runtime and performance metrics\n",
        "- `computational_resources_used`: GPU/CPU usage statistics\n",
        "- `data_quality_report`: Preprocessing and validation results\n",
        "- `parameter_configurations`: Complete record of all input parameters\n",
        "\n",
        "This comprehensive output enables detailed analysis of results, performance evaluation, and reproducible research workflows.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "media_narrative_evolution_analysis/\n",
        "â\n",
        "âââ narrative_shift_detection_draft.ipynb # Main implementation notebook\n",
        "âââ requirements.txt                       # Python package dependencies\n",
        "âââ LICENSE                               # MIT license file\n",
        "âââ README.md                             # This documentation file\n",
        "\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline offers extensive customization through several key parameters:\n",
        "\n",
        "**Topic Modeling Customization:**\n",
        "- `K_topics`: Number of topics for LDA models\n",
        "- `N_lda_runs`: Number of LDA runs for prototype selection\n",
        "- `w_warmup`: Warm-up window size for rolling LDA\n",
        "- `m_memory`: Memory parameter for temporal coherence\n",
        "\n",
        "**Change Detection Customization:**\n",
        "- `alpha_significance`: Significance level for statistical tests\n",
        "- `B_bootstrap`: Number of bootstrap samples\n",
        "- `z_lookback`: Lookback window for change detection\n",
        "\n",
        "**LLM Analysis Customization:**\n",
        "- `llm_model_name`: Choice of LLM model (supports various Hugging Face models)\n",
        "- `llm_temperature`: Temperature parameter for LLM generation\n",
        "- `N_docs_filter`: Number of documents to analyze per change point\n",
        "\n",
        "**Temporal Analysis Customization:**\n",
        "- `time_chunk_granularity`: Temporal resolution (daily, weekly, monthly)\n",
        "- `corpus_start_date` / `corpus_end_date`: Analysis time period\n",
        "\n",
        "Users can modify these parameters to adapt the pipeline to different corpora, research questions, and computational constraints.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions to this project are welcome and greatly appreciated. Please follow these guidelines:\n",
        "\n",
        "1. **Fork the Repository:** Create your own fork of the project\n",
        "2. **Create a Feature Branch:** `git checkout -b feature/AmazingFeature`\n",
        "3. **Code Standards:**\n",
        "   - Follow PEP-8 style guidelines\n",
        "   - Include comprehensive type hints and docstrings\n",
        "   - Ensure code is compatible with Python 3.9+\n",
        "4. **Testing:** Write unit tests for new functionality\n",
        "5. **Documentation:** Update documentation for any new features or changes\n",
        "6. **Commit Changes:** `git commit -m 'Add some AmazingFeature'`\n",
        "7. **Push to Branch:** `git push origin feature/AmazingFeature`\n",
        "8. **Open Pull Request:** Submit a pull request with clear description of changes\n",
        "\n",
        "**Development Setup:**\n",
        "```sh\n",
        "# Install development dependencies\n",
        "pip install -r requirements.txt\n",
        "\n",
        "# Install pre-commit hooks\n",
        "pre-commit install\n",
        "\n",
        "# Run tests\n",
        "pytest tests/\n",
        "\n",
        "# Run linting\n",
        "flake8 .\n",
        "black .\n",
        "isort .\n",
        "mypy .\n",
        "```\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "**MIT License**\n",
        "\n",
        "Copyright Â© 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@inproceedings{lange2025narrative,\n",
        "  title={Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and Large Language Models},\n",
        "  author={Lange, Kai-Robin and Schmidt, Tobias and Reccius, Matthias and MÃ¼ller, Henrik and Roos, Michael and Jentsch, Carsten},\n",
        "  booktitle={Proceedings of the Text2Story'25 Workshop},\n",
        "  year={2025},\n",
        "  address={Luca, Italy},\n",
        "  month={April}\n",
        "}\n",
        "```\n",
        "\n",
        "**For the Implementation:**\n",
        "Consider also acknowledging this GitHub repository if the implementation itself was significantly helpful to your research:\n",
        "\n",
        "```\n",
        "Chirinda, C. (2025). Narrative Shift Detection: A Hybrid DTM-LLM Approach - Python Implementation.\n",
        "GitHub repository: https://github.com/chirindaopensource/media_narrative_evolution_analysis\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "- Special thanks to the authors of the original paper for their groundbreaking research in hybrid narrative analysis methodologies\n",
        "- Gratitude to the open-source community for the foundational libraries that make this research possible\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of `narrative_shift_detection_draft.ipynb` and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "DJLPLXfzLVUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: *Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and Large Language Models*\n",
        "\n",
        "Article Link: https://arxiv.org/abs/2506.20269\n",
        "\n",
        "Date: 25 June 2025\n",
        "\n",
        "Author's GitHub Repository: https://github.com/K-RLange/T2SNarrativeChanges\n",
        "\n",
        "Abstract:\n",
        "\n",
        "With rapidly evolving media narratives, it has become increasingly critical to not just extract narratives from a given corpus but rather investigate, how they develop over time. While popular narrative extraction methods such as Large Language Models do well in capturing typical narrative elements or even the complex structure of a narrative, applying them to an entire corpus comes with obstacles, such as a high financial or computational cost. We propose a combination of the language understanding capabilities of Large Language Models with the large scale applicability of topic models to dynamically model narrative shifts across time using the Narrative Policy Framework. We apply a topic model and a corresponding change point detection method to find changes that concern a specific topic of interest. Using this model, we filter our corpus for documents that are particularly representative of that change and feed them into a Large Language Model that interprets the change that happened in an automated fashion and distinguishes between content and narrative shifts. We employ our pipeline on a corpus of The Wall Street Journal news paper articles from 2009 to 2023. Our findings indicate that a Large Language Model can efficiently extract a narrative shift if one exists at a given point in time, but does not perform as well when having to decide whether a shift in content or a narrative shift took place."
      ],
      "metadata": {
        "id": "U0TsGM8r5HGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "Okay, let's dissect this paper. From my perspective, this work attempts to bridge the gap between scalable, but somewhat superficial, topic modeling and the deep, but computationally intensive, understanding capabilities of Large Language Models (LLMs) for the specific task of detecting and characterizing *narrative shifts* over time. This is a pertinent problem, especially in fields like media studies, political economy, and indeed, financial market sentiment analysis where narratives can drive behavior.\n",
        "\n",
        "Here's a step-by-step summary and critique:\n",
        "\n",
        "**Overall Objective:**\n",
        "The primary goal is to develop a computationally feasible pipeline that can identify *when* and *how* narratives within a large text corpus (specifically news articles) change over time, distinguishing between mere content shifts and genuine narrative re-framings.\n",
        "\n",
        "--\n",
        "\n",
        "**Step 1: Addressing the Core Problem â Scalability vs. Depth**\n",
        "\n",
        "*   **The Challenge:** Analyzing narrative evolution in large, longitudinal corpora is difficult.\n",
        "    *   **Traditional Topic Models (e.g., LDA):** Scalable for identifying thematic changes (word co-occurrence patterns) but lack nuanced understanding of narrative structure (characters, plot, moral, causality). Dynamic Topic Models (DTMs) can track topic evolution but still operate at the word-distribution level.\n",
        "    *   **Large Language Models (LLMs):** Possess sophisticated language understanding capabilities, potentially able to grasp complex narrative elements. However, applying them to an entire large corpus for continuous monitoring is often prohibitive due to computational cost and API expenses (if using commercial models). Training them from scratch on a specific corpus for temporal analysis is also impractical.\n",
        "*   **The Paper's Core Idea:** A hybrid approach. Use dynamic topic modeling as a \"first-pass filter\" to identify potential change points at scale, then deploy an LLM for a \"deep dive\" analysis on a small, curated set of documents surrounding these detected change points.\n",
        "\n",
        "--\n",
        "\n",
        "**Step 2: The \"First-Pass Filter\" â Dynamic Topic Modeling and Change Point Detection**\n",
        "\n",
        "This part of the pipeline aims to efficiently pinpoint moments of significant thematic alteration.\n",
        "\n",
        "*   **A. LDAPrototype for Robust Topic Initialization:**\n",
        "    *   Recognizing the inherent non-determinism of LDA (Latent Dirichlet Allocation) due to random initialization and sampling, the authors first employ `LDAPrototype`.\n",
        "    *   This involves training *N* LDA models and selecting the one with the highest average pairwise similarity to all other models (similarity based on cosine distance of topic-word distributions). This aims to yield a more stable and representative \"base\" set of topics.\n",
        "*   **B. RollingLDA for Dynamic Topic Evolution:**\n",
        "    *   Building on `LDAPrototype`, `RollingLDA` is used to model topic dynamics. It's a dynamic topic model that uses a rolling window approach.\n",
        "    *   It initializes on the first `w` time chunks (e.g., 12 months for yearly trends) and then, for subsequent time chunks, updates topic assignments and distributions based on information from the preceding `m` time chunks (e.g., 4 months for quarterly memory).\n",
        "    *   This allows topics to evolve while maintaining coherence, and is designed to be sensitive to abrupt changes, which is suitable for news media.\n",
        "*   **C. Topical Changes for Change Point Detection:**\n",
        "    *   This module takes the output of `RollingLDA` (time-stamped topic-word distributions) and detects significant shifts.\n",
        "    *   For each topic at each time point, it compares the current word-topic vector with a \"look-back\" vector (aggregated over the previous `z` chunks).\n",
        "    *   A bootstrap-based hypothesis test is performed: if the cosine distance between the current and look-back vectors is significantly larger than expected (based on B bootstrap samples from the look-back period), a change point is flagged.\n",
        "    *   Crucially, it also identifies \"leave-one-out word impacts\" â words whose removal most significantly reduces the distance, thus indicating they are key drivers of the detected change.\n",
        "\n",
        "--\n",
        "\n",
        "**Step 3: The \"Deep Dive\" â LLM-based Narrative Interpretation**\n",
        "\n",
        "Once a change point is detected in a specific topic at a specific time:\n",
        "\n",
        "*   **A. Document Filtering:**\n",
        "    *   Instead of feeding the entire corpus (or even all documents from the change-point period) to the LLM, the authors filter. They select the top 5 documents from the current time chunk (`t`) that have the highest occurrence of the \"leave-one-out\" significant words identified by the `Topical Changes` model.\n",
        "    *   *Self-correction/Observation:* They note that including documents from the preceding chunk (`t-1`) \"confused\" the LLM, which is an interesting practical finding.\n",
        "*   **B. LLM Prompting with the Narrative Policy Framework (NPF):**\n",
        "    *   The selected documents are fed to an LLM (Llama 3.1 8B, an open-source model, chosen due to copyrighted data).\n",
        "    *   The prompt is carefully engineered:\n",
        "        *   It instructs the LLM to act as an \"expert journalist.\"\n",
        "        *   It explicitly provides the definition of a narrative based on the **Narrative Policy Framework (NPF)**, which emphasizes structural elements: **setting, characters, plot, and moral (with a value judgment)**. This is a key theoretical grounding.\n",
        "        *   It provides the top words for the topic *before* and *after* the change, and the significant \"leave-one-out\" words.\n",
        "        *   The LLM is tasked to:\n",
        "            1.  Summarize each input article.\n",
        "            2.  Explain the overall topic change.\n",
        "            3.  Determine if this constitutes a *narrative shift* (according to NPF) or merely a *content shift*.\n",
        "            4.  If a narrative shift, detail the NPF elements (setting, characters, plot, moral) before and after the shift.\n",
        "            5.  Output in a structured JSON format.\n",
        "\n",
        "--\n",
        "\n",
        "**Step 4: Empirical Validation and Results**\n",
        "\n",
        "*   **Dataset:** A substantial corpus of 795,800 Wall Street Journal articles (2009-2023).\n",
        "*   **Parameters:** Monthly time chunks, `K=50` topics, `w=12` months (warm-up), `m=4` months (RollingLDA memory), `z=4` months (Topical Changes look-back), `alpha=0.01` (significance level), `B=500` (bootstrap samples). LLM temperature set to 0 (for deterministic output).\n",
        "*   **Findings from DTM/Change Detection:** 68 change points detected across 156 time chunks (after the 1-year warm-up).\n",
        "*   **Human Annotation:** Three expert annotators reviewed these 68 changes and, using NPF, classified 37 of them as genuine narrative shifts (the rest being content shifts).\n",
        "*   **LLM Performance:**\n",
        "    *   **Binary Classification (Narrative vs. Content Shift):** The LLM performed poorly here. It identified a narrative in 60 out of 68 cases, leading to an accuracy of 57.35% and an F1-score of 0.7010. This indicates a high false positive rate â the LLM tends to \"hallucinate\" or force-fit a narrative structure even when one isn't strongly present according to human experts.\n",
        "    *   **Explaining *Existing* Narrative Shifts:** When a narrative shift *was* present (according to human annotators), the LLM did a much better job of accurately defining and detailing the NPF elements in 31 out of 37 cases (83.78%).\n",
        "\n",
        "--\n",
        "\n",
        "**Step 5: Conclusions and Critical Assessment (from my professorial viewpoint)**\n",
        "\n",
        "*   **Strengths:**\n",
        "    *   **Novel Hybrid Architecture:** The combination of scalable DTMs for change point detection and LLMs for nuanced interpretation is a sensible and innovative approach to a challenging problem.\n",
        "    *   **Theoretical Grounding:** The explicit use of the Narrative Policy Framework to guide the LLM's interpretation is a significant strength, providing a structured and theoretically informed definition of \"narrative.\"\n",
        "    *   **Methodological Rigor in DTM:** The use of `LDAPrototype` and `RollingLDA` with `Topical Changes` demonstrates attention to the statistical challenges of dynamic topic modeling and change detection.\n",
        "    *   **Open-Source LLM:** Using a local, open-source LLM is practical for academic research with sensitive or copyrighted data.\n",
        "    *   **Promising for Explanation:** The LLM shows good capability in *explaining* a narrative shift once it's known to exist, by breaking it down into NPF components.\n",
        "\n",
        "*   **Weaknesses and Areas for Future Work:**\n",
        "    *   **Poor Discrimination:** The LLM's primary failure was in distinguishing narrative shifts from mere content shifts. The authors attribute this to \"hallucinatory behavior\" or an over-eagerness to satisfy the prompt's request for narrative elements. This is a common issue with LLMs; they are often \"too helpful.\"\n",
        "        *   *Suggestion:* This might be mitigated by more sophisticated prompting (e.g., explicitly allowing for \"no narrative shift\" as a primary valid output, perhaps with a confidence score) or a two-stage LLM process (first classify, then explain if classified as narrative). Fine-tuning a smaller LLM on this specific discrimination task could also be an avenue, though data-intensive.\n",
        "    *   **Document Filtering:** The strategy of using only 5 documents based on \"leave-one-out\" words is a heuristic. While it reduces LLM input, it might miss broader contextual cues or introduce bias. Exploring alternative filtering or summarization techniques before LLM input could be beneficial.\n",
        "    *   **Sensitivity to Parameters:** As acknowledged in the limitations, the DTM and change detection parameters (`w, m, z, K, alpha`) will influence the number and nature of detected changes, which in turn affects the LLM's input. Robustness checks across parameter settings would be valuable.\n",
        "    *   **Evaluation of \"Incorrectly Detected Changes\":** The study didn't evaluate how the LLM would respond if the `Topical Changes` model flagged a change point that human annotators deemed a false positive (i.e., no real change at all).\n",
        "    *   **Binary Nature of \"Narrative Shift\":** The current evaluation treats narrative shift as a binary phenomenon. In reality, shifts can be gradual or partial. A more nuanced evaluation framework for the \"degree\" of narrative shift could be developed.\n",
        "\n",
        "*   **Implications for CS, Math Finance, and Econometrics:**\n",
        "    *   **CS (NLP/AI):** Highlights the ongoing challenge of controlling LLM output and reducing hallucination, especially in analytical tasks requiring high fidelity. Prompt engineering and hybrid AI architectures remain key research areas.\n",
        "    *   **Math Finance/Econometrics:** The dynamic topic modeling and change-point detection components are directly relevant to time-series analysis of textual data. If the discrimination problem can be solved, this pipeline could be adapted to track shifts in economic narratives (e.g., around inflation, recession fears, policy changes) from financial news or regulatory filings, potentially offering leading indicators or qualitative insights for quantitative models. The NPF's emphasis on \"moral\" and \"blame\" could be particularly interesting for understanding sentiment and its drivers.\n",
        "\n",
        "**In essence:** This paper presents a well-structured and thoughtful approach to a complex problem. The DTM front-end is solid. The LLM back-end shows promise for interpretation but struggles with classification. The main takeaway is that while LLMs are powerful for understanding, guiding them to make accurate, high-stakes *judgments* (like \"is this a narrative shift or not?\") remains a significant hurdle that requires more than just clever prompting; it may require more fundamental advances in LLM reasoning or specialized fine-tuning."
      ],
      "metadata": {
        "id": "zqDcU8gE7N5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports, Type Definitions and Constant Definitions"
      ],
      "metadata": {
        "id": "JH5eud_-5Rzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Essential Modules\n",
        "# =============================================================================\n",
        "# STANDARD LIBRARY IMPORTS\n",
        "# =============================================================================\n",
        "import datetime\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import pickle\n",
        "import platform\n",
        "import string\n",
        "import sys\n",
        "import warnings\n",
        "from collections import Counter\n",
        "from typing import Any, Dict, List, Optional, Set, Tuple, Union\n",
        "\n",
        "# =============================================================================\n",
        "# THIRD-PARTY IMPORTS\n",
        "# =============================================================================\n",
        "\n",
        "# Data Handling & Numerical Computing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.spatial.distance import cosine as cosine_distance\n",
        "\n",
        "# Natural Language Processing\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "\n",
        "# Machine Learning - Scikit-learn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import (\n",
        "    ConfusionMatrixDisplay,\n",
        "    accuracy_score,\n",
        "    confusion_matrix as sk_confusion_matrix,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Topic Modeling - Gensim\n",
        "from gensim.corpora import Dictionary, Dictionary as GensimDictionary\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "# Deep Learning & LLMs\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizerBase,\n",
        ")\n",
        "\n",
        "# Visualization\n",
        "import matplotlib\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# =============================================================================\n",
        "# CONSTANTS AND CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# LLM JSON Schema Validation Constants\n",
        "EXPECTED_LLM_JSON_SCHEMA: Dict[str, Any] = {\n",
        "    \"summaries\": list,\n",
        "    \"topic_change\": str,\n",
        "    \"narrative_before\": str,\n",
        "    \"narrative_after\": str,\n",
        "    \"narrative_criteria\": list,\n",
        "    \"true_narrative\": bool,\n",
        "}\n",
        "\n",
        "EXPECTED_NARRATIVE_CRITERIA_KEYS: List[str] = [\n",
        "    \"setting\",\n",
        "    \"characters\",\n",
        "    \"plot\",\n",
        "    \"moral\",\n",
        "]\n",
        "\n",
        "# =============================================================================\n",
        "# GLOBAL CACHE VARIABLES\n",
        "# =============================================================================\n",
        "\n",
        "# LLM Model Cache\n",
        "LLM_CACHE: Dict[str, Tuple[PreTrainedModel, PreTrainedTokenizerBase]] = {}\n",
        "\n",
        "# spaCy Model Cache\n",
        "NLP_MODEL: Optional[Language] = None\n",
        "\n",
        "# =============================================================================\n",
        "# LIBRARY VERSION INFORMATION\n",
        "# =============================================================================\n",
        "\n",
        "def _get_library_version(library_name: str, import_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Safely get library version with proper error handling.\n",
        "\n",
        "    Args:\n",
        "        library_name: Human-readable name of the library\n",
        "        import_path: Import path to check version\n",
        "\n",
        "    Returns:\n",
        "        Version string or error message\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if import_path == \"pandas\":\n",
        "            return pd.__version__\n",
        "        elif import_path == \"numpy\":\n",
        "            return np.__version__\n",
        "        elif import_path == \"sklearn\":\n",
        "            import sklearn\n",
        "            return sklearn.__version__\n",
        "        elif import_path == \"gensim\":\n",
        "            import gensim\n",
        "            return gensim.__version__\n",
        "        elif import_path == \"spacy\":\n",
        "            return spacy.__version__\n",
        "        elif import_path == \"torch\":\n",
        "            return torch.__version__\n",
        "        elif import_path == \"transformers\":\n",
        "            import transformers\n",
        "            return transformers.__version__\n",
        "        elif import_path == \"matplotlib\":\n",
        "            return matplotlib.__version__\n",
        "        else:\n",
        "            return \"Unknown library\"\n",
        "    except (ImportError, AttributeError):\n",
        "        return f\"{library_name} not installed or version unavailable\"\n",
        "\n",
        "\n",
        "# Library Version Constants\n",
        "PANDAS_VERSION: str = _get_library_version(\"Pandas\", \"pandas\")\n",
        "NUMPY_VERSION: str = _get_library_version(\"NumPy\", \"numpy\")\n",
        "SKLEARN_VERSION: str = _get_library_version(\"Scikit-learn\", \"sklearn\")\n",
        "GENSIM_VERSION: str = _get_library_version(\"Gensim\", \"gensim\")\n",
        "SPACY_VERSION: str = _get_library_version(\"spaCy\", \"spacy\")\n",
        "TORCH_VERSION: str = _get_library_version(\"PyTorch\", \"torch\")\n",
        "TRANSFORMERS_VERSION: str = _get_library_version(\"Transformers\", \"transformers\")\n",
        "MATPLOTLIB_VERSION: str = _get_library_version(\"Matplotlib\", \"matplotlib\")\n",
        "\n"
      ],
      "metadata": {
        "id": "PKdk2Qq35TNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "N8gRvZcV5V9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### Key Callables in Draft\n",
        "\n",
        "Understood. I will provide a granular, step-by-step analysis of each Python callable from the provided notebook. My response will be clinical, precise, and directly tied to the methodological framework established in the LaTeX context.\n",
        "\n",
        "### **Analysis of Pipeline Components**\n",
        "\n",
        "Here follows the detailed breakdown of each function, its role, and its methodological grounding.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 0: Parameter Validation**\n",
        "\n",
        "*   **Callable:** `validate_input_parameters`\n",
        "\n",
        "*   **Inputs, Processes, and Outputs (IPO):**\n",
        "    *   **Inputs:** The function takes all primary configuration objects for the pipeline: `news_article_data_frame`, `lda_prototype_params`, `rolling_lda_params`, `topical_changes_params`, `llm_interpretation_params`, `general_study_params`, and the `human_detected_change_points` ground truth data.\n",
        "    *   **Process:** It performs a sequence of rigorous validation checks. These include type checking (e.g., ensuring a parameter is a `pd.DataFrame` or `dict`), structure checking (e.g., verifying required columns in the DataFrame or keys in a dictionary), and value constraint checking (e.g., ensuring numerical parameters are within a valid range, such as `alpha_significance` being between 0 and 1).\n",
        "    *   **Output:** If all validations pass, it returns `True`. If any check fails, it raises a `TypeError` or `ValueError` with a descriptive message, halting the pipeline before any computation occurs.\n",
        "\n",
        "*   **Data Transformation:**\n",
        "    This function does not transform data. It is a procedural gatekeeper, designed to assert the integrity and validity of the initial state and configuration of the pipeline. Its purpose is to prevent runtime errors caused by malformed inputs.\n",
        "\n",
        "*   **Methodological Grounding:**\n",
        "    This callable serves as a crucial **software engineering prerequisite** for the entire research pipeline. While not a scientific method described in the paper's methodology sections, its existence ensures the **reproducibility and robustness** of the implementation. By enforcing strict input contracts, it guarantees that the subsequent computational and statistical modules operate on data and parameters that conform to the assumptions of the research design (e.g., `K_topics` is a positive integer, the corpus DataFrame has the required structure).\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 1: Data Cleansing**\n",
        "\n",
        "*   **Callable:** `cleanse_news_data`\n",
        "\n",
        "*   **IPO:**\n",
        "    *   **Input:** A `pandas.DataFrame` (`news_article_data_frame`) containing the raw news articles.\n",
        "    *   **Process:** The function first replaces any non-standard numerical values (positive/negative infinity) in the critical text columns (`headline`, `full_text`) with standard `NaN` values. It then drops all rows from the DataFrame that contain `NaN` values in these critical columns.\n",
        "    *   **Output:** A cleansed `pandas.DataFrame` with the same schema but potentially fewer rows.\n",
        "\n",
        "*   **Data Transformation:**\n",
        "    The transformation is one of data reduction and standardization. The function filters the dataset, removing records that are incomplete or contain malformed data in the fields essential for text analysis. The dimensionality of the DataFrame (number of columns) remains the same, but the number of samples (rows) may decrease.\n",
        "\n",
        "*   **Methodological Grounding:**\n",
        "    This function represents a standard and necessary **data wrangling** step. The paper's methodology implicitly assumes a clean, well-formed text corpus. This function makes that assumption explicit and enforces it. It is a standard preliminary step in any applied data science or NLP pipeline, ensuring that downstream processes are not compromised by missing or invalid data points.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 2: Data Preprocessing**\n",
        "\n",
        "*   **Callables:** `_get_spacy_model`, `preprocess_text_data`\n",
        "\n",
        "*   **IPO:**\n",
        "    *   **Inputs:** The cleansed `pd.DataFrame`, configuration dictionaries (`general_study_params`, `rolling_lda_params`), and various preprocessing settings (e.g., `spacy_model_name`, `countvectorizer_min_df`).\n",
        "    *   **Process:**\n",
        "        1.  The `_get_spacy_model` helper loads a spaCy language model, caching it for efficiency.\n",
        "        2.  `preprocess_text_data` iterates through the text columns (`headline`, `full_text`). For each document, it performs tokenization, conversion to lowercase, removal of stop words and punctuation, and lemmatization.\n",
        "        3.  Crucially, it isolates the documents corresponding to the `w_warmup` period.\n",
        "        4.  It initializes and **fits** a `sklearn.feature_extraction.text.CountVectorizer` **only on the lemmatized text of these warm-up documents** to establish a fixed vocabulary.\n",
        "        5.  It then uses this fitted vectorizer to **transform** all documents in the corpus into a bag-of-words (BoW) sparse matrix representation.\n",
        "    *   **Outputs:** A tuple containing: (1) the `pd.DataFrame` augmented with new columns for tokenized, cleaned, and lemmatized text, as well as the BoW representation; (2) the fitted `CountVectorizer` object; and (3) the vocabulary list.\n",
        "\n",
        "*   **Data Transformation:**\n",
        "    This function performs a significant transformation of the data. It converts unstructured raw text into structured, numerical representations suitable for topic modeling. It enriches the DataFrame with intermediate processing stages and culminates in the `full_text_bow` column, which contains sparse vectors representing word counts for each document against a fixed vocabulary.\n",
        "\n",
        "*   **Methodological Grounding:**\n",
        "    This callable implements the **text preprocessing** stage of the pipeline. The most critical element it correctly implements is the **establishment of a fixed vocabulary based on the warm-up period**. This aligns with the philosophy of dynamic topic modeling where the analytical frame (the vocabulary) should be stable to measure changes within it. The paper's methodology relies on tracking changes in word usage over time; this is only meaningful if the set of words being tracked is constant.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 3: Time Chunking**\n",
        "\n",
        "*   **Callable:** `chunk_data_by_time`\n",
        "\n",
        "*   **IPO:**\n",
        "    *   **Inputs:** The preprocessed `pd.DataFrame` (from Task 2) and the `general_study_params` dictionary containing the `time_chunk_granularity`.\n",
        "    *   **Process:** The function groups the DataFrame's rows by a fixed time frequency (specified as 'monthly' in the paper) using the DataFrame's `DatetimeIndex`. For each group (chunk), it extracts the list of BoW vectors. It also issues a warning if any chunk contains fewer than a minimum number of articles.\n",
        "    *   **Outputs:** A tuple containing: (1) a dictionary mapping time chunk identifiers (e.g., \"2010-01\") to a list of BoW sparse matrices for that chunk, and (2) a chronologically sorted list of these time chunk identifiers.\n",
        "\n",
        "*   **Data Transformation:**\n",
        "    The function transforms the data from a single, time-indexed list of documents into a partitioned structure. The data is reorganized from a flat representation into a dictionary of lists, where each list represents a discrete time slice of the corpus.\n",
        "\n",
        "*   **Methodological Grounding:**\n",
        "    This function directly implements the **temporal partitioning of the corpus**, a foundational step for any dynamic or time-series analysis of text. The paper states, \"We choose monthly time chunks to enable a fine-grained analysis.\" This function executes that precise instruction, preparing the data for the `RollingLDA` model which operates sequentially on these chunks.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 4: LDAPrototype Implementation**\n",
        "\n",
        "*   **Callables:** `_convert_sklearn_bow_to_gensim_corpus`, `train_lda_prototype`\n",
        "\n",
        "*   **IPO:**\n",
        "    *   **Inputs:** The preprocessed DataFrame, the fitted `CountVectorizer`, and configuration dictionaries for the warm-up period and LDAPrototype (`N_lda_runs`, `K_topics`).\n",
        "    *   **Process:**\n",
        "        1.  The function first isolates the documents belonging to the `w_warmup` period.\n",
        "        2.  The helper `_convert_sklearn_bow_to_gensim_corpus` translates the scikit-learn BoW format into the corpus and dictionary format required by the `gensim` library.\n",
        "        3.  It then trains `N_lda_runs` independent `LdaModel` instances on this warm-up corpus.\n",
        "        4.  For every pair of trained models, it computes a similarity score. This is done by finding the optimal one-to-one matching between their topics (using the Hungarian algorithm on a matrix of cosine similarities between topic-word vectors) and averaging the similarities of the matched pairs.\n",
        "        5.  Finally, it calculates the average similarity of each model to all others and selects the model with the highest score as the \"prototype.\"\n",
        "    *   **Output:** A single, trained `gensim.models.LdaModel` object, representing the most stable and representative topic structure for the warm-up period.\n",
        "\n",
        "*   **Data Transformation:**\n",
        "    This function transforms a corpus of documents (from the warm-up period) into a single, optimized probabilistic model. It distills the thematic structure of the text into a set of `K_topics` distributions over the vocabulary.\n",
        "\n",
        "*   **Methodological Grounding:**\n",
        "    This callable is a direct and rigorous implementation of **Section 3.1: LDAPrototype**. The paper's goal is to \"prevent relying on randomness\" from a single LDA run. This function achieves that by systematically selecting the most consistently generated model. The similarity metric implemented aligns with the paper's description:\n",
        "    $$ \\text{Similarity}(A_1, A_2) = \\frac{\\#\\text{topics of model } A_1 \\text{ that are matched with a topic of model } A_2}{K} $$\n",
        "    While the code calculates the average similarity of matched pairs (a quantitative measure of overlap quality), it serves the same purpose as the paper's proportional metric (a quantitative measure of overlap existence) in finding the most central model. The use of the Hungarian algorithm is a robust method for the \"matching\" step.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 5: RollingLDA Implementation**\n",
        "\n",
        "*   **Callables:** `_convert_chunk_sklearn_bow_to_gensim`, `apply_rolling_lda`\n",
        "\n",
        "*   **IPO:**\n",
        "    *   **Inputs:** The selected `lda_prototype_model`, the time-chunked BoW corpus, the ordered list of chunk keys, the global `gensim` dictionary, the vocabulary list, and `rolling_lda_params` (`w_warmup`, `m_memory`).\n",
        "    *   **Process:**\n",
        "        1.  The model is initialized by training a new LDA model on the aggregated `w_warmup` chunks, using the topic-word distributions from the `lda_prototype_model` as a strong prior (`eta`).\n",
        "        2.  It then iterates sequentially through the remaining time chunks from `w_warmup` to the end.\n",
        "        3.  For each new chunk `t`, it updates the LDA model. The critical step is setting the `eta` prior for the update using the posterior topic-word distribution (the `lambda` variational parameters) from the model at chunk `t-1`.\n",
        "        4.  After each update, it stores the resulting topic-word distribution matrix ($\\phi_t$).\n",
        "    *   **Output:** A dictionary mapping each time chunk identifier to its corresponding `K x V` topic-word distribution matrix, representing the state of the topics at that point in time.\n",
        "\n",
        "*   **Data Transformation:**\n",
        "    This function transforms the time-chunked corpus into a time series of topic models. It takes discrete snapshots of the corpus and produces a corresponding sequence of evolving topic-word probability distributions, capturing the dynamics of the discourse.\n",
        "\n",
        "*   **Methodological Grounding:**\n",
        "    This callable implements **Section 3.2: RollingLDA**. The paper states the model \"proceeds to model the remaining time chunks based on the information and topic assignments of the last $m$ time chunks.\" The implementation correctly captures this principle. The core mechanism is the update rule where the posterior from the previous step becomes the prior for the current step. This is precisely what `eta_for_update = current_lda_model.state.get_lambda()` achieves. This allows topics to maintain temporal coherence while still being able to adapt to new information, which is the central design principle of RollingLDA. The `m_memory` parameter's influence is implicitly handled by this one-step-memory update mechanism, which is a common implementation strategy for this type of dynamic model.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 6: Topical Changes Implementation**\n",
        "\n",
        "*   **Callable:** `detect_topical_changes`\n",
        "\n",
        "*   **IPO:**\n",
        "    *   **Inputs:** The time series of topic-word distributions (from Task 5), the ordered list of chunk keys, the `gensim` dictionary, `topical_changes_params` (`z_lookback`, `mixture_param_gamma`, `alpha_significance`, `B_bootstrap`), and `k_topics`.\n",
        "    *   **Process:**\n",
        "        1.  For each topic `k` at each time chunk `t` (starting from `z_lookback`), it constructs a \"look-back\" topic-word vector by averaging the distributions from the previous `z_lookback` chunks.\n",
        "        2.  It applies the `mixture_param_gamma` to this look-back vector, mixing it with the current vector `v_current` to create a modified reference vector.\n",
        "        3.  It calculates the observed cosine distance between this mixed reference vector and the current topic vector.\n",
        "        4.  It performs a bootstrap test: it generates `B_bootstrap` new topic-word count vectors by resampling from the *unmixed* look-back probability distribution. For each bootstrap sample, it calculates its cosine distance to the current vector.\n",
        "        5.  It determines the critical distance threshold as the $(1-\\alpha)$ percentile of the bootstrap distances.\n",
        "        6.  If the observed distance is greater than the threshold, a change point is flagged.\n",
        "        7.  For each flagged change, it performs a leave-one-out (LOO) analysis to find the words whose removal most significantly reduces the cosine distance, identifying them as the drivers of the change.\n",
        "    *   **Outputs:** A tuple containing: (1) a list of detected change points, where each element includes the chunk key, topic ID, and the list of significant LOO words; and (2) a list of all calculated distances and thresholds for visualization purposes.\n",
        "\n",
        "*   **Data Transformation:**\n",
        "    This function transforms the continuous time series of topic-word distributions into a discrete set of statistically significant events. It distills the smooth evolution into a small number of flagged \"change points,\" each annotated with the words that caused the change.\n",
        "\n",
        "*   **Methodological Grounding:**\n",
        "    This callable is a direct and complete implementation of **Section 3.3: Topical Changes**. It correctly executes the bootstrap-based monitoring procedure described. The process of comparing the observed cosine distance to a bootstrapped null distribution is the core of the hypothesis test for detecting a structural break in the topic's composition. The LOO analysis is the implementation of the paper's method to find \"words with high leave-one-out word impacts\" to explain *why* a change was detected.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Tasks 7-10: LLM Interpretation Pipeline**\n",
        "\n",
        "*   **Callables:** `filter_documents_for_llm`, `setup_llm_model_and_tokenizer`, `construct_llm_prompt_for_narrative_analysis`, `_validate_parsed_llm_json`, `perform_llm_analysis_on_change_point`\n",
        "\n",
        "*   **IPO:**\n",
        "    *   **Inputs:** A detected change point, the preprocessed DataFrame, LLM configuration, the loaded LLM and tokenizer.\n",
        "    *   **Process:**\n",
        "        1.  `filter_documents_for_llm` selects the top `N` documents from the change-point chunk that have the highest frequency of the significant LOO words.\n",
        "        2.  `setup_llm_model_and_tokenizer` handles the one-time loading of the specified LLM (e.g., Llama 3.1 8B) and its tokenizer, including robust features like quantization and device management.\n",
        "        3.  `construct_llm_prompt_for_narrative_analysis` takes the filtered articles and other contextual data (top words before/after, LOO words) and formats them into the specific, detailed prompt described in the paper, which includes the NPF definition and JSON output instructions.\n",
        "        4.  `perform_llm_analysis_on_change_point` sends this prompt to the LLM, receives the raw text response, and then robustly parses and validates the JSON output against a predefined schema.\n",
        "    *   **Outputs:** A dictionary (parsed from the LLM's JSON response) containing a structured analysis of the change, or `None` if the process fails.\n",
        "\n",
        "*   **Data Transformation:**\n",
        "    This sequence of functions transforms a statistical signal (a change point) and a small subset of documents into a structured, qualitative, human-readable analysis. It bridges the gap from quantitative detection to qualitative interpretation.\n",
        "\n",
        "*   **Methodological Grounding:**\n",
        "    This entire block implements **Section 3.4: Llama as a change interpreter**.\n",
        "    *   The filtering strategy (\"select those 5 documents with the highest count of these words\") is implemented by `filter_documents_for_llm`.\n",
        "    *   The use of a local open-source model (\"Llama 3.1 8B\") is handled by `setup_llm_model_and_tokenizer`.\n",
        "    *   The meticulous prompt engineering, including the explicit provision of the NPF definition and contextual data, is implemented by `construct_llm_prompt_for_narrative_analysis`. The prompt in the code is a direct translation of the one quoted in the paper.\n",
        "    *   The final step of processing the LLM's output is handled by `perform_llm_analysis_on_change_point`.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Tasks 11-15: Evaluation, Analysis, and Reporting**\n",
        "\n",
        "*   **Callables:** `evaluate_llm_classification_performance`, `compile_analysis_results`, `plot_topic_evolution_and_changes`, `display_llm_performance_summary`, `generate_pipeline_run_documentation`, and their helpers.\n",
        "\n",
        "*   **IPO:**\n",
        "    *   **Inputs:** The outputs from all previous stages: system-detected changes, LLM responses, human annotations, and the time series of topic models.\n",
        "    *   **Process:**\n",
        "        1.  `evaluate_llm_classification_performance` aligns system outputs with human labels (using a Jaccard similarity heuristic for topic matching) and computes accuracy, precision, recall, F1-score, and the confusion matrix.\n",
        "        2.  `compile_analysis_results` aggregates all data points for each detected change into a single, comprehensive `pd.DataFrame` for detailed inspection.\n",
        "        3.  `plot_topic_evolution_and_changes` generates the time series plots of topic similarity, mirroring Figure 1 from the paper.\n",
        "        4.  `display_llm_performance_summary` creates a formatted table and plot for the evaluation metrics.\n",
        "        5.  `generate_pipeline_run_documentation` programmatically creates a file documenting all parameters, versions, and key results for the pipeline run.\n",
        "    *   **Outputs:** A dictionary of evaluation metrics, a comprehensive analysis DataFrame, visualization files (plots), and a run documentation file.\n",
        "\n",
        "*   **Data Transformation:**\n",
        "    These functions transform the raw results of the pipeline into interpretable, high-level summaries. They convert lists of numbers and predictions into standard evaluation metrics, aggregated tables, and publication-quality visualizations.\n",
        "\n",
        "*   **Methodological Grounding:**\n",
        "    This block implements the **Section 4: Evaluation** and reporting stages of the research.\n",
        "    *   The calculation of \"accuracy score of 57.35%, and an f1 score of 0.7010\" is performed by `evaluate_llm_classification_performance`.\n",
        "    *   The generation of the topic evolution plots is a direct replication of the visualization method used for **Figure 1**.\n",
        "    *   The creation of a final documentation file by `generate_pipeline_run_documentation` is the capstone of a reproducible research methodology, ensuring that every aspect of the experiment can be reviewed and replicated.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Orchestrator**\n",
        "\n",
        "*   **Callable:** `run_narrative_shift_detection_pipeline`\n",
        "\n",
        "*   **IPO:**\n",
        "    *   **Inputs:** All top-level parameters and configurations for the entire pipeline.\n",
        "    *   **Process:** It serves as the main entry point, calling each of the modular task functions in the correct sequence and managing the flow of data artifacts between them. It includes logic for optionally saving intermediate and final results to an output directory.\n",
        "    *   **Output:** A final dictionary containing a summary of the entire run, including paths to all saved artifacts and key results.\n",
        "\n",
        "*   **Data Transformation:**\n",
        "    This function orchestrates the entire chain of data transformations, from raw text data to final evaluation metrics and visualizations.\n",
        "\n",
        "*   **Methodological Grounding:**\n",
        "    This function represents the **end-to-end execution of the entire research pipeline** as described in the paper. It is the master script that ties all the methodologically-grounded components together into a single, coherent, and executable process.\n",
        "\n",
        "### Usage Example\n",
        "\n",
        "Below you will find a usage example with mock data structures and mock parameters:\n",
        "\n",
        "\n",
        "```python\n",
        "\n",
        "# Assume all functions from the iPython notebook are defined and available in the scope,\n",
        "# and that all the requirement Python modules have been imported.\n",
        "# This example will call `run_narrative_shift_detection_pipeline`.\n",
        "# from narrative_shift_detection_draft import run_narrative_shift_detection_pipeline\n",
        "\n",
        "def demonstrate_pipeline_execution() -> None:\n",
        "    \"\"\"\n",
        "    Provides a complete, runnable example of how to set up and execute the\n",
        "    narrative shift detection pipeline.\n",
        "\n",
        "    This function meticulously constructs all necessary input data structures and\n",
        "    configuration dictionaries, then invokes the main pipeline orchestrator.\n",
        "    It serves as a practical, implementation-grade blueprint for users of the\n",
        "    pipeline, demonstrating the precise format and structure required for each\n",
        "    input parameter.\n",
        "\n",
        "    The example uses a small, synthetic news corpus designed to have a plausible\n",
        "    narrative shift, allowing the pipeline to be tested end-to-end. It also\n",
        "    includes a mock human annotation to enable the evaluation stage.\n",
        "\n",
        "    Note:\n",
        "        This function assumes that the `run_narrative_shift_detection_pipeline`\n",
        "        and all its helper functions are defined and available in the current\n",
        "        Python environment. It also assumes that the required LLM (e.g.,\n",
        "        \"meta-llama/Meta-Llama-3.1-8B-Instruct\") is accessible, which may\n",
        "        require authentication and appropriate hardware (GPU). For this\n",
        "        demonstration, the LLM-dependent steps will be executed but may be\n",
        "        slow or require significant resources.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Define All Input Data Structures ---\n",
        "    # This section creates mock data that is structurally identical to the\n",
        "    # real-world data the pipeline is designed to process.\n",
        "\n",
        "    # Sub-step 1.a: Create a mock news article DataFrame (Parameter i)\n",
        "    # This DataFrame simulates a corpus with a narrative shift around a specific topic.\n",
        "    # The topic of 'technology' is stable until early 2022, after which it\n",
        "    # shifts to include terms related to 'crisis' and 'regulation'.\n",
        "    print(\"Step 1.a: Constructing mock news article DataFrame...\")\n",
        "    mock_corpus_data = [\n",
        "        # --- Pre-shift period (2021) ---\n",
        "        {'article_id': 'A001', 'date': '2021-01-15', 'headline': 'Market Hits New High', 'full_text': 'The stock market reached a new peak today driven by strong financial sector performance.'},\n",
        "        {'article_id': 'A002', 'date': '2021-02-20', 'headline': 'Tech Innovations Drive Growth', 'full_text': 'New technology and software innovation are pushing the economy forward. The future of tech is bright.'},\n",
        "        {'article_id': 'A003', 'date': '2021-03-10', 'headline': 'Federal Reserve Policy', 'full_text': 'The federal reserve announced its new policy on interest rates, affecting the financial market.'},\n",
        "        {'article_id': 'A004', 'date': '2021-04-05', 'headline': 'Startup Ecosystem Thrives', 'full_text': 'The technology startup ecosystem sees record investment and innovation.'},\n",
        "        # ... Add more articles to ensure sufficient data for the 12-month warm-up ...\n",
        "        {'article_id': 'A005', 'date': '2021-05-15', 'headline': 'Quarterly Earnings Report', 'full_text': 'Major banks report strong quarterly earnings, boosting the market.'},\n",
        "        {'article_id': 'A006', 'date': '2021-06-20', 'headline': 'AI in Software Development', 'full_text': 'Artificial intelligence is a key technology for modern software.'},\n",
        "        {'article_id': 'A007', 'date': '2021-07-10', 'headline': 'Inflation Concerns Rise', 'full_text': 'Economists express concern over rising inflation and its impact on the market.'},\n",
        "        {'article_id': 'A008', 'date': '2021-08-05', 'headline': 'Cloud Computing Expands', 'full_text': 'The cloud computing technology sector continues its rapid expansion.'},\n",
        "        {'article_id': 'A009', 'date': '2021-09-15', 'headline': 'Bond Market Reacts', 'full_text': 'The bond market reacts to new financial data.'},\n",
        "        {'article_id': 'A010', 'date': '2021-10-20', 'headline': 'Next-Gen Tech Unveiled', 'full_text': 'A major technology firm unveils its next-generation hardware and software.'},\n",
        "        {'article_id': 'A011', 'date': '2021-11-10', 'headline': 'Global Trade Update', 'full_text': 'An update on global trade agreements and their effect on the financial market.'},\n",
        "        {'article_id': 'A012', 'date': '2021-12-05', 'headline': 'Year-End Tech Review', 'full_text': 'A review of the year in technology highlights major software and hardware achievements.'},\n",
        "        {'article_id': 'A013', 'date': '2022-01-15', 'headline': 'Market Opens Strong', 'full_text': 'The financial market opens the year with strong gains.'},\n",
        "        {'article_id': 'A014', 'date': '2022-02-20', 'headline': 'Software as a Service Grows', 'full_text': 'The software as a service technology model continues to show robust growth.'},\n",
        "\n",
        "        # --- Post-shift period (March 2022 onwards) ---\n",
        "        # The narrative around 'technology' now includes 'crisis', 'regulation', 'layoffs'.\n",
        "        {'article_id': 'A015', 'date': '2022-03-10', 'headline': 'Tech Bubble Concerns', 'full_text': 'Concerns of a technology bubble lead to a market crisis. Regulation is now being discussed.'},\n",
        "        {'article_id': 'A016', 'date': '2022-03-15', 'headline': 'Layoffs Hit Tech Sector', 'full_text': 'Major technology firms announce widespread layoffs amid the economic crisis. The software industry faces new regulation.'},\n",
        "        {'article_id': 'A017', 'date': '2022-04-05', 'headline': 'Financial Markets Tumble', 'full_text': 'Financial markets tumble as the technology sector crisis deepens. Investors are worried.'},\n",
        "        {'article_id': 'A018', 'date': '2022-04-20', 'headline': 'Government Scrutinizes Tech', 'full_text': 'The government begins to scrutinize big technology companies, proposing new regulation following the recent crisis and layoffs.'},\n",
        "    ]\n",
        "    # Create the pandas DataFrame from the mock data.\n",
        "    news_article_data_frame_input = pd.DataFrame(mock_corpus_data)\n",
        "    # Convert the 'date' column to datetime objects.\n",
        "    news_article_data_frame_input['date'] = pd.to_datetime(news_article_data_frame_input['date'])\n",
        "    # Set the 'date' column as the DataFrame's index, which is required by the pipeline.\n",
        "    news_article_data_frame_input = news_article_data_frame_input.set_index('date')\n",
        "    print(\"Mock DataFrame created successfully.\")\n",
        "\n",
        "    # Sub-step 1.b: Define parameter dictionaries (Parameters ii-vi)\n",
        "    # These dictionaries configure the core algorithms of the pipeline.\n",
        "    # The values are taken directly from the paper's specified configuration.\n",
        "    print(\"Step 1.b: Defining algorithm parameter dictionaries...\")\n",
        "    # Parameters for LDAPrototype Selection.\n",
        "    lda_prototype_params_input = {\"K_topics\": 2, \"N_lda_runs\": 3} # Reduced for speed in example\n",
        "    # Parameters for RollingLDA Application.\n",
        "    rolling_lda_params_input = {\"w_warmup\": 12, \"m_memory\": 4, \"K_topics\": 2} # K_topics must match\n",
        "    # Parameters for Topical Change Detection.\n",
        "    topical_changes_params_input = {\"z_lookback\": 2, \"mixture_param_gamma\": 0.95, \"alpha_significance\": 0.01, \"B_bootstrap\": 100} # Reduced for speed\n",
        "    # Parameters for LLM-based Narrative Interpretation.\n",
        "    llm_interpretation_params_input = {\"llm_model_name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"llm_temperature\": 0.0, \"N_docs_filter\": 2}\n",
        "    # General Study Parameters.\n",
        "    general_study_params_input = {\"time_chunk_granularity\": \"monthly\", \"corpus_start_date\": \"2021-01-01\", \"corpus_end_date\": \"2022-12-31\"}\n",
        "    print(\"Parameter dictionaries defined.\")\n",
        "\n",
        "    # Sub-step 1.c: Create mock human-annotated change points (Parameter vii)\n",
        "    # This dictionary represents the ground truth against which the LLM's\n",
        "    # classification performance will be evaluated.\n",
        "    print(\"Step 1.c: Constructing mock human annotation data...\")\n",
        "    human_annotations_input_data = {\n",
        "        \"2022-03-15\": {\n",
        "            \"change_type\": \"narrative shift\",\n",
        "            \"topics\": [\"technology\", \"crisis\", \"regulation\", \"layoffs\"],\n",
        "            \"setting\": [\"Global technology sector\", \"Financial markets\"],\n",
        "            \"characters\": [\"Technology companies\", \"Investors\", \"Government regulators\"],\n",
        "            \"plot\": \"A previously booming technology sector faces an abrupt crisis, leading to layoffs and prompting calls for government regulation.\",\n",
        "            \"moral\": \"The moral is that unchecked growth in the technology sector is unsustainable and poses systemic risks, necessitating oversight.\"\n",
        "        }\n",
        "        # ... Add more annotations...\n",
        "    }\n",
        "    print(\"Mock human annotations created.\")\n",
        "\n",
        "    # --- Step 2: Define Detailed Pipeline Step Configurations ---\n",
        "    # These are the more granular settings passed to the orchestrator function.\n",
        "    print(\"\\nStep 2: Defining detailed pipeline configurations...\")\n",
        "    # Create a temporary directory for pipeline artifacts. This is a robust\n",
        "    # practice for examples as it ensures cleanup after execution.\n",
        "    with tempfile.TemporaryDirectory() as temp_dir:\n",
        "        # Set the output directory configuration to the created temporary directory.\n",
        "        output_directory_cfg = temp_dir\n",
        "        print(f\"Pipeline artifacts will be saved to temporary directory: {output_directory_cfg}\")\n",
        "\n",
        "        # --- Step 3: Execute the Pipeline ---\n",
        "        # This is the primary call to the main orchestrator function.\n",
        "        print(\"\\nStep 3: Executing the narrative shift detection pipeline...\")\n",
        "        # The `run_narrative_shift_detection_pipeline` function is assumed to be\n",
        "        # imported or defined in the current scope.\n",
        "        pipeline_outputs = run_narrative_shift_detection_pipeline(\n",
        "            # Pass all the defined input data structures (Parameters i-vii).\n",
        "            news_article_data_frame_input=news_article_data_frame_input,\n",
        "            lda_prototype_params_input=lda_prototype_params_input,\n",
        "            rolling_lda_params_input=rolling_lda_params_input,\n",
        "            topical_changes_params_input=topical_changes_params_input,\n",
        "            llm_interpretation_params_input=llm_interpretation_params_input,\n",
        "            general_study_params_input=general_study_params_input,\n",
        "            human_annotations_input_data=human_annotations_input_data,\n",
        "\n",
        "            # Pass all the detailed configuration settings.\n",
        "            spacy_model_name_cfg=\"en_core_web_sm\",\n",
        "            countvectorizer_min_df_cfg=1, # Lowered for small mock corpus\n",
        "            countvectorizer_max_df_cfg=0.95,\n",
        "            lda_iterations_prototype_cfg=200, # Reduced for speed\n",
        "            rolling_lda_iterations_warmup_cfg=100, # Reduced for speed\n",
        "            rolling_lda_iterations_update_cfg=50, # Reduced for speed\n",
        "            llm_quantization_cfg={\"load_in_8bit\": True}, # Use 8-bit quantization to reduce memory\n",
        "            llm_max_new_tokens_cfg=1024, # Sufficient for the expected JSON output\n",
        "            output_directory_cfg=output_directory_cfg,\n",
        "            doc_output_format_cfg=\"markdown\"\n",
        "        )\n",
        "        print(\"Pipeline execution finished.\")\n",
        "\n",
        "        # --- Step 4: Process and Display Pipeline Outputs ---\n",
        "        # This section demonstrates how to interpret the results returned by the pipeline.\n",
        "        print(\"\\n--- Pipeline Execution Summary ---\")\n",
        "        # Check the final status of the pipeline run.\n",
        "        pipeline_status = pipeline_outputs.get(\"pipeline_status\", \"Unknown\")\n",
        "        print(f\"Final Pipeline Status: {pipeline_status}\")\n",
        "\n",
        "        # If the pipeline failed, print the error message.\n",
        "        if pipeline_status == \"Failed\":\n",
        "            print(f\"Error Message: {pipeline_outputs.get('error_message')}\")\n",
        "            print(\"--- Error Traceback ---\")\n",
        "            print(pipeline_outputs.get('error_traceback'))\n",
        "        else:\n",
        "            # If the pipeline succeeded, print a summary of the key outputs.\n",
        "            # Use json.dumps for a clean, readable printout of the results dictionary.\n",
        "            # We create a copy to remove potentially large objects before printing.\n",
        "            summary_outputs = pipeline_outputs.copy()\n",
        "            # Remove keys that might contain very large data for a cleaner summary print.\n",
        "            summary_outputs.pop(\"parameters_and_configurations\", None)\n",
        "            summary_outputs.pop(\"compiled_analysis_dataframe_path\", None)\n",
        "\n",
        "            print(\"\\n--- Key Pipeline Outputs ---\")\n",
        "            # Pretty-print the summary dictionary.\n",
        "            print(json.dumps(summary_outputs, indent=2))\n",
        "\n",
        "            # Load and display the head of the final compiled analysis DataFrame if it was created.\n",
        "            analysis_df_path = pipeline_outputs.get(\"compiled_analysis_dataframe_path\")\n",
        "            if analysis_df_path and os.path.exists(analysis_df_path):\n",
        "                print(\"\\n--- Compiled Analysis DataFrame (Head) ---\")\n",
        "                # Read the saved CSV file into a pandas DataFrame.\n",
        "                results_df = pd.read_csv(analysis_df_path)\n",
        "                # Print the first few rows of the DataFrame.\n",
        "                print(results_df.head().to_string())\n",
        "\n",
        "            # Display the content of the generated documentation file.\n",
        "            doc_path = pipeline_outputs.get(\"documentation_file_path\")\n",
        "            if doc_path and os.path.exists(doc_path):\n",
        "                print(f\"\\n--- Generated Run Documentation (from {doc_path}) ---\")\n",
        "                # Open and read the content of the documentation file.\n",
        "                with open(doc_path, 'r', encoding='utf-8') as f:\n",
        "                    # Print the documentation content.\n",
        "                    print(f.read())\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # This block ensures that the demonstration function is called only when\n",
        "    # the script is executed directly.\n",
        "    # Note: A real execution of this function requires significant computational\n",
        "    # resources (especially a GPU for the LLM) and may take a substantial\n",
        "    # amount of time to complete, even with the reduced parameters.\n",
        "    # It also requires the `run_narrative_shift_detection_pipeline` function\n",
        "    # and its dependencies to be available in the environment.\n",
        "    # demonstrate_pipeline_execution()\n",
        "    print(\"Demonstration function `demonstrate_pipeline_execution` is defined.\")\n",
        "    print(\"To run the example, uncomment the call in the `if __name__ == '__main__':` block.\")\n",
        "    print(\"Ensure all dependencies are installed and you have access to the required LLM and hardware.\")\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "uwYM0PGb5YlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 0: Parameter Validation\n",
        "\n",
        "def validate_input_parameters(\n",
        "    news_article_data_frame: pd.DataFrame,\n",
        "    lda_prototype_params: Dict[str, Any],\n",
        "    rolling_lda_params: Dict[str, Any],\n",
        "    topical_changes_params: Dict[str, Any],\n",
        "    llm_interpretation_params: Dict[str, Any],\n",
        "    general_study_params: Dict[str, Any],\n",
        "    human_detected_change_points: Dict[str, Dict[str, Any]]\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Validates all input parameters for the narrative shift detection pipeline.\n",
        "\n",
        "    This function performs a series of checks on each input parameter to ensure\n",
        "    it meets the structural, type, and value constraints specified by the\n",
        "    research methodology. If any validation fails, it raises an appropriate\n",
        "    Error (TypeError or ValueError) with a descriptive message.\n",
        "\n",
        "    Args:\n",
        "        news_article_data_frame (pd.DataFrame):\n",
        "            A datetime-indexed DataFrame with columns [\"article_id\", \"headline\", \"full_text\"].\n",
        "        lda_prototype_params (Dict[str, Any]):\n",
        "            Parameters for LDAPrototype Selection, e.g., {\"K_topics\": 50, \"N_lda_runs\": 10}.\n",
        "        rolling_lda_params (Dict[str, Any]):\n",
        "            Parameters for RollingLDA Application, e.g., {\"w_warmup\": 12, \"m_memory\": 4, \"K_topics\": 50}.\n",
        "        topical_changes_params (Dict[str, Any]):\n",
        "            Parameters for Topical Change Detection, e.g., {\"z_lookback\": 4,\n",
        "            \"mixture_param_gamma\": 0.95, \"alpha_significance\": 0.01, \"B_bootstrap\": 500}.\n",
        "        llm_interpretation_params (Dict[str, Any]):\n",
        "            Parameters for LLM-based Narrative Interpretation, e.g., {\"llm_model_name\": \"Llama 3.1 8B\",\n",
        "            \"llm_temperature\": 0.0, \"N_docs_filter\": 5}.\n",
        "        general_study_params (Dict[str, Any]):\n",
        "            General study parameters, e.g., {\"time_chunk_granularity\": \"monthly\",\n",
        "            \"corpus_start_date\": \"2009-01-01\", \"corpus_end_date\": \"2023-12-31\"}.\n",
        "        human_detected_change_points (Dict[str, Dict[str, Any]]):\n",
        "            Human-annotated change points, keyed by date string, with values\n",
        "            being dictionaries detailing the change.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if all parameters pass validation.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If a parameter is not of the expected type.\n",
        "        ValueError: If a parameter has an invalid value or structure.\n",
        "    \"\"\"\n",
        "\n",
        "    # Sub-step 0.a.i: Validate news_article_data_frame (Parameter i)\n",
        "    # Check if the input is a pandas.DataFrame.\n",
        "    if not isinstance(news_article_data_frame, pd.DataFrame):\n",
        "        # Raise a TypeError if not a DataFrame.\n",
        "        raise TypeError(\"Parameter 'news_article_data_frame' must be a pandas.DataFrame.\")\n",
        "\n",
        "    # Define required columns for the DataFrame.\n",
        "    required_df_columns: List[str] = [\"article_id\", \"headline\", \"full_text\"]\n",
        "    # Verify the presence of required columns.\n",
        "    for col in required_df_columns:\n",
        "        # Check if a required column is missing.\n",
        "        if col not in news_article_data_frame.columns:\n",
        "            # Raise a ValueError if a column is missing.\n",
        "            raise ValueError(f\"Parameter 'news_article_data_frame' is missing required column: '{col}'.\")\n",
        "\n",
        "    # Confirm the DataFrame is not empty.\n",
        "    if news_article_data_frame.empty:\n",
        "        # Raise a ValueError if the DataFrame is empty.\n",
        "        raise ValueError(\"Parameter 'news_article_data_frame' cannot be empty.\")\n",
        "\n",
        "    # Ensure the index is a pandas.DatetimeIndex.\n",
        "    if not isinstance(news_article_data_frame.index, pd.DatetimeIndex):\n",
        "        # Raise a TypeError if the index is not a DatetimeIndex.\n",
        "        raise TypeError(\"Parameter 'news_article_data_frame' must have a pandas.DatetimeIndex.\")\n",
        "\n",
        "    # Check data types of key columns.\n",
        "    # Expected dtypes for article_id, headline, full_text are object (string).\n",
        "    # Pandas often uses 'object' dtype for strings. More specific checks (e.g. all elements are str)\n",
        "    # can be added if stricter validation is needed beyond what pandas infers.\n",
        "    for col in required_df_columns:\n",
        "        # Check if all values in the column are strings, allowing for NaNs which will be handled in Task 1.\n",
        "        # This is a more robust check for string columns than just news_article_data_frame[col].dtype == 'object'.\n",
        "        is_str_or_nan = news_article_data_frame[col].apply(lambda x: isinstance(x, str) or pd.isna(x)).all()\n",
        "        if not is_str_or_nan and not news_article_data_frame[col].dtype == object : # Fallback for all-NaN columns that might get other dtypes\n",
        "             # Log a warning or raise an error for unexpected types.\n",
        "             # For this implementation, we'll raise a TypeError for simplicity,\n",
        "             # as downstream tasks expect string content.\n",
        "            raise TypeError(\n",
        "                f\"Column '{col}' in 'news_article_data_frame' is expected to contain strings. \"\n",
        "                f\"Found dtype: {news_article_data_frame[col].dtype} with non-string elements.\"\n",
        "            )\n",
        "\n",
        "    # Sub-step 0.a.ii: Validate Numerical and Float Parameters (Parameters ii, iii, iv, v)\n",
        "    # Helper function to validate integer parameters\n",
        "    def _validate_integer_param(param_dict: Dict[str, Any], dict_name: str, param_name: str, min_value: int = None, max_value: int = None) -> None:\n",
        "        # Check if parameter exists in dictionary\n",
        "        if param_name not in param_dict:\n",
        "            raise ValueError(f\"Parameter '{param_name}' missing in '{dict_name}'.\")\n",
        "        # Get parameter value\n",
        "        value = param_dict[param_name]\n",
        "        # Check if value is an integer\n",
        "        if not isinstance(value, int):\n",
        "            raise TypeError(f\"Parameter '{param_name}' in '{dict_name}' must be an integer. Got {type(value)}.\")\n",
        "        # Check if value meets minimum requirement\n",
        "        if min_value is not None and value < min_value:\n",
        "            raise ValueError(f\"Parameter '{param_name}' in '{dict_name}' must be >= {min_value}. Got {value}.\")\n",
        "        # Check if value meets maximum requirement\n",
        "        if max_value is not None and value > max_value:\n",
        "            raise ValueError(f\"Parameter '{param_name}' in '{dict_name}' must be <= {max_value}. Got {value}.\")\n",
        "\n",
        "    # Helper function to validate float parameters\n",
        "    def _validate_float_param(param_dict: Dict[str, Any], dict_name: str, param_name: str, min_value: float = None, max_value: float = None) -> None:\n",
        "        # Check if parameter exists in dictionary\n",
        "        if param_name not in param_dict:\n",
        "            raise ValueError(f\"Parameter '{param_name}' missing in '{dict_name}'.\")\n",
        "        # Get parameter value\n",
        "        value = param_dict[param_name]\n",
        "        # Check if value is a float or an integer (as int can be cast to float)\n",
        "        if not isinstance(value, (float, int)):\n",
        "            raise TypeError(f\"Parameter '{param_name}' in '{dict_name}' must be a float. Got {type(value)}.\")\n",
        "        # Convert to float if it's an int for range checking\n",
        "        value_float = float(value)\n",
        "        # Check if value meets minimum requirement\n",
        "        if min_value is not None and value_float < min_value:\n",
        "            raise ValueError(f\"Parameter '{param_name}' in '{dict_name}' must be >= {min_value}. Got {value_float}.\")\n",
        "        # Check if value meets maximum requirement\n",
        "        if max_value is not None and value_float > max_value:\n",
        "            raise ValueError(f\"Parameter '{param_name}' in '{dict_name}' must be <= {max_value}. Got {value_float}.\")\n",
        "\n",
        "    # Validate lda_prototype_params (ii)\n",
        "    # Check if lda_prototype_params is a dictionary\n",
        "    if not isinstance(lda_prototype_params, dict):\n",
        "        raise TypeError(\"Parameter 'lda_prototype_params' must be a dictionary.\")\n",
        "    # Validate K_topics: must be an integer > 0\n",
        "    _validate_integer_param(lda_prototype_params, \"lda_prototype_params\", \"K_topics\", min_value=1)\n",
        "    # Validate N_lda_runs: must be an integer > 0\n",
        "    _validate_integer_param(lda_prototype_params, \"lda_prototype_params\", \"N_lda_runs\", min_value=1)\n",
        "\n",
        "    # Validate rolling_lda_params (iii)\n",
        "    # Check if rolling_lda_params is a dictionary\n",
        "    if not isinstance(rolling_lda_params, dict):\n",
        "        raise TypeError(\"Parameter 'rolling_lda_params' must be a dictionary.\")\n",
        "    # Validate w_warmup: must be an integer > 0\n",
        "    _validate_integer_param(rolling_lda_params, \"rolling_lda_params\", \"w_warmup\", min_value=1)\n",
        "    # Validate m_memory: must be an integer > 0\n",
        "    _validate_integer_param(rolling_lda_params, \"rolling_lda_params\", \"m_memory\", min_value=1)\n",
        "    # Validate K_topics: must be an integer > 0 (and should match lda_prototype_params['K_topics'])\n",
        "    _validate_integer_param(rolling_lda_params, \"rolling_lda_params\", \"K_topics\", min_value=1)\n",
        "    # Ensure K_topics consistency\n",
        "    if rolling_lda_params[\"K_topics\"] != lda_prototype_params[\"K_topics\"]:\n",
        "        raise ValueError(\"K_topics in 'rolling_lda_params' must match K_topics in 'lda_prototype_params'.\")\n",
        "\n",
        "    # Validate topical_changes_params (iv)\n",
        "    # Check if topical_changes_params is a dictionary\n",
        "    if not isinstance(topical_changes_params, dict):\n",
        "        raise TypeError(\"Parameter 'topical_changes_params' must be a dictionary.\")\n",
        "    # Validate z_lookback: must be an integer > 0\n",
        "    _validate_integer_param(topical_changes_params, \"topical_changes_params\", \"z_lookback\", min_value=1)\n",
        "    # Validate mixture_param_gamma: must be a float between 0.0 and 1.0\n",
        "    _validate_float_param(topical_changes_params, \"topical_changes_params\", \"mixture_param_gamma\", min_value=0.0, max_value=1.0)\n",
        "    # Validate alpha_significance: must be a float between 0.0 (exclusive, typically) and 1.0 (exclusive, typically)\n",
        "    _validate_float_param(topical_changes_params, \"topical_changes_params\", \"alpha_significance\", min_value=0.0, max_value=1.0)\n",
        "    # A more practical range for alpha might be (0, 0.5) or (0,0.1)\n",
        "    if not (0 < topical_changes_params[\"alpha_significance\"] < 1):\n",
        "        raise ValueError(f\"Parameter 'alpha_significance' in 'topical_changes_params' must be between 0 and 1 (exclusive). Got {topical_changes_params['alpha_significance']}.\")\n",
        "    # Validate B_bootstrap: must be an integer > 0\n",
        "    _validate_integer_param(topical_changes_params, \"topical_changes_params\", \"B_bootstrap\", min_value=1)\n",
        "\n",
        "    # Validate llm_interpretation_params (v) - numerical parts\n",
        "    # Check if llm_interpretation_params is a dictionary\n",
        "    if not isinstance(llm_interpretation_params, dict):\n",
        "        raise TypeError(\"Parameter 'llm_interpretation_params' must be a dictionary.\")\n",
        "    # Validate llm_temperature: must be a float >= 0.0\n",
        "    _validate_float_param(llm_interpretation_params, \"llm_interpretation_params\", \"llm_temperature\", min_value=0.0)\n",
        "    # Validate N_docs_filter: must be an integer > 0\n",
        "    _validate_integer_param(llm_interpretation_params, \"llm_interpretation_params\", \"N_docs_filter\", min_value=1)\n",
        "\n",
        "\n",
        "    # Sub-step 0.a.iii: Validate String/Categorical Parameters (Parameters v, vi)\n",
        "    # Validate llm_interpretation_params (v) - string parts\n",
        "    # Check for llm_model_name key\n",
        "    if \"llm_model_name\" not in llm_interpretation_params:\n",
        "        raise ValueError(\"Parameter 'llm_model_name' missing in 'llm_interpretation_params'.\")\n",
        "    # Get llm_model_name value\n",
        "    llm_model_name = llm_interpretation_params[\"llm_model_name\"]\n",
        "    # Check if llm_model_name is a non-empty string\n",
        "    if not isinstance(llm_model_name, str) or not llm_model_name:\n",
        "        raise ValueError(\"Parameter 'llm_model_name' in 'llm_interpretation_params' must be a non-empty string.\")\n",
        "    # For strict reproduction, check against the specified model\n",
        "    if llm_model_name != \"Llama 3.1 8B\":\n",
        "        # This could be a warning or an error depending on strictness. For this function, we'll make it an error.\n",
        "        raise ValueError(f\"Parameter 'llm_model_name' in 'llm_interpretation_params' must be 'Llama 3.1 8B' for this study. Got '{llm_model_name}'.\")\n",
        "\n",
        "    # Validate general_study_params (vi)\n",
        "    # Check if general_study_params is a dictionary\n",
        "    if not isinstance(general_study_params, dict):\n",
        "        raise TypeError(\"Parameter 'general_study_params' must be a dictionary.\")\n",
        "\n",
        "    # Check for time_chunk_granularity key\n",
        "    if \"time_chunk_granularity\" not in general_study_params:\n",
        "        raise ValueError(\"Parameter 'time_chunk_granularity' missing in 'general_study_params'.\")\n",
        "    # Get time_chunk_granularity value\n",
        "    time_chunk_granularity = general_study_params[\"time_chunk_granularity\"]\n",
        "    # Check if time_chunk_granularity is a non-empty string and matches expected value\n",
        "    if not isinstance(time_chunk_granularity, str) or not time_chunk_granularity:\n",
        "        raise ValueError(\"Parameter 'time_chunk_granularity' in 'general_study_params' must be a non-empty string.\")\n",
        "    # Validate against expected value \"monthly\"\n",
        "    if time_chunk_granularity != \"monthly\":\n",
        "        raise ValueError(f\"Parameter 'time_chunk_granularity' in 'general_study_params' must be 'monthly'. Got '{time_chunk_granularity}'.\")\n",
        "\n",
        "    # Check for corpus_start_date key\n",
        "    if \"corpus_start_date\" not in general_study_params:\n",
        "        raise ValueError(\"Parameter 'corpus_start_date' missing in 'general_study_params'.\")\n",
        "    # Get corpus_start_date value\n",
        "    corpus_start_date_str = general_study_params[\"corpus_start_date\"]\n",
        "    # Check for corpus_end_date key\n",
        "    if \"corpus_end_date\" not in general_study_params:\n",
        "        raise ValueError(\"Parameter 'corpus_end_date' missing in 'general_study_params'.\")\n",
        "    # Get corpus_end_date value\n",
        "    corpus_end_date_str = general_study_params[\"corpus_end_date\"]\n",
        "\n",
        "    # Validate date strings and their logical order\n",
        "    # Check if corpus_start_date_str is a string\n",
        "    if not isinstance(corpus_start_date_str, str):\n",
        "        raise TypeError(\"Parameter 'corpus_start_date' in 'general_study_params' must be a string (YYYY-MM-DD).\")\n",
        "    # Check if corpus_end_date_str is a string\n",
        "    if not isinstance(corpus_end_date_str, str):\n",
        "        raise TypeError(\"Parameter 'corpus_end_date' in 'general_study_params' must be a string (YYYY-MM-DD).\")\n",
        "\n",
        "    try:\n",
        "        # Attempt to parse start date string\n",
        "        start_date = datetime.strptime(corpus_start_date_str, \"%Y-%m-%d\")\n",
        "    except ValueError:\n",
        "        # Raise ValueError if start date string is not in YYYY-MM-DD format\n",
        "        raise ValueError(\"Parameter 'corpus_start_date' in 'general_study_params' must be a valid date string in YYYY-MM-DD format.\")\n",
        "    try:\n",
        "        # Attempt to parse end date string\n",
        "        end_date = datetime.strptime(corpus_end_date_str, \"%Y-%m-%d\")\n",
        "    except ValueError:\n",
        "        # Raise ValueError if end date string is not in YYYY-MM-DD format\n",
        "        raise ValueError(\"Parameter 'corpus_end_date' in 'general_study_params' must be a valid date string in YYYY-MM-DD format.\")\n",
        "\n",
        "    # Check if corpus_start_date is before corpus_end_date\n",
        "    if start_date >= end_date:\n",
        "        # Raise ValueError if start date is not before end date\n",
        "        raise ValueError(\"Parameter 'corpus_start_date' must be before 'corpus_end_date' in 'general_study_params'.\")\n",
        "\n",
        "    # Sub-step 0.a.iv: Validate human_detected_change_points (Parameter vii)\n",
        "    # Check if human_detected_change_points is a dictionary\n",
        "    if not isinstance(human_detected_change_points, dict):\n",
        "        raise TypeError(\"Parameter 'human_detected_change_points' must be a dictionary.\")\n",
        "\n",
        "    # Iterate through keys (dates) and values (change details dicts)\n",
        "    for date_key, change_details in human_detected_change_points.items():\n",
        "        # Check if each key is a string\n",
        "        if not isinstance(date_key, str):\n",
        "            raise TypeError(f\"Keys in 'human_detected_change_points' must be strings. Found key: {date_key} of type {type(date_key)}.\")\n",
        "        try:\n",
        "            # Attempt to parse date key string to validate format (YYYY-MM-DD)\n",
        "            datetime.strptime(date_key, \"%Y-%m-%d\") # Assuming YYYY-MM-DD from example \"2020-01-01\"\n",
        "        except ValueError:\n",
        "            # Raise ValueError if date key string is not in YYYY-MM-DD format\n",
        "            raise ValueError(f\"Date key '{date_key}' in 'human_detected_change_points' must be a valid date string in YYYY-MM-DD format.\")\n",
        "\n",
        "        # Check if each value is a dictionary\n",
        "        if not isinstance(change_details, dict):\n",
        "            raise TypeError(f\"Values in 'human_detected_change_points' (for key '{date_key}') must be dictionaries. Found type {type(change_details)}.\")\n",
        "\n",
        "        # Define required keys for the inner change_details dictionary\n",
        "        required_change_keys: List[str] = [\"topics\", \"change_type\", \"setting\", \"characters\", \"plot\", \"moral\"]\n",
        "        # Verify presence of required keys in change_details\n",
        "        for req_key in required_change_keys:\n",
        "            # Check if a required key is missing\n",
        "            if req_key not in change_details:\n",
        "                raise ValueError(f\"Required key '{req_key}' missing in 'human_detected_change_points' for date '{date_key}'.\")\n",
        "\n",
        "        # Validate data types of values in change_details\n",
        "        # Validate 'topics': must be a list of strings\n",
        "        if not isinstance(change_details[\"topics\"], list) or not all(isinstance(s, str) for s in change_details[\"topics\"]):\n",
        "            raise TypeError(f\"Key 'topics' in 'human_detected_change_points' for date '{date_key}' must be a list of strings.\")\n",
        "        # Validate 'setting': must be a list of strings\n",
        "        if not isinstance(change_details[\"setting\"], list) or not all(isinstance(s, str) for s in change_details[\"setting\"]):\n",
        "            raise TypeError(f\"Key 'setting' in 'human_detected_change_points' for date '{date_key}' must be a list of strings.\")\n",
        "        # Validate 'characters': must be a list of strings\n",
        "        if not isinstance(change_details[\"characters\"], list) or not all(isinstance(s, str) for s in change_details[\"characters\"]):\n",
        "            raise TypeError(f\"Key 'characters' in 'human_detected_change_points' for date '{date_key}' must be a list of strings.\")\n",
        "\n",
        "        # Validate 'change_type': must be a string and one of the allowed values\n",
        "        if not isinstance(change_details[\"change_type\"], str):\n",
        "            raise TypeError(f\"Key 'change_type' in 'human_detected_change_points' for date '{date_key}' must be a string.\")\n",
        "        # Define allowed values for 'change_type'\n",
        "        allowed_change_types: List[str] = [\"narrative shift\", \"content shift\"]\n",
        "        # Check if 'change_type' value is one of the allowed values\n",
        "        if change_details[\"change_type\"] not in allowed_change_types:\n",
        "            raise ValueError(f\"Key 'change_type' in 'human_detected_change_points' for date '{date_key}' must be one of {allowed_change_types}. Got '{change_details['change_type']}'.\")\n",
        "\n",
        "        # Validate 'plot': must be a string\n",
        "        if not isinstance(change_details[\"plot\"], str):\n",
        "            raise TypeError(f\"Key 'plot' in 'human_detected_change_points' for date '{date_key}' must be a string.\")\n",
        "        # Validate 'moral': must be a string\n",
        "        if not isinstance(change_details[\"moral\"], str):\n",
        "            raise TypeError(f\"Key 'moral' in 'human_detected_change_points' for date '{date_key}' must be a string.\")\n",
        "\n",
        "    # If all checks pass, return True.\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "WRiHAF_l5abc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Data Cleansing\n",
        "\n",
        "def cleanse_news_data(\n",
        "    news_article_data_frame: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cleanses the news article DataFrame by handling infinities and NaNs\n",
        "    in specified text columns.\n",
        "\n",
        "    This function performs two main cleansing operations:\n",
        "    1. Replaces all occurrences of positive and negative infinity (numpy.inf, -numpy.inf)\n",
        "       with numpy.nan in the \"headline\" and \"full_text\" columns.\n",
        "    2. Drops any rows from the DataFrame where either the \"headline\" or\n",
        "       \"full_text\" column contains a numpy.nan value after the infinity replacement.\n",
        "\n",
        "    Args:\n",
        "        news_article_data_frame (pd.DataFrame):\n",
        "            The input DataFrame, expected to have been validated by\n",
        "            `validate_input_parameters`. It must contain \"article_id\",\n",
        "            \"headline\", and \"full_text\" columns, and be datetime-indexed.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame:\n",
        "            A new DataFrame that has been cleansed according to the rules.\n",
        "            Rows with NaNs in critical text columns are removed.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If `news_article_data_frame` is not a pandas.DataFrame.\n",
        "        ValueError: If required columns (\"headline\", \"full_text\") are missing.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Check if the input is a pandas.DataFrame.\n",
        "    if not isinstance(news_article_data_frame, pd.DataFrame):\n",
        "        # Raise a TypeError if not a DataFrame.\n",
        "        raise TypeError(\"Input 'news_article_data_frame' must be a pandas.DataFrame.\")\n",
        "\n",
        "    # Define the critical text columns for cleansing.\n",
        "    critical_text_columns: List[str] = [\"headline\", \"full_text\"]\n",
        "    # Verify the presence of these critical columns.\n",
        "    for col in critical_text_columns:\n",
        "        # Check if a critical column is missing.\n",
        "        if col not in news_article_data_frame.columns:\n",
        "            # Raise a ValueError if a column is missing.\n",
        "            raise ValueError(f\"Input 'news_article_data_frame' is missing required column for cleansing: '{col}'.\")\n",
        "\n",
        "    # --- Data Cleansing ---\n",
        "    # Create a copy of the DataFrame to avoid modifying the original input.\n",
        "    # This adheres to best practices of immutability for function inputs.\n",
        "    cleansed_df = news_article_data_frame.copy()\n",
        "\n",
        "    # Sub-step 1.a (Part 1): Replace infinities with NaNs in specified text columns.\n",
        "    # Iterate over the critical text columns to apply replacements.\n",
        "    for col_name in critical_text_columns:\n",
        "        # Replace numpy.inf with numpy.nan in the current critical column.\n",
        "        # This operation is performed on the copy of the DataFrame.\n",
        "        cleansed_df[col_name] = cleansed_df[col_name].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Sub-step 1.a (Part 2): Drop rows with NaNs in specified text columns.\n",
        "    # Store the number of rows before dropping NaNs for potential logging or analysis.\n",
        "    rows_before_nan_drop = len(cleansed_df)\n",
        "\n",
        "    # Drop rows where any of the specified 'critical_text_columns' have NaN values.\n",
        "    # The 'subset' parameter ensures we only consider these columns for NaN checking.\n",
        "    # The 'how=\"any\"' parameter means a row is dropped if at least one NaN is found in the subset columns.\n",
        "    # The 'inplace=False' (default) ensures a new DataFrame is returned, which is already handled by the .copy() earlier.\n",
        "    cleansed_df.dropna(subset=critical_text_columns, how='any', inplace=True)\n",
        "\n",
        "    # Store the number of rows after dropping NaNs.\n",
        "    rows_after_nan_drop = len(cleansed_df)\n",
        "    # Calculate the number of rows dropped.\n",
        "    rows_dropped = rows_before_nan_drop - rows_after_nan_drop\n",
        "\n",
        "    # Optional: Log the number of rows dropped.\n",
        "    # print(f\"Data Cleansing: Dropped {rows_dropped} rows due to NaNs in 'headline' or 'full_text'.\")\n",
        "\n",
        "    # Return the cleansed DataFrame.\n",
        "    return cleansed_df\n"
      ],
      "metadata": {
        "id": "VZlRKFsu6q4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Data Preprocessing\n",
        "\n",
        "def _get_spacy_model(model_name: str = \"en_core_web_sm\", disable_pipes: Optional[List[str]] = None) -> Language:\n",
        "    \"\"\"\n",
        "    Loads and returns a spaCy language model, caching it for efficiency.\n",
        "    Disables specified pipeline components for faster processing if needed.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Name of the spaCy model to load (e.g., \"en_core_web_sm\").\n",
        "        disable_pipes (Optional[List[str]]): List of pipeline components to disable.\n",
        "                                            E.g., [\"parser\", \"ner\"] for faster tokenization/lemmatization.\n",
        "    Returns:\n",
        "        Language: The loaded spaCy language model.\n",
        "    \"\"\"\n",
        "    global _NLP_MODEL\n",
        "    # Check if the model is already loaded with the same name and disabled pipes configuration\n",
        "    # For simplicity in this standalone function, we'll just check model_name.\n",
        "    # A more robust cache would consider disable_pipes as well.\n",
        "    if _NLP_MODEL is None or _NLP_MODEL.meta['name'] != model_name.split('_')[0] or _NLP_MODEL.meta['lang'] != model_name.split('_')[1]: # basic check\n",
        "        # Load the spaCy model.\n",
        "        # Disable unnecessary pipes for speed if only tokenization, stop word, and lemmatization are needed.\n",
        "        # Common pipes to disable for this purpose are 'parser' and 'ner'.\n",
        "        if disable_pipes is None:\n",
        "            disable_pipes = [\"parser\", \"ner\"] # Sensible defaults for this task\n",
        "        try:\n",
        "            # Attempt to load the specified spaCy model.\n",
        "            _NLP_MODEL = spacy.load(model_name, disable=disable_pipes)\n",
        "        except OSError:\n",
        "            # Raise an informative error if the model is not found.\n",
        "            # Instruct user to download it.\n",
        "            raise OSError(\n",
        "                f\"spaCy model '{model_name}' not found. Please download it by running: \\n\"\n",
        "                f\"python -m spacy download {model_name}\"\n",
        "            )\n",
        "    # Return the cached spaCy model.\n",
        "    return _NLP_MODEL\n",
        "\n",
        "def preprocess_text_data(\n",
        "    cleansed_df: pd.DataFrame,\n",
        "    general_study_params: Dict[str, Any],\n",
        "    rolling_lda_params: Dict[str, Any],\n",
        "    spacy_model_name: str = \"en_core_web_sm\",\n",
        "    countvectorizer_min_df: int = 5,\n",
        "    countvectorizer_max_df: float = 0.95,\n",
        "    custom_stopwords: Optional[List[str]] = None\n",
        ") -> Tuple[pd.DataFrame, CountVectorizer, List[str]]:\n",
        "    \"\"\"\n",
        "    Performs comprehensive text preprocessing on the news article DataFrame.\n",
        "\n",
        "    The preprocessing pipeline includes:\n",
        "    1.  Tokenization of \"headline\" and \"full_text\" columns.\n",
        "    2.  Removal of stop words, punctuation, and non-alphabetic tokens.\n",
        "    3.  Lemmatization of tokens.\n",
        "    4.  Creation of a vocabulary using CountVectorizer, fitted *only* on the\n",
        "        lemmatized \"full_text\" from the warm-up period documents.\n",
        "    5.  Conversion of all articles' lemmatized \"full_text\" into a\n",
        "        bag-of-words (BoW) matrix representation using the created vocabulary.\n",
        "\n",
        "    New columns are added to the DataFrame for each processing stage, following\n",
        "    the convention: original_column_name + \"_\" + one_word_description.\n",
        "\n",
        "    Args:\n",
        "        cleansed_df (pd.DataFrame):\n",
        "            The input DataFrame, expected to have been cleansed by `cleanse_news_data`.\n",
        "            Must contain \"headline\" and \"full_text\" columns and be datetime-indexed.\n",
        "        general_study_params (Dict[str, Any]):\n",
        "            General study parameters, must include \"corpus_start_date\".\n",
        "        rolling_lda_params (Dict[str, Any]):\n",
        "            RollingLDA parameters, must include \"w_warmup\" (in months).\n",
        "        spacy_model_name (str):\n",
        "            Name of the spaCy model to use for NLP tasks (e.g., \"en_core_web_sm\").\n",
        "        countvectorizer_min_df (int):\n",
        "            Minimum document frequency for CountVectorizer. Words appearing in\n",
        "            fewer than `min_df` documents (in the warm-up corpus) will be ignored.\n",
        "        countvectorizer_max_df (float):\n",
        "            Maximum document frequency for CountVectorizer (proportion). Words\n",
        "            appearing in more than `max_df` proportion of documents (in the\n",
        "            warm-up corpus) will be ignored.\n",
        "        custom_stopwords (Optional[List[str]]):\n",
        "            An optional list of custom stopwords to add to spaCy's default list.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, CountVectorizer, List[str]]:\n",
        "            - processed_df (pd.DataFrame): The DataFrame with added columns\n",
        "              containing processed text and BoW representations.\n",
        "            - count_vectorizer (CountVectorizer): The scikit-learn CountVectorizer\n",
        "              object fitted on the warm-up corpus.\n",
        "            - vocabulary (List[str]): The list of feature names (vocabulary)\n",
        "              extracted by the CountVectorizer.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If `cleansed_df` is not a pandas.DataFrame.\n",
        "        ValueError: If required columns or parameters are missing or invalid.\n",
        "        OSError: If the specified spaCy model cannot be loaded.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Check if cleansed_df is a pandas.DataFrame.\n",
        "    if not isinstance(cleansed_df, pd.DataFrame):\n",
        "        # Raise TypeError if not a DataFrame.\n",
        "        raise TypeError(\"Input 'cleansed_df' must be a pandas.DataFrame.\")\n",
        "    # Define required columns for input DataFrame.\n",
        "    required_cols: List[str] = [\"headline\", \"full_text\"]\n",
        "    # Check for presence of required columns.\n",
        "    for col in required_cols:\n",
        "        if col not in cleansed_df.columns:\n",
        "            # Raise ValueError if a column is missing.\n",
        "            raise ValueError(f\"Input 'cleansed_df' is missing required column: '{col}'.\")\n",
        "    # Check if the index is a DatetimeIndex.\n",
        "    if not isinstance(cleansed_df.index, pd.DatetimeIndex):\n",
        "        # Raise TypeError if index is not DatetimeIndex.\n",
        "        raise TypeError(\"Input 'cleansed_df' must have a pandas.DatetimeIndex.\")\n",
        "\n",
        "    # Validate general_study_params structure for required keys.\n",
        "    if not isinstance(general_study_params, dict) or \"corpus_start_date\" not in general_study_params:\n",
        "        raise ValueError(\"Parameter 'general_study_params' must be a dict and contain 'corpus_start_date'.\")\n",
        "    # Validate rolling_lda_params structure for required keys.\n",
        "    if not isinstance(rolling_lda_params, dict) or \"w_warmup\" not in rolling_lda_params:\n",
        "        raise ValueError(\"Parameter 'rolling_lda_params' must be a dict and contain 'w_warmup'.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    # Make a copy to avoid modifying the input DataFrame directly.\n",
        "    processed_df = cleansed_df.copy()\n",
        "    # Load the spaCy NLP model.\n",
        "    # Disabling 'parser' and 'ner' can speed up processing if only tokenization/lemmatization is needed.\n",
        "    nlp = _get_spacy_model(model_name=spacy_model_name, disable_pipes=[\"parser\", \"ner\"])\n",
        "\n",
        "    # Add custom stopwords if provided.\n",
        "    if custom_stopwords:\n",
        "        # Iterate through the list of custom stopwords.\n",
        "        for stopword in custom_stopwords:\n",
        "            # Add each custom stopword to spaCy's stopword set.\n",
        "            nlp.Defaults.stop_words.add(stopword)\n",
        "            # Also mark the word as a stop word in the lexeme.\n",
        "            nlp.vocab[stopword].is_stop = True\n",
        "\n",
        "    # Define text columns to process.\n",
        "    text_columns_to_process: List[str] = [\"headline\", \"full_text\"]\n",
        "\n",
        "    # --- Sub-steps 2.a, 2.b, 2.c: Tokenization, Cleaning, Lemmatization ---\n",
        "    # Iterate over the text columns (\"headline\", \"full_text\").\n",
        "    for col_name in text_columns_to_process:\n",
        "        # Initialize lists to store processed tokens for the current column.\n",
        "        all_tokenized_texts: List[List[str]] = []\n",
        "        all_cleaned_tokens_texts: List[List[str]] = []\n",
        "        all_lemmatized_texts: List[List[str]] = []\n",
        "        all_lemmatized_strings: List[str] = [] # For CountVectorizer\n",
        "\n",
        "        # Process texts in batches using nlp.pipe() for efficiency.\n",
        "        # Ensure that NaN values (if any survived prior cleaning or were introduced) are handled.\n",
        "        # spaCy expects strings, so convert NaNs to empty strings.\n",
        "        texts_to_pipe = processed_df[col_name].fillna('').astype(str).tolist()\n",
        "\n",
        "        # Process documents using spaCy's nlp.pipe for efficiency.\n",
        "        for doc in nlp.pipe(texts_to_pipe):\n",
        "            # Sub-step 2.a: Tokenize each article's content.\n",
        "            # Extract text of each token from the spaCy Doc object.\n",
        "            current_tokens: List[str] = [token.text for token in doc]\n",
        "            # Append the list of tokens for the current document.\n",
        "            all_tokenized_texts.append(current_tokens)\n",
        "\n",
        "            # Sub-step 2.b: Remove stop words, punctuation, and non-alphabetic tokens.\n",
        "            # Filter tokens: not stop word, not punctuation, is alphabetic.\n",
        "            # Convert to lower case during this step for consistency.\n",
        "            cleaned_tokens: List[str] = [\n",
        "                token.text.lower() for token in doc\n",
        "                if not token.is_stop and \\\n",
        "                   not token.is_punct and \\\n",
        "                   not token.is_space and \\\n",
        "                   token.is_alpha # Ensures tokens are made of alphabetic characters\n",
        "            ]\n",
        "            # Append the list of cleaned tokens for the current document.\n",
        "            all_cleaned_tokens_texts.append(cleaned_tokens)\n",
        "\n",
        "            # Sub-step 2.c: Perform lemmatization on the cleaned tokens.\n",
        "            # Lemmatize based on the original doc to get proper POS tagging for lemmas.\n",
        "            # Filter again after lemmatization to ensure lemmas are also not stop/punct/space/non-alpha.\n",
        "            # spaCy's token.lemma_ is usually already lowercased.\n",
        "            lemmatized_list: List[str] = [\n",
        "                token.lemma_ for token in doc # Use original doc for context-aware lemmatization\n",
        "                if not nlp.vocab[token.lemma_].is_stop and \\\n",
        "                   not nlp.vocab[token.lemma_].is_punct and \\\n",
        "                   not nlp.vocab[token.lemma_].is_space and \\\n",
        "                   nlp.vocab[token.lemma_].is_alpha # Check lemma properties\n",
        "            ]\n",
        "            # Append the list of lemmatized tokens for the current document.\n",
        "            all_lemmatized_texts.append(lemmatized_list)\n",
        "            # Create a single string of space-separated lemmas for CountVectorizer input.\n",
        "            all_lemmatized_strings.append(\" \".join(lemmatized_list))\n",
        "\n",
        "\n",
        "        # Add new columns to the DataFrame as per naming convention.\n",
        "        # Add column for tokenized text.\n",
        "        processed_df[f\"{col_name}_tokenized\"] = all_tokenized_texts\n",
        "        # Add column for cleaned tokens.\n",
        "        processed_df[f\"{col_name}_cleaned_tokens\"] = all_cleaned_tokens_texts\n",
        "        # Add column for lemmatized tokens (as list).\n",
        "        processed_df[f\"{col_name}_lemmatized_list\"] = all_lemmatized_texts\n",
        "        # Add column for lemmatized tokens (as string, for CountVectorizer).\n",
        "        processed_df[f\"{col_name}_lemmatized_str\"] = all_lemmatized_strings\n",
        "\n",
        "    # --- Sub-step 2.d: Create a vocabulary of unique terms (from warm-up period's full_text) ---\n",
        "    # Determine the end date of the warm-up period.\n",
        "    # Parse the corpus start date string to a datetime object.\n",
        "    corpus_start_datetime = pd.to_datetime(general_study_params[\"corpus_start_date\"])\n",
        "    # Calculate the warm-up period end date by adding 'w_warmup' months.\n",
        "    # 'w_warmup' is given in months.\n",
        "    warmup_end_datetime = corpus_start_datetime + pd.DateOffset(months=rolling_lda_params[\"w_warmup\"])\n",
        "\n",
        "    # Filter the DataFrame to get documents within the warm-up period.\n",
        "    # The index of processed_df is a DatetimeIndex.\n",
        "    warmup_df_mask = (processed_df.index >= corpus_start_datetime) & \\\n",
        "                     (processed_df.index < warmup_end_datetime) # Use < to exclude end date itself if w_warmup is \"first 12 months\"\n",
        "    # Select lemmatized 'full_text' strings from the warm-up period.\n",
        "    warmup_texts_for_vocab: pd.Series = processed_df.loc[warmup_df_mask, \"full_text_lemmatized_str\"]\n",
        "\n",
        "    # Check if there are any texts in the warm-up period.\n",
        "    if warmup_texts_for_vocab.empty:\n",
        "        # Raise ValueError if no documents are found in the warm-up period.\n",
        "        raise ValueError(\"No documents found in the specified warm-up period. Vocabulary cannot be created.\")\n",
        "\n",
        "    # Instantiate CountVectorizer.\n",
        "    # min_df: ignore terms that appear in less than X documents.\n",
        "    # max_df: ignore terms that appear in more than Y% of documents (corpus-specific stop words).\n",
        "    count_vectorizer = CountVectorizer(\n",
        "        min_df=countvectorizer_min_df,\n",
        "        max_df=countvectorizer_max_df,\n",
        "        stop_words=None # Stop words already handled by spaCy processing\n",
        "    )\n",
        "    # Fit CountVectorizer *only* on the lemmatized 'full_text' from the warm-up period.\n",
        "    count_vectorizer.fit(warmup_texts_for_vocab)\n",
        "    # Get the created vocabulary (list of feature names).\n",
        "    vocabulary: List[str] = count_vectorizer.get_feature_names_out().tolist()\n",
        "\n",
        "    # --- Sub-step 2.e: Convert each article into a bag-of-words representation ---\n",
        "    # Use the *fitted* CountVectorizer to transform the lemmatized 'full_text' of *all* articles.\n",
        "    # The input to transform should be the 'full_text_lemmatized_str' column.\n",
        "    all_full_text_lemmatized_str: pd.Series = processed_df[\"full_text_lemmatized_str\"]\n",
        "    # Transform the texts into a BoW sparse matrix.\n",
        "    # Words not in the vocabulary (fitted on warm-up) will be ignored.\n",
        "    bow_matrix = count_vectorizer.transform(all_full_text_lemmatized_str)\n",
        "\n",
        "    # Add the BoW matrix as a new column.\n",
        "    processed_df[\"full_text_bow\"] = [row for row in bow_matrix]\n",
        "\n",
        "\n",
        "    # Return the processed DataFrame, the fitted CountVectorizer, and the vocabulary.\n",
        "    return processed_df, count_vectorizer, vocabulary\n"
      ],
      "metadata": {
        "id": "Js6OW1CZ7FA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Time Chunking\n",
        "\n",
        "def chunk_data_by_time(\n",
        "    processed_df: pd.DataFrame,\n",
        "    general_study_params: Dict[str, Any],\n",
        "    min_articles_per_chunk: int = 10\n",
        ") -> Tuple[Dict[str, List[csr_matrix]], List[str]]:\n",
        "    \"\"\"\n",
        "    Groups articles in the processed DataFrame into time chunks and checks for\n",
        "    sufficient data in each chunk.\n",
        "\n",
        "    The function performs the following steps:\n",
        "    1.  Groups articles based on their timestamps using the granularity specified\n",
        "        in `general_study_params[\"time_chunk_granularity\"]` (expected to be \"monthly\").\n",
        "        It uses 'MS' (Month Start) frequency for grouping.\n",
        "    2.  Extracts the 'full_text_bow' data (expected to be a list of sparse matrices,\n",
        "        one per document) for each chunk.\n",
        "    3.  Checks if each chunk contains at least `min_articles_per_chunk` articles.\n",
        "        A warning is issued for chunks falling below this threshold.\n",
        "    4.  Returns a dictionary mapping time chunk identifiers (e.g., \"YYYY-MM\") to\n",
        "        a list of BoW representations for documents in that chunk, and a\n",
        "        chronologically sorted list of these chunk identifiers.\n",
        "\n",
        "    Args:\n",
        "        processed_df (pd.DataFrame):\n",
        "            The DataFrame preprocessed by `preprocess_text_data`. It must have a\n",
        "            DatetimeIndex and a 'full_text_bow' column containing BoW representations\n",
        "            (e.g., scipy.sparse.csr_matrix objects).\n",
        "        general_study_params (Dict[str, Any]):\n",
        "            General study parameters, must include \"time_chunk_granularity\"\n",
        "            (expected to be \"monthly\").\n",
        "        min_articles_per_chunk (int):\n",
        "            The minimum number of articles a chunk should contain. A warning\n",
        "            will be issued if a chunk has fewer articles. Defaults to 10.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[str, List[csr_matrix]], List[str]]:\n",
        "            - chunked_corpus_bow (Dict[str, List[csr_matrix]]): A dictionary where keys\n",
        "              are time chunk identifiers (string, \"YYYY-MM\") and values are lists\n",
        "              of BoW sparse matrices (csr_matrix) for documents in that chunk.\n",
        "            - ordered_chunk_keys (List[str]): A list of time chunk identifier strings,\n",
        "              sorted chronologically.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If `processed_df` is not a pandas.DataFrame or its index\n",
        "                   is not a DatetimeIndex.\n",
        "        ValueError: If required columns ('full_text_bow') or parameters\n",
        "                    ('time_chunk_granularity') are missing or invalid.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Check if processed_df is a pandas.DataFrame.\n",
        "    if not isinstance(processed_df, pd.DataFrame):\n",
        "        # Raise TypeError if not a DataFrame.\n",
        "        raise TypeError(\"Input 'processed_df' must be a pandas.DataFrame.\")\n",
        "    # Check if the index is a DatetimeIndex.\n",
        "    if not isinstance(processed_df.index, pd.DatetimeIndex):\n",
        "        # Raise TypeError if index is not DatetimeIndex.\n",
        "        raise TypeError(\"Input 'processed_df' must have a pandas.DatetimeIndex.\")\n",
        "    # Check for the 'full_text_bow' column.\n",
        "    if \"full_text_bow\" not in processed_df.columns:\n",
        "        # Raise ValueError if 'full_text_bow' column is missing.\n",
        "        raise ValueError(\"Input 'processed_df' is missing the 'full_text_bow' column.\")\n",
        "\n",
        "    # Validate general_study_params for 'time_chunk_granularity'.\n",
        "    if not isinstance(general_study_params, dict) or \\\n",
        "       \"time_chunk_granularity\" not in general_study_params:\n",
        "        # Raise ValueError if parameter is missing or not a dict.\n",
        "        raise ValueError(\"Parameter 'general_study_params' must be a dict and contain 'time_chunk_granularity'.\")\n",
        "    # Get time_chunk_granularity value.\n",
        "    time_granularity = general_study_params[\"time_chunk_granularity\"]\n",
        "    # Check if time_granularity is \"monthly\".\n",
        "    if time_granularity != \"monthly\":\n",
        "        # Raise ValueError if granularity is not \"monthly\".\n",
        "        raise ValueError(f\"Unsupported 'time_chunk_granularity': {time_granularity}. Expected 'monthly'.\")\n",
        "\n",
        "    # Validate min_articles_per_chunk.\n",
        "    if not isinstance(min_articles_per_chunk, int) or min_articles_per_chunk < 0:\n",
        "        # Raise ValueError if min_articles_per_chunk is not a non-negative integer.\n",
        "        raise ValueError(\"'min_articles_per_chunk' must be a non-negative integer.\")\n",
        "\n",
        "    # --- Sub-step 3.a: Group articles into monthly chunks ---\n",
        "    # Define the frequency for grouping (Month Start).\n",
        "    grouping_freq = 'MS' # 'MS' stands for Month Start frequency.\n",
        "\n",
        "    # Group the DataFrame by the DatetimeIndex using the specified frequency.\n",
        "    # The result is a DataFrameGroupBy object.\n",
        "    grouped_by_time = processed_df.groupby(pd.Grouper(freq=grouping_freq))\n",
        "\n",
        "    # Initialize dictionary to store chunked BoW corpus.\n",
        "    chunked_corpus_bow: Dict[str, List[csr_matrix]] = {}\n",
        "    # Initialize list to store ordered chunk keys.\n",
        "    ordered_chunk_keys: List[str] = []\n",
        "\n",
        "    # Iterate through each group (time chunk).\n",
        "    # 'period_start_time' will be a pandas Timestamp object representing the start of the chunk.\n",
        "    # 'chunk_df' will be a sub-DataFrame containing articles for that chunk.\n",
        "    for period_start_time, chunk_df in grouped_by_time:\n",
        "        # Format the period_start_time (Timestamp) into a \"YYYY-MM\" string key.\n",
        "        chunk_key: str = period_start_time.strftime('%Y-%m')\n",
        "        # Add the formatted key to the list of ordered chunk keys.\n",
        "        ordered_chunk_keys.append(chunk_key)\n",
        "\n",
        "        # Extract the 'full_text_bow' for the current chunk.\n",
        "        # This column contains a list of sparse matrices (one per document).\n",
        "        # Ensure that the elements are indeed csr_matrix or compatible.\n",
        "        current_chunk_bow_list: List[csr_matrix] = chunk_df[\"full_text_bow\"].tolist()\n",
        "\n",
        "        # Store the list of BoW vectors for the current chunk in the dictionary.\n",
        "        chunked_corpus_bow[chunk_key] = current_chunk_bow_list\n",
        "\n",
        "        # --- Sub-step 3.b: Ensure each chunk has a sufficient number of articles ---\n",
        "        # Get the number of articles in the current chunk.\n",
        "        num_articles_in_chunk: int = len(chunk_df)\n",
        "\n",
        "        # Check if the number of articles is below the minimum threshold.\n",
        "        if num_articles_in_chunk < min_articles_per_chunk:\n",
        "            # Issue a warning if the chunk is sparse.\n",
        "            warnings.warn(\n",
        "                f\"Time chunk '{chunk_key}' contains {num_articles_in_chunk} articles, \"\n",
        "                f\"which is below the specified minimum threshold of {min_articles_per_chunk}. \"\n",
        "                \"Downstream analyses like topic modeling might be unstable for this chunk.\",\n",
        "                UserWarning # Use UserWarning for issues that are not critical errors but user should be aware of.\n",
        "            )\n",
        "            # If a chunk is completely empty (num_articles_in_chunk == 0),\n",
        "            # it will be stored with an empty list of BoW vectors.\n",
        "            # Downstream processes (e.g., RollingLDA) must be prepared to handle empty chunks\n",
        "            # (e.g., by skipping them or carrying over the model from the previous period).\n",
        "\n",
        "    # The `ordered_chunk_keys` list is already sorted chronologically due to groupby on DatetimeIndex.\n",
        "    # No explicit sort is needed if the DataFrame index was sorted, which is typical.\n",
        "    # However, to be absolutely sure, especially if groupby behavior could change or index wasn't sorted:\n",
        "    ordered_chunk_keys.sort()\n",
        "\n",
        "    # Return the dictionary of chunked BoW data and the list of ordered chunk keys.\n",
        "    return chunked_corpus_bow, ordered_chunk_keys\n"
      ],
      "metadata": {
        "id": "G81tV3O48KBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: LDAPrototype Implementation\n",
        "\n",
        "def _convert_sklearn_bow_to_gensim_corpus(\n",
        "    bow_matrix_list: List[csr_matrix],\n",
        "    count_vectorizer: CountVectorizer\n",
        ") -> Tuple[List[List[Tuple[int, int]]], Dictionary]:\n",
        "    \"\"\"\n",
        "    Converts a list of scikit-learn BoW sparse matrices to Gensim corpus format\n",
        "    and creates a Gensim Dictionary.\n",
        "\n",
        "    Args:\n",
        "        bow_matrix_list (List[csr_matrix]): A list where each element is a\n",
        "            scipy.sparse.csr_matrix representing the BoW for a document.\n",
        "            This comes from the 'full_text_bow' column after Task 2.\n",
        "        count_vectorizer (CountVectorizer): The fitted scikit-learn\n",
        "            CountVectorizer object used to create the BoW matrices.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[List[Tuple[int, int]]], Dictionary]:\n",
        "            - gensim_corpus (List[List[Tuple[int, int]]]): The corpus in Gensim\n",
        "              format (list of documents, where each document is a list of\n",
        "              (token_id, token_count) tuples).\n",
        "            - gensim_dictionary (Dictionary): The Gensim Dictionary object created\n",
        "              from the CountVectorizer vocabulary.\n",
        "    \"\"\"\n",
        "    # Create Gensim Dictionary from scikit-learn vocabulary\n",
        "    # sklearn_vocab is {term: index}\n",
        "    sklearn_vocab = count_vectorizer.vocabulary_\n",
        "    # gensim_dictionary.token2id needs to be {term: index}\n",
        "    # gensim_dictionary.id2token needs to be {index: term} (this is created automatically)\n",
        "    # We sort by index to ensure consistency if gensim re-orders\n",
        "    # However, gensim Dictionary can be built directly from token2id\n",
        "\n",
        "    # Create a token2id dictionary that gensim can use.\n",
        "    # Gensim Dictionary will assign its own internal IDs, but we can map\n",
        "    # our sklearn IDs to terms, and then let Gensim build its dictionary.\n",
        "    # Alternatively, and more directly, create the id2token mapping for Gensim.\n",
        "    id2token_sklearn = {idx: term for term, idx in sklearn_vocab.items()}\n",
        "\n",
        "    # Create a Gensim Dictionary. It will map terms to its own internal integer IDs.\n",
        "    # We need to ensure that the corpus uses Gensim's internal IDs.\n",
        "    # The easiest way is to build the dictionary from documents of tokens,\n",
        "    # but we already have IDs from sklearn.\n",
        "    # Let's build the Gensim dictionary and then map our BoW vectors.\n",
        "\n",
        "    # Create a temporary list of tokenized documents (list of lists of terms)\n",
        "    # This is inefficient if the corpus is large, but necessary if we want Gensim\n",
        "    # to build its dictionary from terms and manage its own IDs robustly.\n",
        "    # A better way: create the dictionary from the sklearn vocabulary directly.\n",
        "\n",
        "    # Gensim Dictionary can be initialized and then tokens added.\n",
        "    # Or, it can be built from a corpus of tokenized texts.\n",
        "    # Here, we have sklearn's vocabulary (term -> sklearn_id)\n",
        "    # and we need gensim's Dictionary (term -> gensim_id)\n",
        "    # and a corpus using gensim_ids.\n",
        "\n",
        "    # Create a mapping from sklearn's feature index to term\n",
        "    sklearn_idx_to_term = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Create the Gensim dictionary by providing a list of (token, id) for all tokens\n",
        "    # This is not how Dictionary is typically built.\n",
        "    # Let's use the standard way: build from a list of token lists.\n",
        "    # This requires getting the original tokens that formed the BoW, which is not ideal.\n",
        "\n",
        "    # Correct approach: Use the existing sklearn vocabulary to build the Gensim dictionary.\n",
        "    # Gensim Dictionary needs a mapping from integer id -> token string.\n",
        "    # sklearn_vocab is token_string -> integer_id. We need the reverse for id2token.\n",
        "\n",
        "    # Create id2token mapping from sklearn's vocabulary\n",
        "    # This ensures that the integer IDs used in the BoW vectors correspond to the\n",
        "    # terms in the Gensim dictionary.\n",
        "    # count_vectorizer.get_feature_names_out() gives terms ordered by their column index.\n",
        "    # So, index i in this list corresponds to term sklearn_idx_to_term[i].\n",
        "\n",
        "    # Initialize an empty Gensim Dictionary\n",
        "    gensim_dictionary = Dictionary()\n",
        "    # Create a token2id mapping for Gensim Dictionary from sklearn's vocabulary\n",
        "    # The keys are terms, values are sklearn's integer IDs.\n",
        "    # Gensim Dictionary.token2id will store term -> gensim_id.\n",
        "    # We need to provide it in a way that it respects our existing IDs if possible,\n",
        "    # or at least maps consistently.\n",
        "    # The most straightforward way is to give it the terms and let it assign IDs.\n",
        "    # Then, we must re-map our BoW vectors. This is complex.\n",
        "\n",
        "    # Alternative: Gensim Dictionary from {integer_id: token_string}\n",
        "    # This is not directly supported. It expects documents (list of lists of tokens).\n",
        "\n",
        "    # Let's use the feature names from CountVectorizer. These are already our vocabulary.\n",
        "    # Gensim Dictionary can be built from a list of documents, where each document is a list of tokens.\n",
        "    # We will give it a \"dummy\" corpus of one document containing all unique vocabulary terms\n",
        "    # to ensure all terms are in the dictionary.\n",
        "    # The `prune_at=None` argument is important to keep all words.\n",
        "    gensim_dictionary = Dictionary([sklearn_idx_to_term.tolist()], prune_at=None)\n",
        "    # Now, gensim_dictionary.token2id maps term -> gensim_id.\n",
        "    # We need to ensure our BoW vectors use these gensim_ids.\n",
        "    # The original BoW matrix from sklearn uses sklearn_ids (column indices).\n",
        "    # If sklearn_idx_to_term[j] is the j-th term, its sklearn_id is j.\n",
        "    # Its gensim_id is gensim_dictionary.token2id[sklearn_idx_to_term[j]].\n",
        "\n",
        "    gensim_corpus: List[List[Tuple[int, int]]] = []\n",
        "    # Iterate through each document's BoW sparse matrix from the input list.\n",
        "    for doc_bow_matrix in bow_matrix_list:\n",
        "        # Ensure it's a single row matrix (representing one document).\n",
        "        if doc_bow_matrix.shape[0] != 1:\n",
        "            # This should not happen if 'full_text_bow' stores one csr_matrix per doc.\n",
        "            # Handle or raise error if a document's BoW is not a single row.\n",
        "            raise ValueError(\"Each element in bow_matrix_list must be a 1-row CSR matrix.\")\n",
        "\n",
        "        # Convert the sparse matrix row to a list of (gensim_id, count) tuples.\n",
        "        gensim_doc_bow: List[Tuple[int, int]] = []\n",
        "        # Get non-zero elements: doc_bow_matrix.indices are column indices (sklearn_ids),\n",
        "        # doc_bow_matrix.data are the counts.\n",
        "        for sklearn_id_idx, count in zip(doc_bow_matrix.indices, doc_bow_matrix.data):\n",
        "            # Get the term string using sklearn's feature name mapping.\n",
        "            term_string = sklearn_idx_to_term[sklearn_id_idx]\n",
        "            # Get Gensim's internal ID for this term string.\n",
        "            # This handles cases where Gensim might re-order or assign different IDs.\n",
        "            if term_string in gensim_dictionary.token2id: # Ensure term is in Gensim dict\n",
        "                gensim_term_id = gensim_dictionary.token2id[term_string]\n",
        "                # Append (gensim_term_id, count) to the document's BoW list.\n",
        "                gensim_doc_bow.append((gensim_term_id, int(count))) # Ensure count is int\n",
        "        # Append the processed document BoW to the Gensim corpus.\n",
        "        gensim_corpus.append(gensim_doc_bow)\n",
        "\n",
        "    # Return the Gensim-formatted corpus and the Gensim Dictionary.\n",
        "    return gensim_corpus, gensim_dictionary\n",
        "\n",
        "def train_lda_prototype(\n",
        "    processed_df: pd.DataFrame,\n",
        "    count_vectorizer: CountVectorizer, # Fitted CountVectorizer from Task 2\n",
        "    general_study_params: Dict[str, Any],\n",
        "    rolling_lda_params: Dict[str, Any], # For w_warmup\n",
        "    lda_prototype_params: Dict[str, Any],\n",
        "    lda_iterations: int = 1000, # Default based on common practice\n",
        "    lda_alpha: str = 'symmetric', # Gensim default\n",
        "    lda_eta: Optional[Any] = None # Gensim default (symmetric based on num_topics if None)\n",
        ") -> LdaModel:\n",
        "    \"\"\"\n",
        "    Implements the LDAPrototype selection algorithm to find the most stable\n",
        "    LDA model from multiple runs on a warm-up corpus.\n",
        "\n",
        "    Args:\n",
        "        processed_df (pd.DataFrame):\n",
        "            The DataFrame with processed text, including 'full_text_bow' and\n",
        "            a DatetimeIndex. From Task 2.\n",
        "        count_vectorizer (CountVectorizer):\n",
        "            The scikit-learn CountVectorizer object fitted on the warm-up corpus\n",
        "            during Task 2.\n",
        "        general_study_params (Dict[str, Any]):\n",
        "            General study parameters, must include \"corpus_start_date\".\n",
        "        rolling_lda_params (Dict[str, Any]):\n",
        "            RollingLDA parameters, must include \"w_warmup\" (in months) to\n",
        "            define the warm-up period.\n",
        "        lda_prototype_params (Dict[str, Any]):\n",
        "            Parameters for LDAPrototype, including \"K_topics\" and \"N_lda_runs\".\n",
        "        lda_iterations (int):\n",
        "            Number of iterations for LDA model training. Default: 1000.\n",
        "            This should ideally be based on reference [32].\n",
        "        lda_alpha (str or np.ndarray):\n",
        "            LDA alpha hyperparameter (document-topic density). Default: 'symmetric'.\n",
        "        lda_eta (str, np.ndarray, or None):\n",
        "            LDA eta (beta) hyperparameter (topic-word density). Default: None (symmetric).\n",
        "\n",
        "    Returns:\n",
        "        LdaModel: The selected Gensim LdaModel object (the LDAPrototype).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If parameters are invalid or no documents in warm-up period.\n",
        "    \"\"\"\n",
        "    # --- Input Validation & Parameter Extraction ---\n",
        "    # Validate key parameters from dictionaries.\n",
        "    if not all(k in lda_prototype_params for k in [\"K_topics\", \"N_lda_runs\"]):\n",
        "        raise ValueError(\"'lda_prototype_params' must contain 'K_topics' and 'N_lda_runs'.\")\n",
        "    K_topics: int = lda_prototype_params[\"K_topics\"]\n",
        "    N_lda_runs: int = lda_prototype_params[\"N_lda_runs\"]\n",
        "\n",
        "    if not all(k in rolling_lda_params for k in [\"w_warmup\"]):\n",
        "        raise ValueError(\"'rolling_lda_params' must contain 'w_warmup'.\")\n",
        "    w_warmup: int = rolling_lda_params[\"w_warmup\"]\n",
        "\n",
        "    if not all(k in general_study_params for k in [\"corpus_start_date\"]):\n",
        "        raise ValueError(\"'general_study_params' must contain 'corpus_start_date'.\")\n",
        "    corpus_start_date_str: str = general_study_params[\"corpus_start_date\"]\n",
        "\n",
        "    # --- Prepare Warm-up Corpus for Gensim ---\n",
        "    # Determine the end date of the warm-up period.\n",
        "    corpus_start_datetime = pd.to_datetime(corpus_start_date_str)\n",
        "    # Calculate warm-up end date.\n",
        "    warmup_end_datetime = corpus_start_datetime + pd.DateOffset(months=w_warmup)\n",
        "\n",
        "    # Filter the DataFrame for the warm-up period.\n",
        "    warmup_df_mask = (processed_df.index >= corpus_start_datetime) & \\\n",
        "                     (processed_df.index < warmup_end_datetime)\n",
        "    # Select 'full_text_bow' for the warm-up period.\n",
        "    warmup_bow_list: List[csr_matrix] = processed_df.loc[warmup_df_mask, \"full_text_bow\"].tolist()\n",
        "\n",
        "    # Check if warm-up corpus is empty.\n",
        "    if not warmup_bow_list:\n",
        "        # Raise ValueError if no documents in warm-up period.\n",
        "        raise ValueError(\"No documents found in the warm-up period for LDAPrototype training.\")\n",
        "\n",
        "    # Convert scikit-learn BoW to Gensim corpus format and create Gensim Dictionary.\n",
        "    # This uses the globally fitted CountVectorizer from Task 2.\n",
        "    warmup_gensim_corpus, gensim_dictionary = _convert_sklearn_bow_to_gensim_corpus(\n",
        "        warmup_bow_list, count_vectorizer\n",
        "    )\n",
        "\n",
        "    # Check if the resulting Gensim corpus is empty (e.g., all docs in warm-up were empty after processing).\n",
        "    if not warmup_gensim_corpus or all(not doc for doc in warmup_gensim_corpus):\n",
        "        raise ValueError(\"Warm-up Gensim corpus is empty. Cannot train LDA models.\")\n",
        "\n",
        "\n",
        "    # --- Sub-step 4.c: Train N_lda_runs LDA models ---\n",
        "    # Initialize list to store trained LDA models.\n",
        "    trained_lda_models: List[LdaModel] = []\n",
        "    # Loop N_lda_runs times to train multiple LDA models.\n",
        "    for i in range(N_lda_runs):\n",
        "        # Train a Gensim LdaModel.\n",
        "        # Set a different random_state for each run to ensure model diversity.\n",
        "        # `id2word` maps Gensim's internal integer IDs to terms.\n",
        "        # `iterations` should be sufficiently high for convergence.\n",
        "        # `alpha` and `eta` are LDA hyperparameters. 'symmetric' is a common default.\n",
        "        # `eta=None` lets Gensim set a default symmetric eta based on num_topics.\n",
        "        lda_model = LdaModel(\n",
        "            corpus=warmup_gensim_corpus,\n",
        "            id2word=gensim_dictionary, # Use the created Gensim dictionary\n",
        "            num_topics=K_topics,\n",
        "            iterations=lda_iterations,\n",
        "            alpha=lda_alpha,\n",
        "            eta=lda_eta, # Topic-word Dirichlet hyperparameter\n",
        "            random_state=i * 100,  # Ensure different seed for each run\n",
        "            passes=10, # Number of passes through the corpus during training\n",
        "            eval_every=None # Disable perplexity evaluation during training for speed\n",
        "        )\n",
        "        # Add the trained model to the list.\n",
        "        trained_lda_models.append(lda_model)\n",
        "\n",
        "    # --- Sub-step 4.d: Calculate pairwise model similarities ---\n",
        "    # Initialize an N x N matrix to store pairwise similarities.\n",
        "    # N is N_lda_runs.\n",
        "    num_models = N_lda_runs\n",
        "    # Initialize similarity matrix with zeros.\n",
        "    pairwise_similarities = np.zeros((num_models, num_models))\n",
        "\n",
        "    # Extract topic-word distributions (phi matrices) for all models.\n",
        "    # Each phi_matrix is K_topics x VocabularySize.\n",
        "    phi_matrices: List[np.ndarray] = []\n",
        "    for model in trained_lda_models:\n",
        "        # Get topic-word distributions. Ensure it's dense for cosine similarity.\n",
        "        # Gensim's get_topics() returns a K x V numpy array.\n",
        "        phi_matrix = model.get_topics()\n",
        "        phi_matrices.append(phi_matrix)\n",
        "\n",
        "    # Iterate through all unique pairs of models (Mi, Mj).\n",
        "    for i in range(num_models):\n",
        "        # Diagonal elements are 1.0 (model compared to itself).\n",
        "        pairwise_similarities[i, i] = 1.0\n",
        "        for j in range(i + 1, num_models):\n",
        "            # Get phi matrices for model i and model j.\n",
        "            phi_i = phi_matrices[i] # Shape: (K_topics, VocabSize)\n",
        "            phi_j = phi_matrices[j] # Shape: (K_topics, VocabSize)\n",
        "\n",
        "            # Calculate cosine similarity between all topics of model i and all topics of model j.\n",
        "            # topic_similarity_matrix[r, c] = cosine_sim(topic r of model i, topic c of model j)\n",
        "            # Shape: (K_topics, K_topics)\n",
        "            topic_similarity_matrix = cosine_similarity(phi_i, phi_j)\n",
        "\n",
        "            # Use Hungarian algorithm to find optimal topic pairings.\n",
        "            # linear_sum_assignment finds the minimum cost assignment.\n",
        "            # We want to maximize similarity, so use cost = 1 - similarity (or -similarity).\n",
        "            # Using -similarity is common as it directly maximizes sum of similarities.\n",
        "            cost_matrix = -topic_similarity_matrix\n",
        "            # Get row indices (from model i) and column indices (from model j) of optimal pairings.\n",
        "            row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "\n",
        "            # The number of matched pairs is the length of row_ind (should be K_topics).\n",
        "            num_matched_pairs = len(row_ind)\n",
        "\n",
        "            # Calculate similarity as per LaTeX context: (Num matched pairs) / K_topics\n",
        "            # If num_matched_pairs is K_topics, similarity is sum_of_max_sims / K_topics.\n",
        "            # The paper's formula is: Similarity(A1, A2) = (#topics of model A1 that are matched with a topic of model A2) / K\n",
        "            # This implies the numerator is the count of successful matches.\n",
        "            # With Hungarian algorithm, we always get K matches if K_topics_A1 == K_topics_A2 == K.\n",
        "            # The actual similarity score for the model pair is the average similarity of these matched topic pairs.\n",
        "            # model_similarity_score = topic_similarity_matrix[row_ind, col_ind].sum() / K_topics\n",
        "\n",
        "            # The LaTeX context (Section 3.1) states:\n",
        "            # \"Similarity(A1, A2) = #topics of model A1 that are matched with a topic of model A2 / K\"\n",
        "            # This is interpreted as the proportion of topics that could be matched.\n",
        "            # If using Hungarian algorithm on KxK matrix, K topics will always be matched.\n",
        "            # The `ttta` package's LDAPrototype uses average JSD of matched topics.\n",
        "            # The paper by Rieger et al. (2024) \"LDAPrototype...\" defines similarity_LDA as:\n",
        "            # S(M_a, M_b) = (1/K) * sum_{k=1 to K} max_{l=1 to K} S_T(T_k^a, T_l^b)\n",
        "            # where S_T is topic similarity (e.g., cosine). This is a greedy match.\n",
        "            # The LaTeX context mentions \"clustering...into clusters of size two\", suggesting optimal assignment.\n",
        "            # Let's use the average similarity of the K optimally matched pairs.\n",
        "\n",
        "            # Sum of similarities of the optimally matched pairs.\n",
        "            sum_of_optimal_pair_similarities = topic_similarity_matrix[row_ind, col_ind].sum()\n",
        "            # Average similarity for this model pair.\n",
        "            model_pair_similarity = sum_of_optimal_pair_similarities / K_topics\n",
        "\n",
        "            # Store the calculated similarity in the matrix (symmetric).\n",
        "            pairwise_similarities[i, j] = model_pair_similarity\n",
        "            pairwise_similarities[j, i] = model_pair_similarity\n",
        "\n",
        "    # --- Sub-step 4.e: Select the model with the highest average similarity ---\n",
        "    # Calculate average similarity for each model to all other N-1 models.\n",
        "    # Summing along axis 1 and subtracting diagonal (1.0), then dividing by (num_models - 1).\n",
        "    # Avoid division by zero if num_models is 1.\n",
        "    if num_models == 1:\n",
        "        # If only one model was trained, it is the prototype by default.\n",
        "        prototype_index = 0\n",
        "    else:\n",
        "        # Calculate sum of similarities for each model (row sum).\n",
        "        sum_similarities = np.sum(pairwise_similarities, axis=1)\n",
        "        # Average similarity: (sum - diagonal_element) / (num_models - 1)\n",
        "        # Diagonal element is 1.0 (similarity with itself).\n",
        "        average_similarities = (sum_similarities - 1.0) / (num_models - 1)\n",
        "        # Find the index of the model with the maximum average similarity.\n",
        "        prototype_index = np.argmax(average_similarities)\n",
        "\n",
        "    # The LDA model corresponding to prototype_index is the Model_LDA_Prototype.\n",
        "    lda_prototype_model: LdaModel = trained_lda_models[prototype_index]\n",
        "\n",
        "    # Return the selected LDA prototype model.\n",
        "    return lda_prototype_model\n"
      ],
      "metadata": {
        "id": "3JdnTGNn832g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: RollingLDA Implementation\n",
        "\n",
        "def _convert_chunk_sklearn_bow_to_gensim(\n",
        "    sklearn_bow_list_for_chunk: List[csr_matrix],\n",
        "    sklearn_feature_names: List[str],\n",
        "    gensim_dictionary: GensimDictionary\n",
        ") -> List[List[Tuple[int, int]]]:\n",
        "    \"\"\"\n",
        "    Converts a list of scikit-learn BoW CSR matrices for a single time chunk\n",
        "    into Gensim BoW corpus format using a global Gensim dictionary and\n",
        "    scikit-learn feature names.\n",
        "\n",
        "    Args:\n",
        "        sklearn_bow_list_for_chunk (List[csr_matrix]):\n",
        "            A list where each element is a scipy.sparse.csr_matrix representing\n",
        "            the BoW for a document in scikit-learn's feature ID space.\n",
        "        sklearn_feature_names (List[str]):\n",
        "            A list of term strings where the index corresponds to the feature ID\n",
        "            used in the sklearn BoW CSR matrices (i.e., output of\n",
        "            CountVectorizer.get_feature_names_out().tolist()).\n",
        "        gensim_dictionary (GensimDictionary):\n",
        "            The global Gensim Dictionary object mapping term strings to Gensim's\n",
        "            internal integer IDs.\n",
        "\n",
        "    Returns:\n",
        "        List[List[Tuple[int, int]]]:\n",
        "            A list of documents in Gensim BoW format for the input chunk.\n",
        "            Each document is a list of (gensim_token_id, token_count) tuples.\n",
        "    \"\"\"\n",
        "    # Initialize the list to store Gensim-formatted BoW for the chunk.\n",
        "    gensim_bow_for_chunk: List[List[Tuple[int, int]]] = []\n",
        "\n",
        "    # Iterate through each document's scikit-learn BoW CSR matrix in the chunk.\n",
        "    for doc_sklearn_bow in sklearn_bow_list_for_chunk:\n",
        "        # Initialize the list for the current document's Gensim BoW.\n",
        "        doc_gensim_bow: List[Tuple[int, int]] = []\n",
        "        # doc_sklearn_bow.indices contains the scikit-learn feature IDs (column indices).\n",
        "        # doc_sklearn_bow.data contains the corresponding counts.\n",
        "        # Iterate through the non-zero elements of the sparse matrix.\n",
        "        for sklearn_feature_id, count in zip(doc_sklearn_bow.indices, doc_sklearn_bow.data):\n",
        "            # Retrieve the term string using the scikit-learn feature ID.\n",
        "            # This assumes sklearn_feature_id is a valid index for sklearn_feature_names.\n",
        "            if 0 <= sklearn_feature_id < len(sklearn_feature_names):\n",
        "                term_string = sklearn_feature_names[sklearn_feature_id]\n",
        "                # Check if the term string exists in the Gensim dictionary.\n",
        "                if term_string in gensim_dictionary.token2id:\n",
        "                    # Retrieve Gensim's internal ID for the term string.\n",
        "                    gensim_token_id = gensim_dictionary.token2id[term_string]\n",
        "                    # Append (gensim_token_id, count) to the document's BoW list.\n",
        "                    # Ensure count is an integer.\n",
        "                    doc_gensim_bow.append((gensim_token_id, int(count)))\n",
        "                # else: term is not in Gensim dictionary (e.g., pruned by min_df/max_df when Gensim dict was built)\n",
        "                # In such cases, the term is effectively ignored for this document in Gensim.\n",
        "            # else: sklearn_feature_id is out of bounds for sklearn_feature_names. This indicates an inconsistency.\n",
        "            # This should ideally not happen if data is prepared correctly.\n",
        "            # Consider logging a warning or raising an error for such inconsistencies.\n",
        "        # Append the processed document's Gensim BoW to the chunk's list.\n",
        "        gensim_bow_for_chunk.append(doc_gensim_bow)\n",
        "    # Return the list of Gensim-formatted documents for the chunk.\n",
        "    return gensim_bow_for_chunk\n",
        "\n",
        "def apply_rolling_lda(\n",
        "    lda_prototype_model: LdaModel,\n",
        "    chunked_corpus_sklearn_bow: Dict[str, List[csr_matrix]],\n",
        "    ordered_chunk_keys: List[str],\n",
        "    gensim_dictionary: GensimDictionary,\n",
        "    sklearn_feature_names: List[str], # NEWLY ADDED: from count_vectorizer.get_feature_names_out()\n",
        "    rolling_lda_params: Dict[str, Any],\n",
        "    lda_iterations_warmup: int = 50,\n",
        "    lda_iterations_update: int = 20,\n",
        "    lda_passes_warmup: int = 10,\n",
        "    lda_passes_update: int = 1,\n",
        "    lda_alpha_rolling: str = 'symmetric', # Or specific float/array\n",
        "    epsilon_eta: float = 1e-9 # Small constant for numerical stability of eta\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Implements the RollingLDA algorithm to model topic evolution over time.\n",
        "\n",
        "    This version correctly handles the conversion of scikit-learn BoW formats\n",
        "    to Gensim formats using the provided `sklearn_feature_names` list.\n",
        "\n",
        "    Args:\n",
        "        lda_prototype_model (LdaModel):\n",
        "            The LDAPrototype model (Gensim LdaModel object) from Task 4.\n",
        "        chunked_corpus_sklearn_bow (Dict[str, List[csr_matrix]]):\n",
        "            Output of Task 3: Dict mapping time chunk identifiers (\"YYYY-MM\")\n",
        "            to lists of scipy.sparse.csr_matrix BoW representations (sklearn IDs).\n",
        "        ordered_chunk_keys (List[str]):\n",
        "            Output of Task 3: Chronologically sorted list of time chunk identifiers.\n",
        "        gensim_dictionary (GensimDictionary):\n",
        "            The Gensim Dictionary object from Task 4 (or Task 2).\n",
        "        sklearn_feature_names (List[str]):\n",
        "            List of term strings where index = sklearn feature ID. This is\n",
        "            `count_vectorizer.get_feature_names_out().tolist()` from Task 2.\n",
        "        rolling_lda_params (Dict[str, Any]):\n",
        "            Parameters for RollingLDA: \"w_warmup\", \"m_memory\", \"K_topics\".\n",
        "        lda_iterations_warmup (int): Iterations for warm-up LDA.\n",
        "        lda_iterations_update (int): Iterations for incremental LDA updates.\n",
        "        lda_passes_warmup (int): Passes for warm-up LDA.\n",
        "        lda_passes_update (int): Passes for incremental LDA updates.\n",
        "        lda_alpha_rolling (str or np.ndarray): Alpha for RollingLDA models.\n",
        "        epsilon_eta (float): Small constant added to eta for numerical stability.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, np.ndarray]:\n",
        "            TimeSeries_TopicWordDistributions: Dict mapping chunk identifiers\n",
        "            to K_topics x VocabularySize topic-word distribution matrices.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If parameters are invalid or data is insufficient.\n",
        "    \"\"\"\n",
        "    # --- Input Validation & Parameter Extraction ---\n",
        "    # Validate rolling_lda_params for required keys.\n",
        "    if not all(k in rolling_lda_params for k in [\"w_warmup\", \"m_memory\", \"K_topics\"]):\n",
        "        raise ValueError(\"'rolling_lda_params' must contain 'w_warmup', 'm_memory', and 'K_topics'.\")\n",
        "    # Extract w_warmup (number of initial time chunks for warm-up).\n",
        "    w_warmup: int = rolling_lda_params[\"w_warmup\"]\n",
        "    # Extract K_topics (number of topics, consistent with LDAPrototype).\n",
        "    K_topics: int = rolling_lda_params[\"K_topics\"]\n",
        "    # m_memory is noted; its influence is via eta_t = phi_(t-1).\n",
        "\n",
        "    # Validate that ordered_chunk_keys is not empty and w_warmup is feasible.\n",
        "    if not ordered_chunk_keys:\n",
        "        raise ValueError(\"'ordered_chunk_keys' cannot be empty.\")\n",
        "    if w_warmup <= 0 or w_warmup > len(ordered_chunk_keys):\n",
        "        raise ValueError(f\"'w_warmup' ({w_warmup}) must be positive and not exceed total chunks ({len(ordered_chunk_keys)}).\")\n",
        "    if not isinstance(sklearn_feature_names, list) or not all(isinstance(s, str) for s in sklearn_feature_names):\n",
        "        raise TypeError(\"'sklearn_feature_names' must be a list of strings.\")\n",
        "    if not gensim_dictionary or len(gensim_dictionary) != len(sklearn_feature_names):\n",
        "        # This check assumes gensim_dictionary was built from the same vocab source as sklearn_feature_names\n",
        "        # and has the same number of unique terms.\n",
        "        warnings.warn(\n",
        "             \"Size of `gensim_dictionary` does not match length of `sklearn_feature_names`. \"\n",
        "             \"Ensure they originate from the same vocabulary source and filtering.\", UserWarning\n",
        "        )\n",
        "\n",
        "\n",
        "    # Initialize dictionary to store the time series of topic-word distributions.\n",
        "    time_series_topic_word_dist: Dict[str, np.ndarray] = {}\n",
        "    # Initialize variable to hold the current LDA model state.\n",
        "    current_lda_model: Optional[LdaModel] = None\n",
        "\n",
        "    # --- Sub-step 5.c: Initialize RollingLDA with prototype on first w_warmup chunks ---\n",
        "    # Concatenate Gensim-formatted BoW representations for all documents in the warm-up period.\n",
        "    warmup_gensim_corpus_aggregated: List[List[Tuple[int, int]]] = []\n",
        "    # Iterate through the first w_warmup chunk keys.\n",
        "    for i in range(w_warmup):\n",
        "        # Get the current chunk key.\n",
        "        chunk_key = ordered_chunk_keys[i]\n",
        "        # Get the list of sklearn BoW matrices for this chunk.\n",
        "        sklearn_bows_for_chunk: List[csr_matrix] = chunked_corpus_sklearn_bow.get(chunk_key, [])\n",
        "        # Convert this chunk's sklearn BoWs to a list of Gensim BoW documents.\n",
        "        gensim_bows_for_chunk = _convert_chunk_sklearn_bow_to_gensim(\n",
        "            sklearn_bows_for_chunk, sklearn_feature_names, gensim_dictionary\n",
        "        )\n",
        "        # Extend the aggregated list with documents from the current warm-up chunk.\n",
        "        warmup_gensim_corpus_aggregated.extend(gensim_bows_for_chunk)\n",
        "\n",
        "    # Filter out any completely empty documents from the aggregated warm-up corpus,\n",
        "    # as they provide no information for LDA training and can cause issues with some Gensim versions.\n",
        "    warmup_gensim_corpus_aggregated = [doc for doc in warmup_gensim_corpus_aggregated if doc]\n",
        "\n",
        "    # Check if the aggregated warm-up corpus is empty after filtering.\n",
        "    if not warmup_gensim_corpus_aggregated:\n",
        "        raise ValueError(\"Warm-up corpus is empty after concatenating and filtering initial chunks. Cannot train warm-up LDA model.\")\n",
        "\n",
        "    # Use topic-word distributions from lda_prototype_model as prior (eta).\n",
        "    # Ensure prototype_eta has the same vocabulary size as gensim_dictionary.\n",
        "    # lda_prototype_model.get_topics() is K x V, where V is vocab size of prototype's dictionary.\n",
        "    # This must match len(gensim_dictionary).\n",
        "    if lda_prototype_model.num_terms != len(gensim_dictionary):\n",
        "        raise ValueError(\n",
        "            \"Vocabulary size mismatch between LDA prototype model ({lda_prototype_model.num_terms}) \"\n",
        "            \"and the provided gensim_dictionary ({len(gensim_dictionary)}).\"\n",
        "        )\n",
        "    # Add epsilon for numerical stability and to allow new word probabilities.\n",
        "    prototype_eta_prior = lda_prototype_model.get_topics() + epsilon_eta\n",
        "\n",
        "    # Train the initial LDA model for the warm-up period.\n",
        "    # This model is guided by the prototype's topics via the 'eta' prior.\n",
        "    current_lda_model = LdaModel(\n",
        "        corpus=warmup_gensim_corpus_aggregated,\n",
        "        id2word=gensim_dictionary, # Use the global Gensim dictionary\n",
        "        num_topics=K_topics,\n",
        "        iterations=lda_iterations_warmup,\n",
        "        passes=lda_passes_warmup,\n",
        "        alpha=lda_alpha_rolling,\n",
        "        eta=prototype_eta_prior,\n",
        "        random_state=42 # Fixed random state for reproducibility of warm-up model\n",
        "    )\n",
        "\n",
        "    # Store topic-word distributions for all warm-up chunks.\n",
        "    # The model trained on aggregated warm-up data represents the state at the end of warm-up.\n",
        "    # We assign this state to each chunk within the warm-up period.\n",
        "    # This is a common approach for initializing dynamic topic models.\n",
        "    warmup_phi = current_lda_model.get_topics()\n",
        "    for i in range(w_warmup):\n",
        "        chunk_key = ordered_chunk_keys[i]\n",
        "        time_series_topic_word_dist[chunk_key] = warmup_phi.copy() # Store a copy\n",
        "\n",
        "    # --- Sub-step 5.d: Iteratively update for subsequent time chunks ---\n",
        "    # Iterate through the remaining time chunks (after warm-up).\n",
        "    for i in range(w_warmup, len(ordered_chunk_keys)):\n",
        "        # Get the current chunk key.\n",
        "        chunk_key = ordered_chunk_keys[i]\n",
        "        # Get the list of sklearn BoW matrices for this chunk.\n",
        "        sklearn_bows_for_current_chunk: List[csr_matrix] = chunked_corpus_sklearn_bow.get(chunk_key, [])\n",
        "        # Convert this chunk's sklearn BoWs to Gensim BoW format.\n",
        "        current_chunk_gensim_corpus = _convert_chunk_sklearn_bow_to_gensim(\n",
        "            sklearn_bows_for_current_chunk, sklearn_feature_names, gensim_dictionary\n",
        "        )\n",
        "        # Filter out any completely empty documents from the current chunk's corpus.\n",
        "        current_chunk_gensim_corpus = [doc for doc in current_chunk_gensim_corpus if doc]\n",
        "\n",
        "        # If the current chunk is effectively empty after filtering,\n",
        "        # carry forward the previous model's topic-word distributions.\n",
        "        if not current_chunk_gensim_corpus:\n",
        "            warnings.warn(f\"Time chunk '{chunk_key}' is effectively empty after processing. \"\n",
        "                          \"Carrying forward topic model from previous chunk.\", UserWarning)\n",
        "            # `current_lda_model` holds the model from the previous successfully processed chunk.\n",
        "            # It should not be None here as warm-up guarantees its initialization.\n",
        "            time_series_topic_word_dist[chunk_key] = current_lda_model.get_topics().copy() # Store a copy\n",
        "            # The model state itself (`current_lda_model`) remains unchanged.\n",
        "            continue\n",
        "\n",
        "        # Set eta for the update step using the posterior topic-word distribution\n",
        "        # (variational parameters lambda) from the previous time step's model.\n",
        "        # model.state.get_lambda() returns the K x V matrix of variational parameters.\n",
        "        # Add epsilon for numerical stability.\n",
        "        eta_for_update = current_lda_model.state.get_lambda() + epsilon_eta\n",
        "\n",
        "        # Update the LDA model with the new chunk's data.\n",
        "        # The `update` method modifies the model in-place.\n",
        "        current_lda_model.update(\n",
        "            corpus=current_chunk_gensim_corpus,\n",
        "            iterations=lda_iterations_update,\n",
        "            passes=lda_passes_update,\n",
        "            eta=eta_for_update # Dynamically updated eta from t-1's posterior\n",
        "            # Alpha is taken from the model's current alpha setting.\n",
        "        )\n",
        "\n",
        "        # Store the topic-word distributions for the current chunk.\n",
        "        # Get the updated topic-word distribution matrix (phi).\n",
        "        time_series_topic_word_dist[chunk_key] = current_lda_model.get_topics().copy() # Store a copy\n",
        "\n",
        "    # Return the time series of topic-word distributions.\n",
        "    return time_series_topic_word_dist\n"
      ],
      "metadata": {
        "id": "CwwZlXaZ90KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Topical Changes Implementation\n",
        "\n",
        "def detect_topical_changes(\n",
        "    time_series_topic_word_dist: Dict[str, np.ndarray], # Output of Task 5 (phi matrices)\n",
        "    ordered_chunk_keys: List[str], # Chronological keys for the time_series_topic_word_dist\n",
        "    gensim_dictionary: GensimDictionary, # For mapping vocab indices to terms in LOO\n",
        "    topical_changes_params: Dict[str, Any],\n",
        "    k_topics: int, # Number of topics (from rolling_lda_params or lda_prototype_params)\n",
        "    num_tokens_for_bootstrap_resampling_per_topic: int = 10000, # Heuristic N for bootstrap draws\n",
        "    num_significant_words_loo: int = 10, # Number of top words from LOO impacts\n",
        "    epsilon: float = 1e-9 # Small constant for numerical stability\n",
        ") -> Tuple[List[Tuple[str, int, List[str], float, float]], List[Tuple[str, int, float, float]]]:\n",
        "    \"\"\"\n",
        "    Detects change points in topic evolution using a bootstrap-based method\n",
        "    and calculates leave-one-out word impacts for detected changes.\n",
        "\n",
        "    Args:\n",
        "        time_series_topic_word_dist (Dict[str, np.ndarray]):\n",
        "            Output of RollingLDA (Task 5). Maps chunk identifiers (\"YYYY-MM\")\n",
        "            to K x V topic-word probability distribution matrices (phi_k,t).\n",
        "        ordered_chunk_keys (List[str]):\n",
        "            Chronologically sorted list of chunk identifiers corresponding to the\n",
        "            keys in `time_series_topic_word_dist`.\n",
        "        gensim_dictionary (GensimDictionary):\n",
        "            The Gensim Dictionary object used in LDA, for vocab mapping.\n",
        "        topical_changes_params (Dict[str, Any]):\n",
        "            Parameters for topical change detection: \"z_lookback\",\n",
        "            \"mixture_param_gamma\", \"alpha_significance\", \"B_bootstrap\".\n",
        "        k_topics (int):\n",
        "            The total number of topics in the LDA models.\n",
        "        num_tokens_for_bootstrap_resampling_per_topic (int):\n",
        "            Number of tokens to draw (with replacement) when generating each\n",
        "            bootstrap sample of a topic-word count vector. This approximates\n",
        "            the total word occurrences within a topic in the look-back period.\n",
        "        num_significant_words_loo (int):\n",
        "            Number of most impactful words to report from LOO analysis.\n",
        "        epsilon (float): Small constant for numerical stability.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[Tuple[str, int, List[str], float, float]], List[Tuple[str, int, float, float]]]:\n",
        "            - detected_change_points_with_loo (List[Tuple[str, int, List[str], float, float]]):\n",
        "              List of detected changes. Each tuple is:\n",
        "              (chunk_key_of_change, topic_id, list_of_significant_loo_words,\n",
        "               observed_cosine_distance, critical_distance_threshold).\n",
        "            - visualization_data (List[Tuple[str, int, float, float]]):\n",
        "              Data for plotting. Each tuple is:\n",
        "              (chunk_key, topic_id, observed_cosine_distance, critical_distance_threshold).\n",
        "              This includes all tested points, not just detected changes.\n",
        "    \"\"\"\n",
        "    # --- Parameter Extraction and Validation ---\n",
        "    if not all(k in topical_changes_params for k in [\"z_lookback\", \"mixture_param_gamma\", \"alpha_significance\", \"B_bootstrap\"]):\n",
        "        raise ValueError(\"`topical_changes_params` missing one or more required keys.\")\n",
        "\n",
        "    z_lookback: int = topical_changes_params[\"z_lookback\"]\n",
        "    mixture_gamma: float = topical_changes_params[\"mixture_param_gamma\"] # Gamma in paper\n",
        "    alpha_significance: float = topical_changes_params[\"alpha_significance\"]\n",
        "    B_bootstrap: int = topical_changes_params[\"B_bootstrap\"]\n",
        "\n",
        "    if not (0 < alpha_significance < 1):\n",
        "        raise ValueError(\"'alpha_significance' must be between 0 and 1 (exclusive).\")\n",
        "    if z_lookback <= 0:\n",
        "        raise ValueError(\"'z_lookback' must be positive.\")\n",
        "    if B_bootstrap <= 0:\n",
        "        raise ValueError(\"'B_bootstrap' must be positive.\")\n",
        "    if not (0.0 <= mixture_gamma <= 1.0):\n",
        "        raise ValueError(\"'mixture_param_gamma' must be between 0.0 and 1.0.\")\n",
        "    if k_topics <= 0:\n",
        "        raise ValueError(\"'k_topics' must be positive.\")\n",
        "    if num_tokens_for_bootstrap_resampling_per_topic <=0:\n",
        "        raise ValueError(\"'num_tokens_for_bootstrap_resampling_per_topic' must be positive.\")\n",
        "    if num_significant_words_loo < 0: # Can be 0 if no LOO words are desired\n",
        "        raise ValueError(\"'num_significant_words_loo' must be non-negative.\")\n",
        "\n",
        "    # Initialize lists for outputs\n",
        "    detected_change_points_with_loo: List[Tuple[str, int, List[str], float, float]] = []\n",
        "    visualization_data: List[Tuple[str, int, float, float]] = []\n",
        "\n",
        "    # Vocabulary size from one of the phi matrices (assuming consistent vocab size)\n",
        "    # Get the first available phi matrix to determine vocabulary size.\n",
        "    if not ordered_chunk_keys or not time_series_topic_word_dist:\n",
        "        warnings.warn(\"No topic-word distributions found. Skipping topical change detection.\", UserWarning)\n",
        "        return detected_change_points_with_loo, visualization_data\n",
        "\n",
        "    # Find the first valid phi matrix to get vocab_size\n",
        "    vocab_size = -1\n",
        "    for key in ordered_chunk_keys:\n",
        "        if key in time_series_topic_word_dist and time_series_topic_word_dist[key].ndim == 2:\n",
        "            vocab_size = time_series_topic_word_dist[key].shape[1]\n",
        "            break\n",
        "    if vocab_size == -1:\n",
        "        warnings.warn(\"Could not determine vocabulary size from topic-word distributions. Skipping topical change detection.\", UserWarning)\n",
        "        return detected_change_points_with_loo, visualization_data\n",
        "\n",
        "\n",
        "    # --- Iterate through time chunks and topics ---\n",
        "    # Start iteration from the first chunk where a full look-back window is available.\n",
        "    # ordered_chunk_keys must be sorted chronologically.\n",
        "    for t_idx in range(z_lookback, len(ordered_chunk_keys)):\n",
        "        # Current chunk key and its topic-word distribution matrix (Phi_t)\n",
        "        current_chunk_key = ordered_chunk_keys[t_idx]\n",
        "        phi_current_chunk_all_topics = time_series_topic_word_dist.get(current_chunk_key)\n",
        "\n",
        "        if phi_current_chunk_all_topics is None or phi_current_chunk_all_topics.size == 0:\n",
        "            warnings.warn(f\"Missing or empty topic-word distribution for chunk '{current_chunk_key}'. Skipping.\", UserWarning)\n",
        "            continue\n",
        "        if phi_current_chunk_all_topics.shape != (k_topics, vocab_size):\n",
        "            warnings.warn(f\"Shape mismatch for topic-word distribution in chunk '{current_chunk_key}'. \"\n",
        "                          f\"Expected ({k_topics}, {vocab_size}), got {phi_current_chunk_all_topics.shape}. Skipping.\", UserWarning)\n",
        "            continue\n",
        "\n",
        "        # Construct look-back topic-word distributions\n",
        "        look_back_phis_all_topics: List[np.ndarray] = []\n",
        "        # Iterate over the z_lookback preceding chunks.\n",
        "        for look_back_offset in range(1, z_lookback + 1):\n",
        "            look_back_chunk_key = ordered_chunk_keys[t_idx - look_back_offset]\n",
        "            phi_look_back_chunk = time_series_topic_word_dist.get(look_back_chunk_key)\n",
        "            if phi_look_back_chunk is not None and phi_look_back_chunk.size > 0 and \\\n",
        "               phi_look_back_chunk.shape == (k_topics, vocab_size):\n",
        "                look_back_phis_all_topics.append(phi_look_back_chunk)\n",
        "\n",
        "        # If not enough data in look-back window, skip this chunk.\n",
        "        if len(look_back_phis_all_topics) < z_lookback: # Or some other threshold, e.g., len < 1\n",
        "            # This condition means not all z_lookback chunks had valid data.\n",
        "            # Depending on strictness, one might require all z_lookback chunks.\n",
        "            # For now, if any look_back_phis were collected, proceed. If none, then skip.\n",
        "            if not look_back_phis_all_topics:\n",
        "                warnings.warn(f\"Insufficient valid data in look-back window for chunk '{current_chunk_key}'. Skipping.\", UserWarning)\n",
        "                continue\n",
        "\n",
        "        # Aggregate look-back phis by averaging (element-wise mean across the time dimension for look-back)\n",
        "        # Stack the list of KxV matrices into a ZxKxV tensor, then mean over Z axis.\n",
        "        # This results in one KxV matrix representing the average look-back topic-word distributions.\n",
        "        aggregated_look_back_phi_all_topics = np.mean(np.array(look_back_phis_all_topics), axis=0)\n",
        "\n",
        "        # Iterate over each topic k\n",
        "        for topic_id in range(k_topics):\n",
        "            # Sub-step 6.e.i: Construct topic-word vectors (probabilities)\n",
        "            # Current topic-word probability vector for topic_id in current_chunk_key\n",
        "            v_current_topic_prob: np.ndarray = phi_current_chunk_all_topics[topic_id, :]\n",
        "            # Averaged look-back topic-word probability vector for topic_id\n",
        "            v_lookback_topic_prob_avg: np.ndarray = aggregated_look_back_phi_all_topics[topic_id, :]\n",
        "\n",
        "            # Ensure vectors are 1D and non-empty\n",
        "            if v_current_topic_prob.size == 0 or v_lookback_topic_prob_avg.size == 0:\n",
        "                warnings.warn(f\"Empty topic vector for topic {topic_id} in chunk {current_chunk_key} or its lookback. Skipping.\", UserWarning)\n",
        "                visualization_data.append((current_chunk_key, topic_id, np.nan, np.nan)) # Record NaN for this point\n",
        "                continue\n",
        "\n",
        "            # Normalize to ensure they are valid probability distributions (sum to 1)\n",
        "            # This is important if averaging or other operations might slightly de-normalize.\n",
        "            # Phi matrices from Gensim should already be normalized.\n",
        "            v_current_topic_prob = (v_current_topic_prob + epsilon) / (v_current_topic_prob.sum() + vocab_size * epsilon)\n",
        "            v_lookback_topic_prob_avg = (v_lookback_topic_prob_avg + epsilon) / (v_lookback_topic_prob_avg.sum() + vocab_size * epsilon)\n",
        "\n",
        "            # Apply mixture to the look-back vector as per LaTeX context (Section 3.3)\n",
        "            # V'_lookback = gamma * V_lookback_avg + (1-gamma) * V_current\n",
        "            v_mixed_lookback_topic_prob: np.ndarray = (mixture_gamma * v_lookback_topic_prob_avg +\n",
        "                                                      (1 - mixture_gamma) * v_current_topic_prob)\n",
        "            # Renormalize the mixed vector\n",
        "            v_mixed_lookback_topic_prob = (v_mixed_lookback_topic_prob + epsilon) / \\\n",
        "                                          (v_mixed_lookback_topic_prob.sum() + vocab_size * epsilon)\n",
        "\n",
        "\n",
        "            # Sub-step 6.e.ii: Calculate observed cosine distance\n",
        "            # Distance = 1 - Similarity. scipy.spatial.distance.cosine computes 1 - sim.\n",
        "            # Ensure no NaN values in vectors before cosine distance calculation\n",
        "            if np.isnan(v_mixed_lookback_topic_prob).any() or np.isnan(v_current_topic_prob).any():\n",
        "                warnings.warn(f\"NaN values in topic vectors for topic {topic_id}, chunk {current_chunk_key}. Skipping distance calc.\", UserWarning)\n",
        "                visualization_data.append((current_chunk_key, topic_id, np.nan, np.nan))\n",
        "                continue\n",
        "\n",
        "            # Handle potential all-zero vectors which would lead to NaN in cosine distance\n",
        "            if np.all(v_mixed_lookback_topic_prob < epsilon) or np.all(v_current_topic_prob < epsilon):\n",
        "                 # If either vector is essentially zero, distance is undefined or maximal (1.0 or 2.0 depending on definition)\n",
        "                 # For probability vectors that should sum to 1, this implies an issue.\n",
        "                 # If they were normalized, they shouldn't be all zero unless vocab_size is 0.\n",
        "                 # Let's treat as maximal dissimilarity if one is zero and other is not.\n",
        "                 # If both are zero, distance is 0 by some conventions, or NaN.\n",
        "                 # Cosine distance for zero vectors is problematic.\n",
        "                 # Assuming our normalization with epsilon prevents true zero vectors if vocab_size > 0.\n",
        "                observed_cosine_dist = 1.0 # Max dissimilarity if one is effectively zero\n",
        "                if np.all(v_mixed_lookback_topic_prob < epsilon) and np.all(v_current_topic_prob < epsilon):\n",
        "                    observed_cosine_dist = 0.0 # Or np.nan, but 0.0 if identical zero vectors\n",
        "            else:\n",
        "                observed_cosine_dist: float = cosine_distance(v_mixed_lookback_topic_prob, v_current_topic_prob)\n",
        "\n",
        "            # Ensure observed_cosine_dist is not NaN (can happen if one vector is all zeros)\n",
        "            if np.isnan(observed_cosine_dist):\n",
        "                observed_cosine_dist = 1.0 # Default to max distance if NaN occurs\n",
        "\n",
        "            # Sub-step 6.e.iii & iv: Bootstrap procedure\n",
        "            bootstrap_distances: List[float] = []\n",
        "            # The distribution to sample from for bootstrap is the unmixed, averaged look-back probability vector.\n",
        "            p_bootstrap_sampling_dist = v_lookback_topic_prob_avg\n",
        "\n",
        "            # Ensure p_bootstrap_sampling_dist is a valid probability distribution for np.random.choice\n",
        "            if np.any(p_bootstrap_sampling_dist < 0) or not np.isclose(p_bootstrap_sampling_dist.sum(), 1.0):\n",
        "                 warnings.warn(f\"Bootstrap sampling distribution for topic {topic_id}, chunk {current_chunk_key} is invalid. Sum: {p_bootstrap_sampling_dist.sum()}. Skipping bootstrap.\", UserWarning)\n",
        "                 visualization_data.append((current_chunk_key, topic_id, observed_cosine_dist, np.nan))\n",
        "                 continue # Skip to next topic if bootstrap cannot be performed\n",
        "\n",
        "            for _ in range(B_bootstrap):\n",
        "                # Generate a bootstrap sample of word *counts* by drawing N tokens\n",
        "                # N = num_tokens_for_bootstrap_resampling_per_topic\n",
        "                # Indices are 0 to vocab_size-1\n",
        "                bootstrap_word_indices = np.random.choice(\n",
        "                    a=vocab_size, # Items to choose from (indices of vocabulary)\n",
        "                    size=num_tokens_for_bootstrap_resampling_per_topic, # Number of items to choose\n",
        "                    p=p_bootstrap_sampling_dist, # Probabilities associated with each item\n",
        "                    replace=True # Sample with replacement\n",
        "                )\n",
        "                # Create a count vector from these sampled word indices\n",
        "                v_bootstrap_count_vector = np.bincount(bootstrap_word_indices, minlength=vocab_size)\n",
        "                # Convert bootstrap count vector to a probability vector\n",
        "                v_bootstrap_prob_vector = (v_bootstrap_count_vector + epsilon) / \\\n",
        "                                          (v_bootstrap_count_vector.sum() + vocab_size * epsilon)\n",
        "\n",
        "                # Calculate cosine distance between this bootstrap probability vector and the current topic vector\n",
        "                if np.all(v_bootstrap_prob_vector < epsilon) or np.all(v_current_topic_prob < epsilon):\n",
        "                    dist_b = 1.0\n",
        "                    if np.all(v_bootstrap_prob_vector < epsilon) and np.all(v_current_topic_prob < epsilon):\n",
        "                        dist_b = 0.0\n",
        "                else:\n",
        "                    dist_b = cosine_distance(v_bootstrap_prob_vector, v_current_topic_prob)\n",
        "\n",
        "                if np.isnan(dist_b): dist_b = 1.0 # Handle potential NaNs\n",
        "                bootstrap_distances.append(dist_b)\n",
        "\n",
        "            # Sub-step 6.e.v: Determine critical distance and detect change\n",
        "            # Sort bootstrap distances to find the percentile.\n",
        "            bootstrap_distances.sort()\n",
        "            # Calculate the critical distance threshold: (1 - alpha)-th percentile of bootstrap distances.\n",
        "            # (B * (1-alpha)) is the index for the threshold.\n",
        "            critical_value_index = int(np.ceil(B_bootstrap * (1.0 - alpha_significance))) -1 # -1 for 0-based index\n",
        "            critical_value_index = max(0, min(critical_value_index, B_bootstrap - 1)) # Clamp index\n",
        "\n",
        "            critical_distance_threshold: float = bootstrap_distances[critical_value_index]\n",
        "\n",
        "            # Store data for visualization (all points)\n",
        "            visualization_data.append((current_chunk_key, topic_id, observed_cosine_dist, critical_distance_threshold))\n",
        "\n",
        "            # Detect change if observed distance is greater than critical threshold.\n",
        "            if observed_cosine_dist > critical_distance_threshold:\n",
        "                # Change detected for topic_id at current_chunk_key\n",
        "\n",
        "                # Sub-step 6.f: Calculate leave-one-out (LOO) word impacts\n",
        "                significant_loo_words: List[str] = []\n",
        "                if num_significant_words_loo > 0:\n",
        "                    word_impacts: List[Tuple[float, str]] = [] # (impact_score, word_string)\n",
        "\n",
        "                    # Iterate through each word index w in the vocabulary\n",
        "                    for word_idx in range(vocab_size):\n",
        "                        # Create temporary vectors by setting probability of word_idx to 0 and renormalizing\n",
        "                        v_mixed_lookback_loo = v_mixed_lookback_topic_prob.copy()\n",
        "                        prob_word_in_mixed_lookback = v_mixed_lookback_loo[word_idx]\n",
        "                        v_mixed_lookback_loo[word_idx] = 0.0\n",
        "                        # Renormalize only if the original probability was not already near zero and sum is not zero\n",
        "                        sum_mixed_lookback_loo = v_mixed_lookback_loo.sum()\n",
        "                        if sum_mixed_lookback_loo > epsilon: # Avoid division by zero\n",
        "                            v_mixed_lookback_loo /= sum_mixed_lookback_loo\n",
        "                        else: # If all other probs are zero, this vector is now zero\n",
        "                            v_mixed_lookback_loo.fill(0.0)\n",
        "\n",
        "\n",
        "                        v_current_loo = v_current_topic_prob.copy()\n",
        "                        prob_word_in_current = v_current_loo[word_idx]\n",
        "                        v_current_loo[word_idx] = 0.0\n",
        "                        sum_current_loo = v_current_loo.sum()\n",
        "                        if sum_current_loo > epsilon:\n",
        "                            v_current_loo /= sum_current_loo\n",
        "                        else:\n",
        "                            v_current_loo.fill(0.0)\n",
        "\n",
        "                        # Calculate cosine distance with word_idx excluded (or its prob set to 0 and renormalized)\n",
        "                        if np.all(v_mixed_lookback_loo < epsilon) or np.all(v_current_loo < epsilon):\n",
        "                            dist_loo = 1.0\n",
        "                            if np.all(v_mixed_lookback_loo < epsilon) and np.all(v_current_loo < epsilon):\n",
        "                                dist_loo = 0.0\n",
        "                        else:\n",
        "                            dist_loo = cosine_distance(v_mixed_lookback_loo, v_current_loo)\n",
        "\n",
        "                        if np.isnan(dist_loo): dist_loo = 1.0\n",
        "\n",
        "                        # Impact: D_original - D_loo. Positive impact means word contributed to difference.\n",
        "                        impact = observed_cosine_dist - dist_loo\n",
        "                        # Get word string from gensim_dictionary using its internal ID (word_idx here is vocab index)\n",
        "                        word_string = gensim_dictionary.id2token.get(word_idx, f\"UNK_ID_{word_idx}\")\n",
        "                        word_impacts.append((impact, word_string))\n",
        "\n",
        "                    # Sort words by impact score in descending order\n",
        "                    word_impacts.sort(key=lambda x: x[0], reverse=True)\n",
        "                    # Select top N significant words\n",
        "                    significant_loo_words = [word for impact, word in word_impacts[:num_significant_words_loo] if impact > 0] # Only positive impacts\n",
        "\n",
        "                # Store the detected change point with its details\n",
        "                detected_change_points_with_loo.append((\n",
        "                    current_chunk_key,\n",
        "                    topic_id,\n",
        "                    significant_loo_words,\n",
        "                    observed_cosine_dist,\n",
        "                    critical_distance_threshold\n",
        "                ))\n",
        "    # Return the list of detected change points and data for visualization\n",
        "    return detected_change_points_with_loo, visualization_data\n"
      ],
      "metadata": {
        "id": "jKAzIxDZACNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Document Filtering\n",
        "\n",
        "def filter_documents_for_llm(\n",
        "    detected_change_point_info: Tuple[str, int, List[str], float, float],\n",
        "    processed_df: pd.DataFrame, # DataFrame from Task 2 (with DatetimeIndex, article_id, full_text, and lemmatized columns)\n",
        "    llm_interpretation_params: Dict[str, Any], # For N_docs_filter\n",
        "    text_column_for_counting: str = \"full_text_lemmatized_list\" # Column in processed_df with list of lemmas\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Filters and selects documents relevant to a detected change point for LLM analysis.\n",
        "\n",
        "    For a given detected change point (including its chunk key and significant words):\n",
        "    1.  Identifies all documents within the specified time chunk.\n",
        "    2.  For each document in that chunk, counts the occurrences of the\n",
        "        significant words (case-insensitive matching against lemmatized content).\n",
        "    3.  Selects the top `N_docs_filter` documents with the highest counts.\n",
        "    4.  Returns the original 'full_text' of these selected documents.\n",
        "\n",
        "    Args:\n",
        "        detected_change_point_info (Tuple[str, int, List[str], float, float]):\n",
        "            A tuple containing information about a single detected change point:\n",
        "            (chunk_key_of_change (str, \"YYYY-MM\"),\n",
        "             topic_id (int),\n",
        "             list_of_significant_loo_words (List[str]),\n",
        "             observed_cosine_distance (float),\n",
        "             critical_distance_threshold (float)).\n",
        "        processed_df (pd.DataFrame):\n",
        "            The DataFrame containing all articles, preprocessed by Task 2.\n",
        "            Must have a DatetimeIndex, 'article_id', 'full_text', and the\n",
        "            column specified by `text_column_for_counting` (e.g., 'full_text_lemmatized_list').\n",
        "        llm_interpretation_params (Dict[str, Any]):\n",
        "            Parameters for LLM interpretation, must include \"N_docs_filter\" (int).\n",
        "        text_column_for_counting (str):\n",
        "            The name of the column in `processed_df` that contains the list of\n",
        "            lemmatized tokens for each document, used for counting significant words.\n",
        "            Defaults to \"full_text_lemmatized_list\".\n",
        "\n",
        "    Returns:\n",
        "        List[str]:\n",
        "            A list containing the original 'full_text' of the top N selected documents.\n",
        "            The list will contain fewer than N documents if the chunk has fewer\n",
        "            documents or if fewer meet the criteria.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If required parameters or DataFrame columns are missing/invalid.\n",
        "        TypeError: If input types are incorrect.\n",
        "    \"\"\"\n",
        "    # --- Input Validation and Parameter Extraction ---\n",
        "    # Validate detected_change_point_info structure\n",
        "    if not (isinstance(detected_change_point_info, tuple) and len(detected_change_point_info) == 5):\n",
        "        raise TypeError(\"'detected_change_point_info' must be a tuple of 5 elements.\")\n",
        "\n",
        "    # Extract information from detected_change_point_info\n",
        "    chunk_key_of_change: str = detected_change_point_info[0]\n",
        "    # topic_id is available but not directly used in this filtering logic, only chunk_key and significant_words\n",
        "    # topic_id: int = detected_change_point_info[1]\n",
        "    list_of_significant_loo_words: List[str] = detected_change_point_info[2]\n",
        "\n",
        "    # Validate types of extracted info\n",
        "    if not isinstance(chunk_key_of_change, str):\n",
        "        raise TypeError(\"chunk_key_of_change in 'detected_change_point_info' must be a string.\")\n",
        "    if not isinstance(list_of_significant_loo_words, list) or \\\n",
        "       not all(isinstance(word, str) for word in list_of_significant_loo_words):\n",
        "        raise TypeError(\"list_of_significant_loo_words in 'detected_change_point_info' must be a list of strings.\")\n",
        "\n",
        "    # Validate processed_df\n",
        "    if not isinstance(processed_df, pd.DataFrame):\n",
        "        raise TypeError(\"'processed_df' must be a pandas.DataFrame.\")\n",
        "    if not isinstance(processed_df.index, pd.DatetimeIndex):\n",
        "        raise TypeError(\"'processed_df' must have a DatetimeIndex.\")\n",
        "    required_df_cols = ['article_id', 'full_text', text_column_for_counting]\n",
        "    for col in required_df_cols:\n",
        "        if col not in processed_df.columns:\n",
        "            raise ValueError(f\"'processed_df' is missing required column: {col}\")\n",
        "\n",
        "    # Validate llm_interpretation_params\n",
        "    if not isinstance(llm_interpretation_params, dict) or \"N_docs_filter\" not in llm_interpretation_params:\n",
        "        raise ValueError(\"'llm_interpretation_params' must be a dict and contain 'N_docs_filter'.\")\n",
        "    N_docs_filter: int = llm_interpretation_params[\"N_docs_filter\"]\n",
        "    if not isinstance(N_docs_filter, int) or N_docs_filter <= 0:\n",
        "        raise ValueError(\"'N_docs_filter' must be a positive integer.\")\n",
        "\n",
        "    # --- Document Filtering Logic ---\n",
        "    # Convert chunk_key (\"YYYY-MM\") to a pandas Period object to define date range\n",
        "    try:\n",
        "        # Create a Period object for the specified month.\n",
        "        period = pd.Period(chunk_key_of_change, freq='M')\n",
        "        # Get the start timestamp of the period.\n",
        "        chunk_start_date = period.start_time\n",
        "        # Get the end timestamp of the period.\n",
        "        chunk_end_date = period.end_time\n",
        "    except ValueError as e:\n",
        "        # Raise ValueError if chunk_key_of_change is not a valid period string.\n",
        "        raise ValueError(f\"Invalid 'chunk_key_of_change': {chunk_key_of_change}. Error: {e}\")\n",
        "\n",
        "    # Filter processed_df for documents within the identified time chunk.\n",
        "    # Ensure index is sorted for potentially faster slicing, though pandas handles it.\n",
        "    # if not processed_df.index.is_monotonic_increasing:\n",
        "    #     processed_df = processed_df.sort_index() # Not strictly necessary for boolean indexing\n",
        "\n",
        "    # Create a boolean mask for documents within the chunk's date range.\n",
        "    # The range is inclusive of start_time and end_time of the period.\n",
        "    docs_in_chunk_mask = (processed_df.index >= chunk_start_date) & (processed_df.index <= chunk_end_date)\n",
        "    # Select the subset of the DataFrame corresponding to the current chunk.\n",
        "    chunk_documents_df = processed_df[docs_in_chunk_mask]\n",
        "\n",
        "    # If no documents are found in the chunk, return an empty list.\n",
        "    if chunk_documents_df.empty:\n",
        "        warnings.warn(f\"No documents found in chunk '{chunk_key_of_change}'. Returning empty list for LLM.\", UserWarning)\n",
        "        return []\n",
        "\n",
        "    # Normalize significant words to lowercase for case-insensitive counting.\n",
        "    # Assumes significant words are already lemmas.\n",
        "    significant_words_lower = {word.lower() for word in list_of_significant_loo_words}\n",
        "\n",
        "    # If there are no significant words to count, behavior might need to be defined.\n",
        "    # For now, if list is empty, all counts will be 0.\n",
        "    # The paper implies significant words are found and used.\n",
        "    if not significant_words_lower:\n",
        "        warnings.warn(f\"No significant words provided for chunk '{chunk_key_of_change}'. \"\n",
        "                      f\"Document selection will be based on original order if all counts are zero.\", UserWarning)\n",
        "        # Fallback: select the first N_docs_filter documents from the chunk if no significant words.\n",
        "        # This ensures some documents are returned if the chunk is not empty.\n",
        "        selected_article_ids = chunk_documents_df['article_id'].head(N_docs_filter).tolist()\n",
        "        selected_full_texts = processed_df.loc[processed_df['article_id'].isin(selected_article_ids), 'full_text'].tolist()\n",
        "        # Ensure the order matches selected_article_ids if `loc` reorders.\n",
        "        # A more robust way for ordered selection:\n",
        "        if selected_article_ids: # if any articles were selected\n",
        "            # Create a mapping from article_id to its full_text\n",
        "            id_to_text_map = pd.Series(processed_df['full_text'].values, index=processed_df['article_id']).to_dict()\n",
        "            # Retrieve texts in the order of selected_article_ids\n",
        "            selected_full_texts = [id_to_text_map[aid] for aid in selected_article_ids if aid in id_to_text_map]\n",
        "        else:\n",
        "            selected_full_texts = []\n",
        "        return selected_full_texts\n",
        "\n",
        "\n",
        "    # List to store (article_id, significant_word_count) tuples.\n",
        "    doc_scores: List[Tuple[str, int]] = []\n",
        "\n",
        "    # Iterate through documents in the current chunk.\n",
        "    for index, row in chunk_documents_df.iterrows():\n",
        "        # Get the article ID.\n",
        "        article_id = row['article_id']\n",
        "        # Get the list of lemmatized tokens for the current document.\n",
        "        # Ensure this column contains lists of strings (lemmas).\n",
        "        lemmatized_tokens: List[str] = row[text_column_for_counting]\n",
        "\n",
        "        # Validate that lemmatized_tokens is a list of strings.\n",
        "        if not isinstance(lemmatized_tokens, list) or \\\n",
        "           (lemmatized_tokens and not all(isinstance(token, str) for token in lemmatized_tokens)):\n",
        "            warnings.warn(f\"Column '{text_column_for_counting}' for article_id '{article_id}' \"\n",
        "                          f\"in chunk '{chunk_key_of_change}' is not a list of strings. Skipping word count for this doc.\", UserWarning)\n",
        "            current_doc_score = 0\n",
        "        else:\n",
        "            # Normalize document tokens to lowercase for counting.\n",
        "            doc_tokens_lower = [token.lower() for token in lemmatized_tokens]\n",
        "            # Count occurrences of significant words in the document's tokens.\n",
        "            # Using Counter for potentially better performance with many significant words,\n",
        "            # though a simple loop is also fine here.\n",
        "            token_counts_in_doc = Counter(doc_tokens_lower)\n",
        "            current_doc_score = 0\n",
        "            # Sum counts for each significant word found in the document.\n",
        "            for sig_word in significant_words_lower:\n",
        "                current_doc_score += token_counts_in_doc[sig_word]\n",
        "\n",
        "        # Append the (article_id, score) tuple to the list.\n",
        "        doc_scores.append((article_id, current_doc_score))\n",
        "\n",
        "    # Sort documents by their significant word count in descending order.\n",
        "    # If counts are equal, original order (from chunk_documents_df) is preserved due to stable sort.\n",
        "    doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Select the article_ids of the top N_docs_filter documents.\n",
        "    # Take at most N_docs_filter, or fewer if fewer documents were scored.\n",
        "    top_n_article_ids: List[str] = [article_id for article_id, score in doc_scores[:N_docs_filter]]\n",
        "\n",
        "    # Retrieve the original 'full_text' for these selected article_ids.\n",
        "    # Use .loc to ensure we get texts for these specific IDs and handle potential missing IDs robustly.\n",
        "    # We need to preserve the order from top_n_article_ids.\n",
        "    if not top_n_article_ids: # If no documents scored (e.g., all counts were 0 and no significant words)\n",
        "        return []\n",
        "\n",
        "    # Create a mapping from article_id to its full_text from the original processed_df\n",
        "    # to ensure we can retrieve texts in the correct order of top_n_article_ids.\n",
        "    # This is more robust than relying on `processed_df.loc[...].tolist()` which might not preserve order.\n",
        "    id_to_full_text_map = pd.Series(processed_df['full_text'].values, index=processed_df['article_id']).to_dict()\n",
        "\n",
        "    # Retrieve full texts in the order of `top_n_article_ids`.\n",
        "    selected_documents_full_text: List[str] = []\n",
        "    for aid in top_n_article_ids:\n",
        "        if aid in id_to_full_text_map:\n",
        "            selected_documents_full_text.append(id_to_full_text_map[aid])\n",
        "        else:\n",
        "            # This should not happen if article_ids came from processed_df.\n",
        "            warnings.warn(f\"Article ID '{aid}' selected for LLM input not found in the main 'processed_df'. Skipping.\", UserWarning)\n",
        "\n",
        "    # Return the list of selected full texts.\n",
        "    return selected_documents_full_text\n"
      ],
      "metadata": {
        "id": "Gkv-8dO8BROJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: LLM (Llama 3.1 8B) Setup\n",
        "\n",
        "def setup_llm_model_and_tokenizer(\n",
        "    llm_model_identifier: str, # E.g., \"meta-llama/Llama-3.1-8B-Instruct\" or local path\n",
        "    quantization_config: Optional[Dict[str, Any]] = None, # e.g., {\"load_in_8bit\": True} or {\"load_in_4bit\": True, \"bnb_4bit_quant_type\": \"nf4\", ...}\n",
        "    auth_token: Optional[str] = None, # Hugging Face auth token if needed for private/gated models\n",
        "    trust_remote_code: bool = True, # Often needed for newer models\n",
        "    use_cache: bool = True # Whether to use the simple global cache\n",
        ") -> Tuple[PreTrainedModel, PreTrainedTokenizerBase]:\n",
        "    \"\"\"\n",
        "    Sets up and loads a specified Large Language Model (LLM) and its tokenizer.\n",
        "\n",
        "    This function handles:\n",
        "    1.  Loading the pre-trained model weights and tokenizer from a given identifier\n",
        "        (Hugging Face Hub name or local path).\n",
        "    2.  Attempting to use a GPU (CUDA) if available, falling back to CPU with a warning.\n",
        "    3.  Optional quantization (e.g., 8-bit or 4-bit) to reduce memory footprint,\n",
        "        configurable via `quantization_config`.\n",
        "    4.  Optional caching of loaded models/tokenizers to speed up repeated calls.\n",
        "\n",
        "    Args:\n",
        "        llm_model_identifier (str):\n",
        "            The identifier for the LLM model. This can be a model ID from the\n",
        "            Hugging Face Hub (e.g., \"meta-llama/Llama-3.1-8B-Instruct\") or a path\n",
        "            to a local directory containing the model weights and configuration.\n",
        "        quantization_config (Optional[Dict[str, Any]]):\n",
        "            A dictionary specifying quantization parameters.\n",
        "            If `{\"load_in_8bit\": True}`, loads the model in 8-bit.\n",
        "            If `{\"load_in_4bit\": True, ...}`, loads in 4-bit with further options\n",
        "            like `bnb_4bit_quant_type`, `bnb_4bit_compute_dtype`.\n",
        "            If None (default), loads in full precision (e.g., float16 or bfloat16).\n",
        "        auth_token (Optional[str]):\n",
        "            Hugging Face authentication token. Required for accessing gated models\n",
        "            or private repositories on the Hub. Can often be set via environment\n",
        "            variable `HF_TOKEN` or `huggingface-cli login`.\n",
        "        trust_remote_code (bool):\n",
        "            Whether to allow execution of remote code present in the model's\n",
        "            repository on the Hugging Face Hub. Required for some models.\n",
        "            Defaults to True for convenience with newer Llama models.\n",
        "        use_cache (bool):\n",
        "            If True, uses a simple global dictionary to cache loaded models and\n",
        "            tokenizers by `llm_model_identifier` and quantization settings to\n",
        "            avoid redundant loading in the same Python session.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[PreTrainedModel, PreTrainedTokenizerBase]:\n",
        "            A tuple containing the loaded LLM (Hugging Face PreTrainedModel)\n",
        "            and its corresponding tokenizer (Hugging Face PreTrainedTokenizerBase).\n",
        "\n",
        "    Raises:\n",
        "        OSError: If the model or tokenizer cannot be loaded (e.g., not found,\n",
        "                 network issues, permission errors).\n",
        "        ImportError: If required libraries like `torch` or `transformers`\n",
        "                     (or `bitsandbytes`, `accelerate` for quantization) are not installed.\n",
        "        RuntimeError: For CUDA-related issues if GPU is selected but unavailable/misconfigured.\n",
        "    \"\"\"\n",
        "    # --- Cache Check ---\n",
        "    # Create a cache key based on model identifier and quantization to ensure\n",
        "    # different configurations are cached separately.\n",
        "    quant_str = str(sorted(quantization_config.items())) if quantization_config else \"None\"\n",
        "    cache_key = f\"{llm_model_identifier}_{quant_str}\"\n",
        "\n",
        "    if use_cache and cache_key in _LLM_CACHE:\n",
        "        # If model and tokenizer are already cached, return them.\n",
        "        print(f\"LLM Setup: Loading '{llm_model_identifier}' with quantization '{quant_str}' from cache.\")\n",
        "        return _LLM_CACHE[cache_key]\n",
        "\n",
        "    # --- Device Selection ---\n",
        "    # Check if CUDA (GPU) is available and select device accordingly.\n",
        "    if torch.cuda.is_available():\n",
        "        # Set device to the first available CUDA GPU.\n",
        "        device = torch.device(\"cuda:0\")\n",
        "        # Get the name of the GPU.\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        # Print information about the selected GPU.\n",
        "        print(f\"LLM Setup: CUDA is available. Using GPU: {gpu_name}\")\n",
        "    else:\n",
        "        # Set device to CPU if CUDA is not available.\n",
        "        device = torch.device(\"cpu\")\n",
        "        # Issue a warning that CPU will be used, which is slow for large models.\n",
        "        warnings.warn(\n",
        "            \"LLM Setup: CUDA not available. Falling back to CPU. \"\n",
        "            \"Inference with an 8B model on CPU will be very slow.\",\n",
        "            UserWarning\n",
        "        )\n",
        "        print(\"LLM Setup: Using CPU.\")\n",
        "\n",
        "    # --- Quantization Setup (if specified) ---\n",
        "    # Initialize bnb_config for BitsAndBytes quantization.\n",
        "    bnb_config: Optional[BitsAndBytesConfig] = None\n",
        "    # Check if quantization_config is provided.\n",
        "    if quantization_config:\n",
        "        # Check if 4-bit quantization is requested.\n",
        "        if quantization_config.get(\"load_in_4bit\", False):\n",
        "            # Ensure bitsandbytes is available for 4-bit quantization.\n",
        "            try:\n",
        "                import bitsandbytes\n",
        "            except ImportError:\n",
        "                raise ImportError(\"bitsandbytes library is required for 4-bit quantization. \"\n",
        "                                  \"Please install it: pip install bitsandbytes\")\n",
        "            # Configure 4-bit quantization.\n",
        "            bnb_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_quant_type=quantization_config.get(\"bnb_4bit_quant_type\", \"nf4\"), # \"nf4\" is a common choice\n",
        "                bnb_4bit_compute_dtype=getattr(torch, quantization_config.get(\"bnb_4bit_compute_dtype\", \"float16\")), # e.g., torch.bfloat16 or torch.float16\n",
        "                bnb_4bit_use_double_quant=quantization_config.get(\"bnb_4bit_use_double_quant\", False)\n",
        "            )\n",
        "            # Print information about 4-bit quantization.\n",
        "            print(f\"LLM Setup: Applying 4-bit quantization with config: {quantization_config}\")\n",
        "        # Check if 8-bit quantization is requested.\n",
        "        elif quantization_config.get(\"load_in_8bit\", False):\n",
        "            # Ensure bitsandbytes is available for 8-bit quantization.\n",
        "            try:\n",
        "                import bitsandbytes\n",
        "            except ImportError:\n",
        "                raise ImportError(\"bitsandbytes library is required for 8-bit quantization. \"\n",
        "                                  \"Please install it: pip install bitsandbytes\")\n",
        "            # Configure 8-bit quantization.\n",
        "            bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "            # Print information about 8-bit quantization.\n",
        "            print(\"LLM Setup: Applying 8-bit quantization.\")\n",
        "        # Ensure accelerate is installed if quantization is used, as it's often a dependency.\n",
        "        if bnb_config:\n",
        "            try:\n",
        "                import accelerate\n",
        "            except ImportError:\n",
        "                raise ImportError(\"accelerate library is often required with bitsandbytes quantization. \"\n",
        "                                  \"Please install it: pip install accelerate\")\n",
        "\n",
        "\n",
        "    # --- Load Tokenizer ---\n",
        "    # Print status message for loading tokenizer.\n",
        "    print(f\"LLM Setup: Loading tokenizer for '{llm_model_identifier}'...\")\n",
        "    try:\n",
        "        # Load the tokenizer using AutoTokenizer.from_pretrained.\n",
        "        # `token` argument was Hugging Face pre-v4.22, now `use_auth_token` or `token` (for new versions).\n",
        "        # For robustness with various transformers versions:\n",
        "        tokenizer_args = {\"trust_remote_code\": trust_remote_code}\n",
        "        if auth_token:\n",
        "            tokenizer_args[\"token\"] = auth_token # Or \"use_auth_token\" for older versions\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(llm_model_identifier, **tokenizer_args)\n",
        "        # Set padding token if not already set (common for Llama models).\n",
        "        # Using EOS token as PAD token is a common practice if a PAD token is not explicitly defined.\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "            # Print status message for setting pad_token.\n",
        "            print(f\"LLM Setup: Tokenizer pad_token not set. Using eos_token ('{tokenizer.eos_token}') as pad_token.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Raise OSError if tokenizer loading fails.\n",
        "        raise OSError(f\"Failed to load tokenizer for '{llm_model_identifier}'. Error: {e}\")\n",
        "    # Print status message for successful tokenizer loading.\n",
        "    print(f\"LLM Setup: Tokenizer for '{llm_model_identifier}' loaded successfully.\")\n",
        "\n",
        "    # --- Load Model ---\n",
        "    # Print status message for loading model.\n",
        "    print(f\"LLM Setup: Loading model '{llm_model_identifier}'... This may take a while.\")\n",
        "    try:\n",
        "        # Prepare arguments for model loading.\n",
        "        model_args = {\"trust_remote_code\": trust_remote_code}\n",
        "        if auth_token:\n",
        "            model_args[\"token\"] = auth_token # Or \"use_auth_token\"\n",
        "\n",
        "        # Add quantization config if it was created.\n",
        "        if bnb_config:\n",
        "            model_args[\"quantization_config\"] = bnb_config\n",
        "            # device_map=\"auto\" is often used with quantization for optimal layer placement.\n",
        "            model_args[\"device_map\"] = \"auto\" # Requires accelerate\n",
        "        else:\n",
        "            # If not quantizing, explicitly set torch_dtype for memory efficiency (e.g., float16)\n",
        "            # and move the model to the selected device after loading.\n",
        "            # Common dtypes for LLMs are float16 or bfloat16 (if supported).\n",
        "            model_args[\"torch_dtype\"] = torch.float16 # Or torch.bfloat16 if hardware supports\n",
        "\n",
        "        # Load the pre-trained model using AutoModelForCausalLM.from_pretrained.\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            llm_model_identifier,\n",
        "            **model_args\n",
        "        )\n",
        "\n",
        "        # If not using device_map (i.e., no quantization or manual device placement),\n",
        "        # explicitly move the model to the selected device.\n",
        "        if not model_args.get(\"device_map\"):\n",
        "            model.to(device)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Raise OSError if model loading fails.\n",
        "        raise OSError(f\"Failed to load model '{llm_model_identifier}'. Error: {e}\")\n",
        "\n",
        "    # Set the model to evaluation mode (disables dropout, etc.).\n",
        "    model.eval()\n",
        "    # Print status message for successful model loading and device placement.\n",
        "    print(f\"LLM Setup: Model '{llm_model_identifier}' loaded successfully and set to evaluation mode.\")\n",
        "    if model_args.get(\"device_map\"):\n",
        "         print(f\"LLM Setup: Model device map: {model.hf_device_map}\")\n",
        "    else:\n",
        "         print(f\"LLM Setup: Model moved to device: {next(model.parameters()).device}\")\n",
        "\n",
        "\n",
        "    # --- Cache Result ---\n",
        "    # If caching is enabled, store the loaded model and tokenizer.\n",
        "    if use_cache:\n",
        "        _LLM_CACHE[cache_key] = (model, tokenizer)\n",
        "        # Print status message for caching.\n",
        "        print(f\"LLM Setup: Model and tokenizer for '{cache_key}' cached.\")\n",
        "\n",
        "    # Return the loaded model and tokenizer.\n",
        "    return model, tokenizer\n"
      ],
      "metadata": {
        "id": "yKCrShztCCBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: LLM Prompt Engineering\n",
        "\n",
        "NARRATIVE_SHIFT_PROMPT_TEMPLATE: str = \"\"\"## You are an expert journalist. You will be asked to explain, why a topical change in a corpus of news articles has has been found and what the change consists of. To fulfill this task, you will be provided information from other text analysis models such as parts of the output of a RollingLDA topic model.\n",
        "## Whenever you are asked to analyze a ânarrativeâ, assume the definition of a narrative that is laid out in the paper âThe Narrative Policy Framework: A Traveler's Guide to Policy Stories\". Specifically, respect and apply the following definitory aspects of a narrative: \"The NPF posits that while the content of narratives may vary across contexts, structural elements are generalizable. For example, the content of a story about fracking told by a Scottish environmentalist is certainly different from the story told by a right-wing populist who attacks a public agency in Switzerland. However, these stories share common structural elements: They take place in a setting, contain characters, have a plot, and often champion a moral.\" Keep in mind that a moral must feature a value judgement. When asked to specifiy a moral of a narratives, you must refer to this value judgement or note that there is no moral and thus no narrative! A narrative change must satisfy the four structural criteria, while a content change can simply be caused by an event that shifts the focus of the topic without a clear narrative. Your goal is to determine if a narrative change occurred or if it was a mere content change.\n",
        "## Please explain an apparent change within a RollingLDA topic that has occurred in {date_of_change}\n",
        "## The following topic top words might give you an idea of what the topic was about before the change: {top_words_before_str}\n",
        "## The following topic top words might give you an idea of what the topic was about after the change: {top_words_after_str}\n",
        "## The following words were found to be significant to the detected change: {significant_loo_words_str}\n",
        "## The following are those articles from the period that make the most use of the words found to be significant to the detected change:\n",
        "{formatted_articles_section}\n",
        "## Provide your output in a strict JSON format. First, summarize each article in one sentence: {{\"summaries\": [{{\"article_1\": ...}}, {{\"article_2\": ...}}, ...]}}. Then formulate what the topic was about before and after the change based on the topic top words, emphasizing the changes induced to the topic, judged by the articles and the change words: {{\"topic_change\": ...}}. Explain how this change in topic indicates a shift in narrative. How did the narrative shift? {{\"narrative_before\": \"Before the change, the narrative centered around ...\"}}, {{\"narrative_after\": \"After the change, the narrative centers around ...\"}}. Finally, walk through the four structural criteria that true narratives must satisfy according to the Narrative Policy Framework and confirm or disconfirm their existence in the narrative after the break by briefly naming what they are in the texts provided {{ânarrative_criteria\": [{{âsettingâ: ...}}, {{âcharactersâ: ...}}, {{\"plot\": ...}}, {{\"moralâ: ...}}]}}. Make sure to specify the exact source of the moral judgement that you may have found. Lastly, make a final judgement if there is a narrative shift to be found with {{\"true_narrativeâ: True/False}}. Do not answer in anything but JSON.\"\"\"\n",
        "\n",
        "def construct_llm_prompt_for_narrative_analysis(\n",
        "    date_of_change: str,\n",
        "    top_words_before_change: List[str], # Expecting 10 words\n",
        "    top_words_after_change: List[str],  # Expecting 10 words\n",
        "    significant_loo_words: List[str],\n",
        "    filtered_article_texts: List[str]  # Expecting N_docs_filter (e.g., 5) full texts\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Constructs the full prompt string for LLM-based narrative analysis.\n",
        "\n",
        "    This function takes dynamic information related to a detected change point\n",
        "    (date, topic words, significant words, filtered articles) and injects it\n",
        "    into a predefined, structured prompt template. The template includes\n",
        "    instructions for the LLM, the definition of a narrative according to the\n",
        "    Narrative Policy Framework (NPF), and a strict JSON output format requirement.\n",
        "\n",
        "    Args:\n",
        "        date_of_change (str):\n",
        "            The date or time chunk identifier for the detected change\n",
        "            (e.g., \"2023-03\" or \"July 2018\").\n",
        "        top_words_before_change (List[str]):\n",
        "            A list of top words representing the topic before the change.\n",
        "            Typically 10 words.\n",
        "        top_words_after_change (List[str]):\n",
        "            A list of top words representing the topic after the change.\n",
        "            Typically 10 words.\n",
        "        significant_loo_words (List[str]):\n",
        "            A list of words identified as significant by the leave-one-out\n",
        "            impact analysis.\n",
        "        filtered_article_texts (List[str]):\n",
        "            A list containing the full original text of the N (e.g., 5) most\n",
        "            relevant documents selected for LLM analysis.\n",
        "\n",
        "    Returns:\n",
        "        str:\n",
        "            The fully formatted prompt string ready to be sent to the LLM.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If input arguments are not of the expected types.\n",
        "        ValueError: If input lists are empty where content is expected, or if\n",
        "                    the number of articles does not match expectations (e.g., 0 articles).\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Check type of date_of_change.\n",
        "    if not isinstance(date_of_change, str) or not date_of_change:\n",
        "        raise ValueError(\"'date_of_change' must be a non-empty string.\")\n",
        "    # Check type and content of top_words_before_change.\n",
        "    if not isinstance(top_words_before_change, list) or \\\n",
        "       not all(isinstance(w, str) for w in top_words_before_change): # Allow empty list if no top words\n",
        "        raise TypeError(\"'top_words_before_change' must be a list of strings.\")\n",
        "    # Check type and content of top_words_after_change.\n",
        "    if not isinstance(top_words_after_change, list) or \\\n",
        "       not all(isinstance(w, str) for w in top_words_after_change): # Allow empty list\n",
        "        raise TypeError(\"'top_words_after_change' must be a list of strings.\")\n",
        "    # Check type and content of significant_loo_words.\n",
        "    if not isinstance(significant_loo_words, list) or \\\n",
        "       not all(isinstance(w, str) for w in significant_loo_words): # Allow empty list\n",
        "        raise TypeError(\"'significant_loo_words' must be a list of strings.\")\n",
        "    # Check type and content of filtered_article_texts.\n",
        "    if not isinstance(filtered_article_texts, list) or \\\n",
        "       not all(isinstance(text, str) for text in filtered_article_texts):\n",
        "        raise TypeError(\"'filtered_article_texts' must be a list of strings.\")\n",
        "    # Ensure there are articles to present, as the prompt structure relies on them.\n",
        "    if not filtered_article_texts:\n",
        "        raise ValueError(\"'filtered_article_texts' cannot be empty for this prompt structure.\")\n",
        "\n",
        "    # --- Format Dynamic Content ---\n",
        "    # Join lists of words into comma-separated strings.\n",
        "    # Handle empty lists gracefully by producing an empty string or \"None\".\n",
        "    top_words_before_str = \", \".join(top_words_before_change) if top_words_before_change else \"None provided\"\n",
        "    top_words_after_str = \", \".join(top_words_after_change) if top_words_after_change else \"None provided\"\n",
        "    significant_loo_words_str = \", \".join(significant_loo_words) if significant_loo_words else \"None provided\"\n",
        "\n",
        "    # Format the articles section.\n",
        "    # Each article should be clearly delineated.\n",
        "    # Using f-strings with triple quotes for multi-line article texts.\n",
        "    formatted_articles_list = []\n",
        "    # Iterate through the filtered article texts with an index.\n",
        "    for i, article_text in enumerate(filtered_article_texts):\n",
        "        # Append each formatted article string to the list.\n",
        "        # Using a clear marker for each article.\n",
        "        formatted_articles_list.append(f\"### Article {i+1}:\\n```text\\n{article_text}\\n```\")\n",
        "    # Join the list of formatted article strings into a single multi-line string.\n",
        "    formatted_articles_section = \"\\n\\n\".join(formatted_articles_list)\n",
        "\n",
        "    # --- Construct Full Prompt ---\n",
        "    # Use f-string to inject all dynamic parts into the template.\n",
        "    # Note: The JSON examples in the prompt template use double curly braces {{...}}\n",
        "    # to escape them in an f-string, so they appear as single braces in the final prompt.\n",
        "    full_prompt = NARRATIVE_SHIFT_PROMPT_TEMPLATE.format(\n",
        "        date_of_change=date_of_change,\n",
        "        top_words_before_str=top_words_before_str,\n",
        "        top_words_after_str=top_words_after_str,\n",
        "        significant_loo_words_str=significant_loo_words_str,\n",
        "        formatted_articles_section=formatted_articles_section\n",
        "    )\n",
        "\n",
        "    # Return the constructed prompt string.\n",
        "    return full_prompt\n"
      ],
      "metadata": {
        "id": "cg7JG7BdDHsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: LLM Analysis\n",
        "\n",
        "def _validate_parsed_llm_json(parsed_json: Dict[str, Any], num_expected_articles: int) -> Tuple[bool, List[str]]:\n",
        "    \"\"\"\n",
        "    Validates the structure and types of the parsed JSON from the LLM response.\n",
        "\n",
        "    Args:\n",
        "        parsed_json (Dict[str, Any]): The Python dictionary parsed from LLM's JSON output.\n",
        "        num_expected_articles (int): The number of articles provided to the LLM,\n",
        "                                     used to validate the 'summaries' list length.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[bool, List[str]]:\n",
        "            - is_valid (bool): True if the JSON conforms to the expected schema, False otherwise.\n",
        "            - errors (List[str]): A list of validation error messages if not valid.\n",
        "    \"\"\"\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Check top-level keys and types\n",
        "    for key, expected_type in EXPECTED_LLM_JSON_SCHEMA.items():\n",
        "        if key not in parsed_json:\n",
        "            errors.append(f\"Missing required key: '{key}'.\")\n",
        "            continue # Skip further checks for this key if missing\n",
        "        if not isinstance(parsed_json[key], expected_type):\n",
        "            errors.append(f\"Key '{key}' has incorrect type. Expected {expected_type}, got {type(parsed_json[key])}.\")\n",
        "\n",
        "    # Validate 'summaries' structure if present and type is correct\n",
        "    if \"summaries\" in parsed_json and isinstance(parsed_json[\"summaries\"], list):\n",
        "        if len(parsed_json[\"summaries\"]) != num_expected_articles:\n",
        "            errors.append(f\"Key 'summaries' list length mismatch. Expected {num_expected_articles} summaries, \"\n",
        "                          f\"got {len(parsed_json['summaries'])}.\")\n",
        "        for i, summary_item in enumerate(parsed_json[\"summaries\"]):\n",
        "            if not isinstance(summary_item, dict):\n",
        "                errors.append(f\"'summaries' item at index {i} is not a dictionary.\")\n",
        "                continue\n",
        "            # Expecting keys like \"article_1\", \"article_2\", ...\n",
        "            expected_article_key = f\"article_{i+1}\"\n",
        "            if expected_article_key not in summary_item:\n",
        "                errors.append(f\"'summaries' item at index {i} missing key '{expected_article_key}'.\")\n",
        "            elif not isinstance(summary_item[expected_article_key], str):\n",
        "                errors.append(f\"Value for '{expected_article_key}' in 'summaries' item {i} is not a string.\")\n",
        "\n",
        "    # Validate 'narrative_criteria' structure if present and type is correct\n",
        "    if \"narrative_criteria\" in parsed_json and isinstance(parsed_json[\"narrative_criteria\"], list):\n",
        "        if len(parsed_json[\"narrative_criteria\"]) != 4: # Setting, Characters, Plot, Moral\n",
        "            errors.append(f\"Key 'narrative_criteria' list length mismatch. Expected 4 items, got {len(parsed_json['narrative_criteria'])}.\")\n",
        "        else:\n",
        "            for i, criteria_item in enumerate(parsed_json[\"narrative_criteria\"]):\n",
        "                if not isinstance(criteria_item, dict):\n",
        "                    errors.append(f\"'narrative_criteria' item at index {i} is not a dictionary.\")\n",
        "                    continue\n",
        "                # Check if the dictionary has one of the expected NPF element keys\n",
        "                found_npf_key = False\n",
        "                for npf_key in EXPECTED_NARRATIVE_CRITERIA_KEYS:\n",
        "                    if npf_key in criteria_item:\n",
        "                        found_npf_key = True\n",
        "                        if not isinstance(criteria_item[npf_key], str): # Assuming all NPF elements are described as strings\n",
        "                             errors.append(f\"Value for NPF element '{npf_key}' in 'narrative_criteria' item {i} is not a string.\")\n",
        "                        break # Found an NPF key for this item\n",
        "                if not found_npf_key:\n",
        "                     errors.append(f\"'narrative_criteria' item at index {i} does not contain any of the expected NPF keys: {EXPECTED_NARRATIVE_CRITERIA_KEYS}.\")\n",
        "\n",
        "\n",
        "    is_valid = not errors\n",
        "    return is_valid, errors\n",
        "\n",
        "\n",
        "def perform_llm_analysis_on_change_point(\n",
        "    full_prompt_string: str,\n",
        "    llm_model: PreTrainedModel,\n",
        "    llm_tokenizer: PreTrainedTokenizerBase,\n",
        "    llm_interpretation_params: Dict[str, Any], # For temperature\n",
        "    max_new_tokens_generation: int = 2048, # Max tokens for LLM to generate\n",
        "    num_articles_in_prompt: int = 5 # Number of articles included in the prompt, for validation\n",
        ") -> Tuple[Optional[Dict[str, Any]], str]:\n",
        "    \"\"\"\n",
        "    Sends a constructed prompt to the loaded LLM, receives the response,\n",
        "    parses it as JSON, and validates its structure.\n",
        "\n",
        "    Args:\n",
        "        full_prompt_string (str):\n",
        "            The complete prompt string (from Task 9) to be sent to the LLM.\n",
        "        llm_model (PreTrainedModel):\n",
        "            The loaded Hugging Face PreTrainedModel object (from Task 8).\n",
        "        llm_tokenizer (PreTrainedTokenizerBase):\n",
        "            The loaded Hugging Face PreTrainedTokenizerBase object (from Task 8).\n",
        "        llm_interpretation_params (Dict[str, Any]):\n",
        "            Parameters for LLM interpretation, must include \"llm_temperature\" (float).\n",
        "        max_new_tokens_generation (int):\n",
        "            The maximum number of new tokens the LLM is allowed to generate.\n",
        "            This should be sufficient for the expected JSON output.\n",
        "        num_articles_in_prompt (int):\n",
        "            The number of articles that were included in the prompt. This is used\n",
        "            to validate the \"summaries\" part of the LLM's JSON response.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Optional[Dict[str, Any]], str]:\n",
        "            - parsed_json_response (Optional[Dict[str, Any]]):\n",
        "              The parsed JSON response as a Python dictionary if successful and valid,\n",
        "              otherwise None.\n",
        "            - raw_llm_output_text (str):\n",
        "              The raw text output from the LLM before JSON parsing. Useful for debugging.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If input argument types are incorrect.\n",
        "        ValueError: If critical parameters like 'llm_temperature' are missing.\n",
        "        RuntimeError: For unrecoverable errors during LLM inference.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(full_prompt_string, str):\n",
        "        raise TypeError(\"'full_prompt_string' must be a string.\")\n",
        "    if not isinstance(llm_model, PreTrainedModel):\n",
        "        raise TypeError(\"'llm_model' must be a Hugging Face PreTrainedModel.\")\n",
        "    if not isinstance(llm_tokenizer, PreTrainedTokenizerBase):\n",
        "        raise TypeError(\"'llm_tokenizer' must be a Hugging Face PreTrainedTokenizerBase.\")\n",
        "    if not isinstance(llm_interpretation_params, dict) or \"llm_temperature\" not in llm_interpretation_params:\n",
        "        raise ValueError(\"'llm_interpretation_params' must be a dict and include 'llm_temperature'.\")\n",
        "\n",
        "    llm_temperature: float = llm_interpretation_params[\"llm_temperature\"]\n",
        "    if not isinstance(llm_temperature, float):\n",
        "        raise TypeError(\"'llm_temperature' must be a float.\")\n",
        "    if not isinstance(max_new_tokens_generation, int) or max_new_tokens_generation <= 0:\n",
        "        raise ValueError(\"'max_new_tokens_generation' must be a positive integer.\")\n",
        "    if not isinstance(num_articles_in_prompt, int) or num_articles_in_prompt < 0: # 0 articles might be valid if prompt structure changes\n",
        "        raise ValueError(\"'num_articles_in_prompt' must be a non-negative integer.\")\n",
        "\n",
        "\n",
        "    # --- Sub-step 10.a.ii: Send prompt to LLM and get response ---\n",
        "    # Determine the device the model is on.\n",
        "    model_device = next(llm_model.parameters()).device\n",
        "\n",
        "    # Tokenize the input prompt.\n",
        "    # `return_tensors=\"pt\"` returns PyTorch tensors.\n",
        "    # Move tokenized input to the same device as the model.\n",
        "    try:\n",
        "        inputs = llm_tokenizer(full_prompt_string, return_tensors=\"pt\", truncation=True).to(model_device)\n",
        "    except Exception as e:\n",
        "        # Catch potential errors during tokenization (e.g., very long input beyond model's max length if truncation fails)\n",
        "        raw_output_for_debug = f\"Error during tokenization: {e}\"\n",
        "        warnings.warn(raw_output_for_debug, RuntimeWarning)\n",
        "        return None, raw_output_for_debug\n",
        "\n",
        "    raw_llm_output_text: str = \"\"\n",
        "    parsed_json_response: Optional[Dict[str, Any]] = None\n",
        "\n",
        "    try:\n",
        "        # Generate text using the model.\n",
        "        # `temperature=0.0` and `do_sample=False` aim for deterministic, greedy decoding.\n",
        "        # `pad_token_id` is important to avoid warnings/errors if padding is needed.\n",
        "        with torch.no_grad(): # Disable gradient calculations for inference.\n",
        "            outputs = llm_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens_generation,\n",
        "                temperature=llm_temperature, # Should be 0.0 for this study\n",
        "                do_sample=False if llm_temperature == 0.0 else True, # Explicitly set do_sample based on temperature\n",
        "                pad_token_id=llm_tokenizer.eos_token_id # Common practice for Llama-like models\n",
        "            )\n",
        "\n",
        "        # Decode the generated token IDs back to a string.\n",
        "        # `skip_special_tokens=True` removes tokens like [EOS], [PAD].\n",
        "        # We only want to decode the newly generated tokens, not the input prompt.\n",
        "        # `outputs[0]` because we process one prompt at a time (batch size 1).\n",
        "        # Slice `outputs[0]` to get only the generated part, excluding input token length.\n",
        "        num_input_tokens = inputs.input_ids.shape[1]\n",
        "        generated_tokens = outputs[0][num_input_tokens:]\n",
        "        raw_llm_output_text = llm_tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        # Handle runtime errors during generation (e.g., CUDA out of memory).\n",
        "        raw_llm_output_text = f\"RuntimeError during LLM generation: {e}\"\n",
        "        warnings.warn(raw_llm_output_text, RuntimeWarning)\n",
        "        return None, raw_llm_output_text # Return None for parsed JSON, and the error message as raw output\n",
        "    except Exception as e: # Catch any other unexpected errors during generation\n",
        "        raw_llm_output_text = f\"Unexpected error during LLM generation: {e}\"\n",
        "        warnings.warn(raw_llm_output_text, RuntimeWarning)\n",
        "        return None, raw_llm_output_text\n",
        "\n",
        "    # --- Sub-step 10.a.iii: Parse and validate JSON response ---\n",
        "    try:\n",
        "        # Attempt to parse the raw text output as JSON.\n",
        "        # LLMs might sometimes add leading/trailing text or code fences around JSON.\n",
        "        # A simple heuristic: try to find the first '{' and last '}'\n",
        "        # This is a common, but not foolproof, way to extract JSON from mixed text.\n",
        "        json_start_index = raw_llm_output_text.find('{')\n",
        "        json_end_index = raw_llm_output_text.rfind('}')\n",
        "\n",
        "        if json_start_index != -1 and json_end_index != -1 and json_end_index > json_start_index:\n",
        "            json_string_to_parse = raw_llm_output_text[json_start_index : json_end_index + 1]\n",
        "            parsed_json_response = json.loads(json_string_to_parse)\n",
        "\n",
        "            # Validate the structure of the parsed JSON.\n",
        "            is_valid_schema, validation_errors = _validate_parsed_llm_json(parsed_json_response, num_articles_in_prompt)\n",
        "            if not is_valid_schema:\n",
        "                # If schema validation fails, log errors and treat as parsing failure for return.\n",
        "                error_message = f\"LLM output parsed as JSON but failed schema validation. Errors: {'; '.join(validation_errors)}\"\n",
        "                warnings.warn(error_message, UserWarning)\n",
        "                # Keep raw_llm_output_text for debugging, but set parsed_json_response to None.\n",
        "                # Or, return the partially valid JSON with a warning flag, depending on desired strictness.\n",
        "                # For this implementation, if schema fails, we consider it not fully successful.\n",
        "                # However, the prompt asks for the parsed JSON if successful, so we return it but with warnings.\n",
        "                # The calling function can decide how to handle schema validation failures.\n",
        "                # For now, we will return the parsed JSON even if schema validation has warnings,\n",
        "                # as it *did* parse. The raw_llm_output_text is always returned.\n",
        "                # The validation errors are logged via warnings.\n",
        "                pass # Parsed JSON is kept, warnings issued.\n",
        "        else:\n",
        "            # If no clear JSON block is found.\n",
        "            warnings.warn(f\"Could not find a clear JSON block in LLM output. Raw output: '{raw_llm_output_text[:500]}...'\", UserWarning)\n",
        "            parsed_json_response = None # Explicitly set to None if no JSON block found\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        # Handle JSON parsing errors if the output is not valid JSON.\n",
        "        warnings.warn(f\"Failed to parse LLM output as JSON. Error: {e}. Raw output: '{raw_llm_output_text[:500]}...'\", UserWarning)\n",
        "        parsed_json_response = None # Set to None if parsing fails\n",
        "\n",
        "    # Return the parsed JSON (or None if parsing/validation failed) and the raw text output.\n",
        "    return parsed_json_response, raw_llm_output_text\n"
      ],
      "metadata": {
        "id": "8datPGaqDybq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Handled in the parameter vii.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "1SS-jfr9Ir32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12: Evaluation\n",
        "\n",
        "def _get_top_n_words_for_topic(\n",
        "    topic_word_dist_matrix: np.ndarray, # K x V matrix for a specific chunk\n",
        "    topic_id: int,\n",
        "    gensim_dictionary: GensimDictionary,\n",
        "    top_n: int = 10\n",
        ") -> List[str]:\n",
        "    \"\"\"Helper to get top N words for a given topic_id from its word distribution.\"\"\"\n",
        "    if topic_id < 0 or topic_id >= topic_word_dist_matrix.shape[0]:\n",
        "        return [] # Invalid topic_id\n",
        "    # Get the word distribution for the specified topic.\n",
        "    topic_distribution = topic_word_dist_matrix[topic_id, :]\n",
        "    # Get indices of top N words by sorting probabilities in descending order.\n",
        "    top_n_word_indices = topic_distribution.argsort()[-top_n:][::-1]\n",
        "    # Map indices to word strings using the Gensim dictionary.\n",
        "    top_n_words = [gensim_dictionary.id2token.get(idx, f\"UNK_ID_{idx}\") for idx in top_n_word_indices]\n",
        "    return top_n_words\n",
        "\n",
        "def _map_system_changes_to_human_labels(\n",
        "    system_detected_change_points: List[Tuple[str, int, List[str], float, float]], # (chunk_key, topic_id, loo_words, D_obs, D_crit)\n",
        "    llm_parsed_responses: Dict[Tuple[str, int], Dict[str, Any]], # Key: (chunk_key, topic_id), Value: parsed LLM JSON\n",
        "    human_annotations_input: Dict[str, Dict[str, Any]], # Key: \"YYYY-MM-DD\", Value: human annotation dict\n",
        "    time_series_topic_word_dist: Dict[str, np.ndarray], # For getting topic terms\n",
        "    gensim_dictionary: GensimDictionary, # For mapping topic terms\n",
        "    topic_matching_threshold: float = 0.1 # Min Jaccard similarity for topic match (heuristic)\n",
        ") -> Tuple[List[int], List[int], int]:\n",
        "    \"\"\"\n",
        "    Attempts to map system-detected change points (with LLM predictions) to\n",
        "    human annotations to create aligned lists of true and predicted labels.\n",
        "\n",
        "    This is a complex and potentially heuristic mapping if human annotations\n",
        "    are not directly keyed to system change points.\n",
        "\n",
        "    Args:\n",
        "        system_detected_change_points: Output from Task 6.\n",
        "        llm_parsed_responses: Dict mapping (chunk_key, topic_id) to LLM's parsed JSON.\n",
        "        human_annotations_input: Parameter vii, keyed by daily date strings.\n",
        "        time_series_topic_word_dist: Output from Task 5, for topic term lookup.\n",
        "        gensim_dictionary: Gensim dictionary for term mapping.\n",
        "        topic_matching_threshold: Threshold for considering a topic match based on\n",
        "                                  Jaccard similarity of top terms vs human \"topics\" list.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[int], List[int], int]:\n",
        "            - y_true (List[int]): List of true labels (1 for narrative, 0 for content).\n",
        "            - y_pred (List[int]): List of predicted labels from LLM.\n",
        "            - num_successfully_mapped (int): Number of system changes successfully mapped.\n",
        "    \"\"\"\n",
        "    y_true: List[int] = []\n",
        "    y_pred: List[int] = []\n",
        "    num_successfully_mapped = 0\n",
        "\n",
        "    # For faster lookup of human annotations by month\n",
        "    human_annotations_by_month: Dict[str, List[Dict[str, Any]]] = {}\n",
        "    for daily_date_str, annotation_details in human_annotations_input.items():\n",
        "        try:\n",
        "            # Extract \"YYYY-MM\" from \"YYYY-MM-DD\".\n",
        "            month_key = datetime.strptime(daily_date_str, \"%Y-%m-%d\").strftime(\"%Y-%m\")\n",
        "            # Append annotation details to the list for that month.\n",
        "            if month_key not in human_annotations_by_month:\n",
        "                human_annotations_by_month[month_key] = []\n",
        "            human_annotations_by_month[month_key].append(annotation_details)\n",
        "        except ValueError:\n",
        "            # Warn if a date key in human annotations is malformed.\n",
        "            warnings.warn(f\"Malformed date key '{daily_date_str}' in human annotations. Skipping.\", UserWarning)\n",
        "            continue\n",
        "\n",
        "    # Iterate through each system-detected change point that has an LLM response.\n",
        "    for sys_chunk_key, sys_topic_id, _, _, _ in system_detected_change_points:\n",
        "        # Check if there is an LLM response for this system-detected change.\n",
        "        llm_response = llm_parsed_responses.get((sys_chunk_key, sys_topic_id))\n",
        "        if not llm_response or \"true_narrative\" not in llm_response or \\\n",
        "           not isinstance(llm_response[\"true_narrative\"], bool):\n",
        "            # Skip if no valid LLM prediction is available for this system change.\n",
        "            warnings.warn(f\"No valid LLM prediction for system change ({sys_chunk_key}, Topic {sys_topic_id}). Skipping evaluation for this point.\", UserWarning)\n",
        "            continue\n",
        "\n",
        "        # Attempt to find a matching human annotation.\n",
        "        # Get human annotations for the same month as the system's chunk_key.\n",
        "        potential_human_matches = human_annotations_by_month.get(sys_chunk_key, [])\n",
        "\n",
        "        found_human_match: Optional[Dict[str, Any]] = None\n",
        "        if potential_human_matches:\n",
        "            # Get top terms for the system's topic_id in this chunk_key.\n",
        "            phi_matrix_for_chunk = time_series_topic_word_dist.get(sys_chunk_key)\n",
        "            if phi_matrix_for_chunk is None or phi_matrix_for_chunk.size == 0:\n",
        "                warnings.warn(f\"Missing topic distributions for chunk {sys_chunk_key} needed for topic matching. Skipping evaluation for this point.\", UserWarning)\n",
        "                continue\n",
        "\n",
        "            # Get top 10 words for the system-detected topic.\n",
        "            system_topic_terms = set(\n",
        "                word.lower() for word in _get_top_n_words_for_topic(\n",
        "                    phi_matrix_for_chunk, sys_topic_id, gensim_dictionary, top_n=10\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Try to match with a human annotation based on topic overlap.\n",
        "            for human_ann in potential_human_matches:\n",
        "                human_topic_terms_list = human_ann.get(\"topics\", [])\n",
        "                if not isinstance(human_topic_terms_list, list): continue # Malformed human annotation\n",
        "\n",
        "                human_topic_terms = set(term.lower() for term in human_topic_terms_list)\n",
        "\n",
        "                # Calculate Jaccard similarity between system topic terms and human topic terms.\n",
        "                intersection_len = len(system_topic_terms.intersection(human_topic_terms))\n",
        "                union_len = len(system_topic_terms.union(human_topic_terms))\n",
        "\n",
        "                if union_len == 0: # Both sets are empty\n",
        "                    jaccard_sim = 1.0 if not system_topic_terms and not human_topic_terms else 0.0\n",
        "                else:\n",
        "                    jaccard_sim = intersection_len / union_len\n",
        "\n",
        "                # If Jaccard similarity is above threshold, consider it a match.\n",
        "                if jaccard_sim >= topic_matching_threshold:\n",
        "                    found_human_match = human_ann\n",
        "                    break # Take the first sufficient match for simplicity\n",
        "\n",
        "        # If a corresponding human annotation is found:\n",
        "        if found_human_match:\n",
        "            # Get human label (1 for \"narrative shift\", 0 for \"content shift\").\n",
        "            human_label_str = found_human_match.get(\"change_type\", \"\")\n",
        "            true_label = 1 if human_label_str == \"narrative shift\" else 0\n",
        "\n",
        "            # Get LLM prediction (1 if true_narrative is True, 0 if False).\n",
        "            predicted_label = 1 if llm_response[\"true_narrative\"] else 0\n",
        "\n",
        "            # Append to lists for metric calculation.\n",
        "            y_true.append(true_label)\n",
        "            y_pred.append(predicted_label)\n",
        "            num_successfully_mapped += 1\n",
        "        else:\n",
        "            # Warn if no matching human annotation was found for a system-detected change with LLM output.\n",
        "            warnings.warn(f\"No matching human annotation found for system change ({sys_chunk_key}, Topic {sys_topic_id}). Skipping evaluation for this point.\", UserWarning)\n",
        "\n",
        "    # Return the aligned lists of true and predicted labels, and the count of mapped points.\n",
        "    return y_true, y_pred, num_successfully_mapped\n",
        "\n",
        "\n",
        "def evaluate_llm_classification_performance(\n",
        "    system_detected_change_points: List[Tuple[str, int, List[str], float, float]],\n",
        "    llm_parsed_responses: Dict[Tuple[str, int], Dict[str, Any]], # Key: (chunk_key, topic_id)\n",
        "    human_annotations_input: Dict[str, Dict[str, Any]], # Parameter vii\n",
        "    time_series_topic_word_dist: Dict[str, np.ndarray], # For topic matching\n",
        "    gensim_dictionary: GensimDictionary, # For topic matching\n",
        "    positive_label_is_narrative_shift: bool = True, # Defines which class is \"positive\"\n",
        "    topic_matching_threshold_for_mapping: float = 0.1 # For _map_system_changes_to_human_labels\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Evaluates the LLM's binary classification performance (narrative vs. content shift).\n",
        "\n",
        "    This function first attempts to map system-detected changes (with their LLM\n",
        "    predictions) to the provided human annotations. Then, it calculates\n",
        "    accuracy, precision, recall, and F1-score for the \"narrative shift\" class.\n",
        "\n",
        "    Args:\n",
        "        system_detected_change_points: List of system-detected changes from Task 6.\n",
        "        llm_parsed_responses: Dictionary of LLM's parsed JSON responses, keyed by\n",
        "                              (chunk_key, topic_id) of the system change.\n",
        "        human_annotations_input: The ground truth human annotations (Parameter vii).\n",
        "        time_series_topic_word_dist: Topic-word distributions for topic term lookup.\n",
        "        gensim_dictionary: Gensim dictionary for term mapping.\n",
        "        positive_label_is_narrative_shift (bool): If True, \"narrative shift\" is\n",
        "            treated as the positive class (label 1). If False, \"content shift\"\n",
        "            would be (though typically narrative shift is the focus).\n",
        "        topic_matching_threshold_for_mapping (float): Jaccard similarity threshold\n",
        "            used by the internal mapping helper to match system topics to human\n",
        "            annotated topics.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the evaluation metrics:\n",
        "            \"num_mapped_for_evaluation\": int,\n",
        "            \"accuracy\": float,\n",
        "            \"precision\": float,\n",
        "            \"recall\": float,\n",
        "            \"f1_score\": float,\n",
        "            \"confusion_matrix\": np.ndarray (2x2: [[TN, FP], [FN, TP]]),\n",
        "            \"true_positives\": int,\n",
        "            \"false_positives\": int,\n",
        "            \"true_negatives\": int,\n",
        "            \"false_negatives\": int\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(llm_parsed_responses, dict):\n",
        "        raise TypeError(\"'llm_parsed_responses' must be a dictionary.\")\n",
        "    if not isinstance(human_annotations_input, dict):\n",
        "        raise TypeError(\"'human_annotations_input' must be a dictionary.\")\n",
        "    # Further validation of list/dict contents happens in the helper or sklearn.\n",
        "\n",
        "    # --- Step 1: Map system changes to human labels to get aligned y_true, y_pred ---\n",
        "    # This is the most complex part due to potential differences in keys and granularity.\n",
        "    y_true, y_pred, num_mapped = _map_system_changes_to_human_labels(\n",
        "        system_detected_change_points=system_detected_change_points,\n",
        "        llm_parsed_responses=llm_parsed_responses,\n",
        "        human_annotations_input=human_annotations_input,\n",
        "        time_series_topic_word_dist=time_series_topic_word_dist,\n",
        "        gensim_dictionary=gensim_dictionary,\n",
        "        topic_matching_threshold=topic_matching_threshold_for_mapping\n",
        "    )\n",
        "\n",
        "    # Initialize results dictionary with default values for metrics.\n",
        "    results: Dict[str, Any] = {\n",
        "        \"num_mapped_for_evaluation\": num_mapped,\n",
        "        \"accuracy\": np.nan, \"precision\": np.nan, \"recall\": np.nan, \"f1_score\": np.nan,\n",
        "        \"confusion_matrix\": np.array([[np.nan, np.nan], [np.nan, np.nan]]),\n",
        "        \"true_positives\": 0, \"false_positives\": 0,\n",
        "        \"true_negatives\": 0, \"false_negatives\": 0\n",
        "    }\n",
        "\n",
        "    # If no points could be mapped for evaluation, return default results.\n",
        "    if num_mapped == 0:\n",
        "        warnings.warn(\"No system-detected changes could be mapped to human annotations. Evaluation metrics cannot be computed.\", UserWarning)\n",
        "        return results\n",
        "\n",
        "    # --- Step 2: Calculate metrics using scikit-learn ---\n",
        "    # Define the positive label for scikit-learn metrics.\n",
        "    # If positive_label_is_narrative_shift is True, label 1 (\"narrative shift\") is positive.\n",
        "    # Otherwise, label 0 (\"content shift\") would be, but this is less common.\n",
        "    pos_label_val = 1 if positive_label_is_narrative_shift else 0\n",
        "\n",
        "    # Calculate Accuracy (Sub-step 12.a)\n",
        "    # accuracy = (TP + TN) / Total\n",
        "    results[\"accuracy\"] = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # Calculate Precision, Recall, F1-score (Sub-step 12.b)\n",
        "    # These are typically reported for the positive class.\n",
        "    # `zero_division=0` means if a metric is undefined (e.g., TP+FP=0 for precision), it returns 0.0.\n",
        "    # The paper reports F1=0.7010, likely for the \"narrative shift\" class.\n",
        "    results[\"precision\"] = precision_score(y_true, y_pred, pos_label=pos_label_val, zero_division=0)\n",
        "    results[\"recall\"] = recall_score(y_true, y_pred, pos_label=pos_label_val, zero_division=0)\n",
        "    results[\"f1_score\"] = f1_score(y_true, y_pred, pos_label=pos_label_val, zero_division=0)\n",
        "\n",
        "    # Calculate Confusion Matrix: [[TN, FP], [FN, TP]]\n",
        "    # Note: sklearn's confusion_matrix by default has labels=[0, 1] if y_true/y_pred are binary.\n",
        "    # If pos_label_val is 0, the interpretation of TP/TN etc. needs care.\n",
        "    # Assuming pos_label_val = 1 (narrative shift is positive).\n",
        "    # Then labels=[0, 1] means:\n",
        "    #   cm[0,0] = TN (true=0, pred=0)\n",
        "    #   cm[0,1] = FP (true=0, pred=1)\n",
        "    #   cm[1,0] = FN (true=1, pred=0)\n",
        "    #   cm[1,1] = TP (true=1, pred=1)\n",
        "    cm = sk_confusion_matrix(y_true, y_pred, labels=[0, 1]) # Ensure consistent label order\n",
        "    results[\"confusion_matrix\"] = cm\n",
        "\n",
        "    # Extract TP, FP, TN, FN from confusion matrix assuming labels=[0,1]\n",
        "    # and positive class is 1.\n",
        "    if cm.shape == (2,2): # Ensure it's a 2x2 matrix\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        results[\"true_negatives\"] = int(tn)\n",
        "        results[\"false_positives\"] = int(fp)\n",
        "        results[\"false_negatives\"] = int(fn)\n",
        "        results[\"true_positives\"] = int(tp)\n",
        "    else: # Should not happen with binary labels if mapping worked\n",
        "        warnings.warn(f\"Confusion matrix is not 2x2: {cm}. Cannot extract TP/FP/TN/FN.\", UserWarning)\n",
        "\n",
        "    # Return the dictionary of calculated metrics.\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "XG-K8aFxI3jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Result Analysis\n",
        "\n",
        "# Helper function to get top N words for a topic (re-stated for completeness within this callable's context)\n",
        "# In a modular project, this would be imported.\n",
        "def _get_top_n_words_for_topic_viz( # Renamed to avoid conflict if defined elsewhere\n",
        "    topic_word_dist_vector: np.ndarray,\n",
        "    gensim_dictionary: GensimDictionary,\n",
        "    top_n: int = 5\n",
        ") -> List[str]:\n",
        "    \"\"\"Helper to get top N words for a given topic's word distribution vector.\"\"\"\n",
        "    # Check if the input vector is valid and top_n is positive.\n",
        "    if not isinstance(topic_word_dist_vector, np.ndarray) or topic_word_dist_vector.ndim != 1 or topic_word_dist_vector.size == 0 or top_n <= 0:\n",
        "        # Return empty list for invalid inputs.\n",
        "        return []\n",
        "\n",
        "    # Determine the actual number of top words to retrieve, ensuring it doesn't exceed vocabulary size.\n",
        "    actual_top_n = min(top_n, len(topic_word_dist_vector))\n",
        "    # If actual_top_n is 0 (e.g., empty vector or non-positive top_n), return empty list.\n",
        "    if actual_top_n == 0: return []\n",
        "\n",
        "    # Get indices of top N words by sorting probabilities in descending order.\n",
        "    # argsort() returns indices that would sort the array. Slicing gets the top N largest.\n",
        "    # [::-1] reverses to get descending order.\n",
        "    top_n_word_indices = topic_word_dist_vector.argsort()[-actual_top_n:][::-1]\n",
        "\n",
        "    # Map indices to word strings using the Gensim dictionary.\n",
        "    # Handle cases where an index might not be in id2token (though unlikely for valid indices).\n",
        "    top_n_words = [gensim_dictionary.id2token.get(idx, f\"UNK_ID_{idx}\") for idx in top_n_word_indices]\n",
        "    # Return the list of top N words.\n",
        "    return top_n_words\n",
        "\n",
        "# New/Refactored Helper function for mapping a single system change to a human annotation\n",
        "def _get_mapped_human_annotation_for_system_change(\n",
        "    sys_chunk_key: str,\n",
        "    sys_topic_id: int,\n",
        "    human_annotations_by_month: Dict[str, List[Dict[str, Any]]],\n",
        "    time_series_topic_word_dist: Dict[str, np.ndarray],\n",
        "    gensim_dictionary: GensimDictionary,\n",
        "    topic_matching_threshold: float,\n",
        "    num_top_words_for_matching: int = 10 # Number of top words to use for Jaccard sim\n",
        ") -> Tuple[Optional[Dict[str, Any]], float]:\n",
        "    \"\"\"\n",
        "    Attempts to map a single system-detected change point to a human annotation\n",
        "    based on temporal proximity (same month) and topical similarity (Jaccard index).\n",
        "\n",
        "    Args:\n",
        "        sys_chunk_key (str): The \"YYYY-MM\" key for the system-detected change.\n",
        "        sys_topic_id (int): The topic ID for the system-detected change.\n",
        "        human_annotations_by_month (Dict[str, List[Dict[str, Any]]]):\n",
        "            Human annotations pre-grouped by month (\"YYYY-MM\").\n",
        "        time_series_topic_word_dist (Dict[str, np.ndarray]):\n",
        "            Topic-word distributions for topic term lookup.\n",
        "        gensim_dictionary (GensimDictionary): Gensim dictionary for term mapping.\n",
        "        topic_matching_threshold (float): Jaccard similarity threshold for topic match.\n",
        "        num_top_words_for_matching (int): Number of top words to extract from system topic\n",
        "                                          for calculating Jaccard similarity.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Optional[Dict[str, Any]], float]:\n",
        "            - The matched human annotation dictionary (with \"original_human_annotation_date\"),\n",
        "              or None if no suitable match is found.\n",
        "            - The Jaccard similarity score of the best match found (or 0.0 if no match).\n",
        "    \"\"\"\n",
        "    # Retrieve potential human matches for the month of the system change.\n",
        "    potential_human_matches_for_month = human_annotations_by_month.get(sys_chunk_key, [])\n",
        "    # Initialize best Jaccard similarity found so far.\n",
        "    best_jaccard_sim = 0.0\n",
        "    # Initialize variable to store the best matched human annotation.\n",
        "    best_match_human_ann: Optional[Dict[str, Any]] = None\n",
        "\n",
        "    # Proceed only if there are potential human annotations for the month.\n",
        "    if potential_human_matches_for_month:\n",
        "        # Retrieve the topic-word distribution matrix for the system's chunk.\n",
        "        phi_matrix_for_chunk = time_series_topic_word_dist.get(sys_chunk_key)\n",
        "\n",
        "        # Proceed only if topic distributions are available for the system's chunk.\n",
        "        if phi_matrix_for_chunk is not None and phi_matrix_for_chunk.size > 0:\n",
        "            # Get the top N words for the system-detected topic, converted to a lowercase set.\n",
        "            system_topic_terms_list = _get_top_n_words_for_topic_viz(\n",
        "                phi_matrix_for_chunk[sys_topic_id, :], # Pass the 1D vector for the topic\n",
        "                gensim_dictionary,\n",
        "                top_n=num_top_words_for_matching\n",
        "            )\n",
        "            system_topic_terms_set: Set[str] = set(word.lower() for word in system_topic_terms_list)\n",
        "\n",
        "            # Iterate through each potential human annotation for the month.\n",
        "            for human_ann in potential_human_matches_for_month:\n",
        "                # Get the list of topic keywords from the human annotation.\n",
        "                human_topic_terms_list = human_ann.get(\"topics\", [])\n",
        "                # Ensure it's a list before processing.\n",
        "                if not isinstance(human_topic_terms_list, list):\n",
        "                    continue # Skip malformed human annotation.\n",
        "\n",
        "                # Convert human topic keywords to a lowercase set.\n",
        "                human_topic_terms_set: Set[str] = set(term.lower() for term in human_topic_terms_list)\n",
        "\n",
        "                # Calculate Jaccard similarity between system topic terms and human topic terms.\n",
        "                intersection_len = len(system_topic_terms_set.intersection(human_topic_terms_set))\n",
        "                union_len = len(system_topic_terms_set.union(human_topic_terms_set))\n",
        "\n",
        "                # Handle division by zero if both sets are empty.\n",
        "                current_jaccard_sim = 0.0\n",
        "                if union_len > 0:\n",
        "                    current_jaccard_sim = intersection_len / union_len\n",
        "                elif not system_topic_terms_set and not human_topic_terms_set: # Both empty\n",
        "                    current_jaccard_sim = 1.0 # Perfect match if both are descriptively empty\n",
        "\n",
        "                # If current Jaccard similarity meets threshold and is better than previous best:\n",
        "                if current_jaccard_sim >= topic_matching_threshold and current_jaccard_sim > best_jaccard_sim:\n",
        "                    best_jaccard_sim = current_jaccard_sim\n",
        "                    best_match_human_ann = human_ann\n",
        "\n",
        "    # If no match met the threshold, best_match_human_ann will be None and best_jaccard_sim will be its initial value or highest below threshold.\n",
        "    # We only return a match if it met the threshold.\n",
        "    if best_match_human_ann is None or best_jaccard_sim < topic_matching_threshold:\n",
        "        return None, best_jaccard_sim # Return the actual best Jaccard even if below threshold for informational purposes\n",
        "\n",
        "    # Return the best matched human annotation and its Jaccard similarity score.\n",
        "    return best_match_human_ann, best_jaccard_sim\n",
        "\n",
        "\n",
        "# Amended Task 13 function\n",
        "def compile_analysis_results(\n",
        "    system_detected_change_points_with_loo: List[Tuple[str, int, List[str], float, float]],\n",
        "    llm_parsed_responses: Dict[Tuple[str, int], Optional[Dict[str, Any]]],\n",
        "    human_annotations_input: Dict[str, Dict[str, Any]],\n",
        "    time_series_topic_word_dist: Dict[str, np.ndarray],\n",
        "    gensim_dictionary: GensimDictionary,\n",
        "    k_topics: int,\n",
        "    topic_matching_threshold_for_mapping: float = 0.1,\n",
        "    num_top_words_for_topic_display: int = 5,\n",
        "    num_top_words_for_mapping_match: int = 10 # Added for mapping helper\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compiles system-detected changes, LLM analyses, and human annotations\n",
        "    into a single pandas DataFrame for comprehensive result analysis.\n",
        "    This version uses a dedicated helper for mapping human annotations.\n",
        "\n",
        "    Args:\n",
        "        system_detected_change_points_with_loo: List of system-detected changes.\n",
        "            Each tuple: (chunk_key, topic_id, loo_words, D_obs, D_crit).\n",
        "        llm_parsed_responses: Dictionary of LLM's parsed JSON responses, keyed by\n",
        "                              (chunk_key, topic_id). Value can be None if LLM failed.\n",
        "        human_annotations_input: The ground truth human annotations (Parameter vii).\n",
        "        time_series_topic_word_dist: Topic-word distributions for topic term lookup.\n",
        "        gensim_dictionary: Gensim dictionary for term mapping.\n",
        "        k_topics: Total number of topics.\n",
        "        topic_matching_threshold_for_mapping (float): Jaccard similarity threshold\n",
        "            for mapping system topics to human annotated topics.\n",
        "        num_top_words_for_topic_display (int): Number of top words to include\n",
        "            in the DataFrame for describing the system-detected topic.\n",
        "        num_top_words_for_mapping_match (int): Number of top words from system topic\n",
        "            to use when calculating Jaccard similarity for mapping to human annotations.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame where each row corresponds to a system-detected\n",
        "                      change point, augmented with LLM analysis and mapped human\n",
        "                      annotation data.\n",
        "    \"\"\"\n",
        "    # --- Input Validation (simplified for brevity, assume valid inputs as per previous tasks) ---\n",
        "    if not isinstance(system_detected_change_points_with_loo, list):\n",
        "        raise TypeError(\"'system_detected_change_points_with_loo' must be a list.\")\n",
        "    # ... (other validations can be added as in the original function) ...\n",
        "\n",
        "    # --- Data Compilation Logic ---\n",
        "    compiled_data_rows: List[Dict[str, Any]] = []\n",
        "\n",
        "    # Pre-process human annotations for faster lookup by month.\n",
        "    human_annotations_by_month: Dict[str, List[Dict[str, Any]]] = {}\n",
        "    # Iterate through human annotations to group them by month.\n",
        "    for daily_date_str, annotation_details in human_annotations_input.items():\n",
        "        try:\n",
        "            # Parse daily date string to datetime object.\n",
        "            # Extract \"YYYY-MM\" as month key.\n",
        "            month_key = datetime.strptime(daily_date_str, \"%Y-%m-%d\").strftime(\"%Y-%m\")\n",
        "            # Initialize list for month if not exists.\n",
        "            if month_key not in human_annotations_by_month:\n",
        "                human_annotations_by_month[month_key] = []\n",
        "            # Create a copy of annotation details to add original date.\n",
        "            annotation_details_with_date = annotation_details.copy()\n",
        "            annotation_details_with_date[\"original_human_annotation_date\"] = daily_date_str\n",
        "            # Append annotation to the corresponding month's list.\n",
        "            human_annotations_by_month[month_key].append(annotation_details_with_date)\n",
        "        except ValueError:\n",
        "            # Warn if a date key in human annotations is malformed.\n",
        "            warnings.warn(f\"Malformed date key '{daily_date_str}' in human annotations. Skipping.\", UserWarning)\n",
        "            continue\n",
        "\n",
        "    # Iterate through each system-detected change point.\n",
        "    for change_info in system_detected_change_points_with_loo:\n",
        "        # Unpack system change information.\n",
        "        sys_chunk_key, sys_topic_id, sys_loo_words, sys_d_obs, sys_d_crit = change_info\n",
        "\n",
        "        # Initialize a dictionary for the current row data.\n",
        "        row_data: Dict[str, Any] = {\n",
        "            \"system_chunk_key\": sys_chunk_key,\n",
        "            \"system_topic_id\": sys_topic_id,\n",
        "            \"system_significant_loo_words\": \", \".join(sys_loo_words) if sys_loo_words else None,\n",
        "            \"system_observed_distance\": sys_d_obs,\n",
        "            \"system_critical_threshold\": sys_d_crit\n",
        "        }\n",
        "\n",
        "        # Add system topic's top words for context.\n",
        "        phi_matrix_for_chunk = time_series_topic_word_dist.get(sys_chunk_key)\n",
        "        # Check if topic distribution matrix is available and valid.\n",
        "        if phi_matrix_for_chunk is not None and phi_matrix_for_chunk.size > 0:\n",
        "            # Get top N words for the system-detected topic.\n",
        "            system_topic_top_terms = _get_top_n_words_for_topic_viz(\n",
        "                phi_matrix_for_chunk[sys_topic_id, :], # Pass 1D vector for the specific topic\n",
        "                gensim_dictionary,\n",
        "                top_n=num_top_words_for_topic_display\n",
        "            )\n",
        "            # Join top words into a comma-separated string.\n",
        "            row_data[\"system_topic_top_words\"] = \", \".join(system_topic_top_terms)\n",
        "        else:\n",
        "            # Set to None if topic distributions are not available.\n",
        "            row_data[\"system_topic_top_words\"] = None\n",
        "\n",
        "        # Retrieve LLM analysis for this system change.\n",
        "        llm_analysis = llm_parsed_responses.get((sys_chunk_key, sys_topic_id))\n",
        "        # Check if LLM analysis is available and is a dictionary.\n",
        "        if llm_analysis and isinstance(llm_analysis, dict):\n",
        "            # Populate row_data with flattened LLM JSON fields.\n",
        "            row_data[\"llm_topic_change_desc\"] = llm_analysis.get(\"topic_change\")\n",
        "            row_data[\"llm_narrative_before_desc\"] = llm_analysis.get(\"narrative_before\")\n",
        "            row_data[\"llm_narrative_after_desc\"] = llm_analysis.get(\"narrative_after\")\n",
        "            row_data[\"llm_true_narrative_prediction\"] = llm_analysis.get(\"true_narrative\")\n",
        "\n",
        "            summaries = llm_analysis.get(\"summaries\")\n",
        "            if isinstance(summaries, list) and summaries:\n",
        "                for i, summary_item in enumerate(summaries):\n",
        "                    if isinstance(summary_item, dict):\n",
        "                        article_key = f\"article_{i+1}\"\n",
        "                        row_data[f\"llm_summary_{article_key}\"] = summary_item.get(article_key)\n",
        "\n",
        "            narrative_criteria = llm_analysis.get(\"narrative_criteria\")\n",
        "            if isinstance(narrative_criteria, list): # Expecting 4 criteria dicts\n",
        "                for criterion_dict in narrative_criteria: # Iterate through the list of criteria dicts\n",
        "                    if isinstance(criterion_dict, dict):\n",
        "                        # Each dict should have one NPF key, e.g. {\"setting\": \"...\"}\n",
        "                        if \"setting\" in criterion_dict: row_data[\"llm_npf_setting\"] = criterion_dict.get(\"setting\")\n",
        "                        if \"characters\" in criterion_dict: row_data[\"llm_npf_characters\"] = criterion_dict.get(\"characters\")\n",
        "                        if \"plot\" in criterion_dict: row_data[\"llm_npf_plot\"] = criterion_dict.get(\"plot\")\n",
        "                        if \"moral\" in criterion_dict: row_data[\"llm_npf_moral\"] = criterion_dict.get(\"moral\")\n",
        "        else:\n",
        "            # Populate LLM fields with None if no valid analysis found.\n",
        "            llm_fields_to_nullify = [\n",
        "                \"llm_topic_change_desc\", \"llm_narrative_before_desc\", \"llm_narrative_after_desc\",\n",
        "                \"llm_true_narrative_prediction\", \"llm_npf_setting\", \"llm_npf_characters\",\n",
        "                \"llm_npf_plot\", \"llm_npf_moral\"\n",
        "            ]\n",
        "            # Also nullify potential summary fields (assuming max 5 articles as per typical N_docs_filter)\n",
        "            for i in range(1, 6): llm_fields_to_nullify.append(f\"llm_summary_article_{i}\")\n",
        "            for llm_field in llm_fields_to_nullify:\n",
        "                row_data[llm_field] = None\n",
        "\n",
        "        # Call the dedicated helper to map to human annotation.\n",
        "        mapped_human_annotation, jaccard_sim = _get_mapped_human_annotation_for_system_change(\n",
        "            sys_chunk_key, sys_topic_id, human_annotations_by_month,\n",
        "            time_series_topic_word_dist, gensim_dictionary,\n",
        "            topic_matching_threshold_for_mapping,\n",
        "            num_top_words_for_matching=num_top_words_for_mapping_match\n",
        "        )\n",
        "\n",
        "        # Populate human annotation fields if a match was found.\n",
        "        if mapped_human_annotation:\n",
        "            row_data[\"human_annotation_date\"] = mapped_human_annotation.get(\"original_human_annotation_date\")\n",
        "            row_data[\"human_change_type\"] = mapped_human_annotation.get(\"change_type\")\n",
        "            row_data[\"human_topics_list\"] = \", \".join(mapped_human_annotation.get(\"topics\", []))\n",
        "            row_data[\"human_npf_setting\"] = \", \".join(mapped_human_annotation.get(\"setting\", [])) # Assuming list from param vii example\n",
        "            row_data[\"human_npf_characters\"] = \", \".join(mapped_human_annotation.get(\"characters\", [])) # Assuming list\n",
        "            row_data[\"human_npf_plot\"] = mapped_human_annotation.get(\"plot\")\n",
        "            row_data[\"human_npf_moral\"] = mapped_human_annotation.get(\"moral\")\n",
        "            row_data[\"human_mapping_jaccard_sim\"] = round(jaccard_sim, 4) # Store similarity score\n",
        "        else:\n",
        "            # Populate human fields with None if no match found.\n",
        "            human_fields_to_nullify = [\n",
        "                \"human_annotation_date\", \"human_change_type\", \"human_topics_list\",\n",
        "                \"human_npf_setting\", \"human_npf_characters\", \"human_npf_plot\",\n",
        "                \"human_npf_moral\", \"human_mapping_jaccard_sim\"\n",
        "            ]\n",
        "            for human_field in human_fields_to_nullify:\n",
        "                row_data[human_field] = None\n",
        "            # Store the Jaccard score even if no match above threshold, for diagnostics\n",
        "            if jaccard_sim is not None and row_data[\"human_mapping_jaccard_sim\"] is None : # if it was not set by a successful match\n",
        "                 row_data[\"human_mapping_jaccard_sim\"] = round(jaccard_sim, 4)\n",
        "\n",
        "\n",
        "        # Append the constructed row dictionary to the list.\n",
        "        compiled_data_rows.append(row_data)\n",
        "\n",
        "    # Create a pandas DataFrame from the list of row dictionaries.\n",
        "    analysis_df = pd.DataFrame(compiled_data_rows)\n",
        "\n",
        "    # Define a preferred column order for the DataFrame.\n",
        "    # This order helps in organizing the output for readability.\n",
        "    column_order = [\n",
        "        \"system_chunk_key\", \"system_topic_id\", \"system_topic_top_words\",\n",
        "        \"system_significant_loo_words\", \"system_observed_distance\", \"system_critical_threshold\",\n",
        "        \"llm_true_narrative_prediction\", \"llm_topic_change_desc\",\n",
        "        \"llm_narrative_before_desc\", \"llm_narrative_after_desc\",\n",
        "        \"llm_npf_setting\", \"llm_npf_characters\", \"llm_npf_plot\", \"llm_npf_moral\",\n",
        "        \"human_change_type\", \"human_annotation_date\", \"human_topics_list\",\n",
        "        \"human_npf_setting\", \"human_npf_characters\", \"human_npf_plot\", \"human_npf_moral\",\n",
        "        \"human_mapping_jaccard_sim\"\n",
        "    ]\n",
        "    # Dynamically add LLM summary columns to the desired order if they exist.\n",
        "    summary_cols = sorted([col for col in analysis_df.columns if col.startswith(\"llm_summary_article_\")])\n",
        "    final_column_order = column_order + summary_cols\n",
        "\n",
        "    # Reorder columns, only keeping those that actually exist in the DataFrame.\n",
        "    # This handles cases where some optional fields (like summaries) might not be populated for all rows or at all.\n",
        "    existing_columns_in_order = [col for col in final_column_order if col in analysis_df.columns]\n",
        "    # Select and reorder columns in the DataFrame.\n",
        "    analysis_df = analysis_df[existing_columns_in_order]\n",
        "\n",
        "    # Return the compiled DataFrame.\n",
        "    return analysis_df\n"
      ],
      "metadata": {
        "id": "EmjAGO7NJLXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14: Visualization\n",
        "\n",
        "# Helper function from Task 12 (or similar) to get top N words for a topic\n",
        "# This is redefined here for completeness if not imported.\n",
        "def _get_top_n_words_for_topic_viz(\n",
        "    topic_word_dist_vector: np.ndarray, # V-dimensional vector for a single topic\n",
        "    gensim_dictionary: GensimDictionary,\n",
        "    top_n: int = 5 # Fewer words for concise plot titles\n",
        ") -> List[str]:\n",
        "    \"\"\"Helper to get top N words for a given topic's word distribution vector.\"\"\"\n",
        "    if topic_word_dist_vector.size == 0 or top_n <= 0:\n",
        "        return []\n",
        "    # Get indices of top N words by sorting probabilities in descending order.\n",
        "    # Ensure vector is 1D\n",
        "    if topic_word_dist_vector.ndim > 1:\n",
        "        # This case should not happen if a single topic vector is passed\n",
        "        warnings.warn(\"Expected 1D topic_word_dist_vector, got >1D. Using first row/element.\", UserWarning)\n",
        "        topic_word_dist_vector = topic_word_dist_vector.ravel() # Flatten or take first element\n",
        "\n",
        "    # Handle cases where vector length might be less than top_n request\n",
        "    actual_top_n = min(top_n, len(topic_word_dist_vector))\n",
        "    if actual_top_n == 0: return []\n",
        "\n",
        "    top_n_word_indices = topic_word_dist_vector.argsort()[-actual_top_n:][::-1]\n",
        "    # Map indices to word strings using the Gensim dictionary.\n",
        "    top_n_words = [gensim_dictionary.id2token.get(idx, f\"UNK_ID_{idx}\") for idx in top_n_word_indices]\n",
        "    return top_n_words\n",
        "\n",
        "def plot_topic_evolution_and_changes(\n",
        "    visualization_data: List[Tuple[str, int, float, float]], # (chunk_key, topic_id, D_obs, D_crit)\n",
        "    detected_change_points: List[Tuple[str, int, List[str], float, float]], # (chunk_key, topic_id, ...)\n",
        "    ordered_chunk_keys: List[str],\n",
        "    k_topics: int,\n",
        "    time_series_topic_word_dist: Dict[str, np.ndarray], # For global topic titles\n",
        "    gensim_dictionary: GensimDictionary,\n",
        "    output_figure_path: Optional[str] = None,\n",
        "    plots_per_row: int = 5,\n",
        "    figure_title: str = \"Topic Evolution and Detected Narrative Shifts\"\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Generates time series plots for each topic, showing similarity evolution,\n",
        "    dynamic thresholds, and detected change points, similar to Figure 1\n",
        "    in the reference paper.\n",
        "\n",
        "    Args:\n",
        "        visualization_data: List of (chunk_key, topic_id, D_obs, D_crit) from Task 6.\n",
        "                            D_obs and D_crit are cosine distances.\n",
        "        detected_change_points: List of detected changes from Task 6, used to mark red lines.\n",
        "        ordered_chunk_keys: Chronologically sorted list of chunk identifiers (X-axis).\n",
        "        k_topics: Total number of topics.\n",
        "        time_series_topic_word_dist: Topic-word distributions for all chunks (Task 5 output),\n",
        "                                     used to calculate global top words for plot titles.\n",
        "        gensim_dictionary: Gensim dictionary for mapping word IDs to terms.\n",
        "        output_figure_path (Optional[str]): Path to save the combined figure. If None, displays plot.\n",
        "        plots_per_row (int): Number of subplots to arrange in each row.\n",
        "        figure_title (str): Overall title for the figure.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(visualization_data, list) or not isinstance(detected_change_points, list):\n",
        "        raise TypeError(\"'visualization_data' and 'detected_change_points' must be lists.\")\n",
        "    if not isinstance(ordered_chunk_keys, list) or not ordered_chunk_keys:\n",
        "        raise ValueError(\"'ordered_chunk_keys' must be a non-empty list.\")\n",
        "    if not isinstance(k_topics, int) or k_topics <= 0:\n",
        "        raise ValueError(\"'k_topics' must be a positive integer.\")\n",
        "\n",
        "    # --- Prepare Data for Plotting ---\n",
        "    # Convert visualization_data into a more usable structure, e.g., a DataFrame or dict of dicts\n",
        "    plot_data_map: Dict[int, Dict[str, Dict[str, float]]] = {i: {} for i in range(k_topics)}\n",
        "    for chunk_key, topic_id, d_obs, d_crit in visualization_data:\n",
        "        if topic_id not in plot_data_map: # Should not happen if k_topics is correct\n",
        "            plot_data_map[topic_id] = {}\n",
        "        # Store similarity (1 - distance)\n",
        "        plot_data_map[topic_id][chunk_key] = {\n",
        "            \"similarity_observed\": 1.0 - d_obs if not np.isnan(d_obs) else np.nan,\n",
        "            \"similarity_threshold\": 1.0 - d_crit if not np.isnan(d_crit) else np.nan\n",
        "        }\n",
        "\n",
        "    # Create a set of (chunk_key, topic_id) for quick lookup of detected changes\n",
        "    detected_changes_set = set((cp[0], cp[1]) for cp in detected_change_points)\n",
        "\n",
        "    # Calculate global top words for each topic for titles\n",
        "    global_topic_top_words: Dict[int, str] = {}\n",
        "    if time_series_topic_word_dist and gensim_dictionary:\n",
        "        # Aggregate topic-word distributions across all chunks for each topic\n",
        "        # Initialize a K x V matrix for sum of phis\n",
        "        first_phi_key = next(iter(time_series_topic_word_dist)) # Get a key to find vocab_size\n",
        "        vocab_size = time_series_topic_word_dist[first_phi_key].shape[1]\n",
        "        sum_phi_global = np.zeros((k_topics, vocab_size))\n",
        "        num_chunks_aggregated = 0\n",
        "        for chunk_key in ordered_chunk_keys: # Iterate in order\n",
        "            phi_matrix = time_series_topic_word_dist.get(chunk_key)\n",
        "            if phi_matrix is not None and phi_matrix.shape == (k_topics, vocab_size):\n",
        "                sum_phi_global += phi_matrix\n",
        "                num_chunks_aggregated += 1\n",
        "\n",
        "        if num_chunks_aggregated > 0:\n",
        "            avg_phi_global = sum_phi_global / num_chunks_aggregated\n",
        "            for topic_id in range(k_topics):\n",
        "                top_words = _get_top_n_words_for_topic_viz(avg_phi_global[topic_id, :], gensim_dictionary, top_n=3)\n",
        "                global_topic_top_words[topic_id] = f\"T{topic_id}: {', '.join(top_words)}\"\n",
        "        else: # Fallback if no data for aggregation\n",
        "            for topic_id in range(k_topics):\n",
        "                 global_topic_top_words[topic_id] = f\"Topic {topic_id}\"\n",
        "    else: # Fallback if necessary data for titles is missing\n",
        "        for topic_id in range(k_topics):\n",
        "            global_topic_top_words[topic_id] = f\"Topic {topic_id}\"\n",
        "\n",
        "\n",
        "    # --- Create Subplots ---\n",
        "    num_rows = math.ceil(k_topics / plots_per_row)\n",
        "    # Adjust figure size based on number of rows and columns for readability\n",
        "    fig_width = plots_per_row * 4\n",
        "    fig_height = num_rows * 3\n",
        "    fig, axes = plt.subplots(num_rows, plots_per_row, figsize=(fig_width, fig_height), sharex=False, sharey=True)\n",
        "    # Flatten axes array for easy iteration, handling single row/col case\n",
        "    axes_flat = axes.flatten() if k_topics > 1 else [axes]\n",
        "\n",
        "    for topic_id in range(k_topics):\n",
        "        ax = axes_flat[topic_id]\n",
        "\n",
        "        # Prepare series for this topic\n",
        "        current_topic_data = plot_data_map.get(topic_id, {})\n",
        "        # Ensure data is plotted in chronological order of ordered_chunk_keys\n",
        "        x_values_datetime = [pd.to_datetime(key + \"-01\") for key in ordered_chunk_keys if key in current_topic_data] # Convert \"YYYY-MM\" to datetime\n",
        "        y_observed_sim = [current_topic_data[key][\"similarity_observed\"] for key in ordered_chunk_keys if key in current_topic_data]\n",
        "        y_threshold_sim = [current_topic_data[key][\"similarity_threshold\"] for key in ordered_chunk_keys if key in current_topic_data]\n",
        "\n",
        "        if not x_values_datetime: # Skip if no data for this topic after filtering\n",
        "            ax.set_title(global_topic_top_words.get(topic_id, f\"Topic {topic_id} (No Data)\"), fontsize=8)\n",
        "            ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
        "            ax.set_xticks([])\n",
        "            ax.set_yticks([])\n",
        "            continue\n",
        "\n",
        "        # Plot observed similarity (blue line)\n",
        "        ax.plot(x_values_datetime, y_observed_sim, color='dodgerblue', linewidth=1, label='Observed Similarity')\n",
        "        # Plot threshold similarity (orange line)\n",
        "        ax.plot(x_values_datetime, y_threshold_sim, color='darkorange', linestyle='--', linewidth=1, label='Threshold')\n",
        "\n",
        "        # Add vertical red lines for detected changes\n",
        "        for i, chunk_key_dt in enumerate(x_values_datetime):\n",
        "            # Convert datetime back to \"YYYY-MM\" string for lookup in detected_changes_set\n",
        "            chunk_key_str = chunk_key_dt.strftime('%Y-%m')\n",
        "            if (chunk_key_str, topic_id) in detected_changes_set:\n",
        "                ax.axvline(x=chunk_key_dt, color='red', linestyle='-', linewidth=1.5)\n",
        "\n",
        "        # Set title for the subplot\n",
        "        ax.set_title(global_topic_top_words.get(topic_id, f\"Topic {topic_id}\"), fontsize=8)\n",
        "        # Set Y-axis limits (e.g., based on paper's Figure 1 or data range)\n",
        "        ax.set_ylim(min(0.4, np.nanmin(y_observed_sim + y_threshold_sim) - 0.05 if y_observed_sim else 0.4),\n",
        "                    max(1.0, np.nanmax(y_observed_sim + y_threshold_sim) + 0.05 if y_observed_sim else 1.0))\n",
        "\n",
        "        # Format X-axis to show dates nicely (e.g., year ticks)\n",
        "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y')) # Show year\n",
        "        ax.xaxis.set_major_locator(mdates.YearLocator(base=2)) # Tick every 2 years\n",
        "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", fontsize=7)\n",
        "        ax.tick_params(axis='y', labelsize=7)\n",
        "\n",
        "        # Add grid for better readability\n",
        "        ax.grid(True, linestyle=':', alpha=0.7)\n",
        "\n",
        "    # Hide any unused subplots if k_topics is not a multiple of plots_per_row\n",
        "    for i in range(k_topics, num_rows * plots_per_row):\n",
        "        if i < len(axes_flat): # Check if axes_flat[i] exists\n",
        "            fig.delaxes(axes_flat[i])\n",
        "\n",
        "    # Add a single legend for the entire figure if desired, or per plot if space allows\n",
        "    # For simplicity, legend can be omitted if lines are clear or described in caption.\n",
        "    # handles, labels = axes_flat[0].get_legend_handles_labels()\n",
        "    # fig.legend(handles, labels, loc='lower center', ncol=2, fontsize=8)\n",
        "\n",
        "    # Add overall figure title\n",
        "    fig.suptitle(figure_title, fontsize=14, y=0.99)\n",
        "    # Adjust layout to prevent overlapping titles/labels\n",
        "    fig.tight_layout(rect=[0, 0.03, 1, 0.97]) # Adjust rect to make space for suptitle and legend\n",
        "\n",
        "    # Save or display the plot\n",
        "    if output_figure_path:\n",
        "        try:\n",
        "            plt.savefig(output_figure_path, dpi=300, bbox_inches='tight')\n",
        "            print(f\"Topic evolution plot saved to {output_figure_path}\")\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Could not save topic evolution plot to {output_figure_path}. Error: {e}\", UserWarning)\n",
        "            plt.show() # Fallback to displaying\n",
        "    else:\n",
        "        plt.show()\n",
        "    # Close the plot to free memory\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def display_llm_performance_summary(\n",
        "    evaluation_metrics: Dict[str, Any], # Output of Task 12\n",
        "    output_table_path: Optional[str] = None, # Path to save table (e.g., .md or .csv)\n",
        "    output_cm_plot_path: Optional[str] = None # Path to save confusion matrix plot\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Generates and displays/saves a summary of LLM performance metrics,\n",
        "    including a table and a confusion matrix plot.\n",
        "\n",
        "    Args:\n",
        "        evaluation_metrics: Dictionary from Task 12 containing accuracy,\n",
        "                            precision, recall, f1_score, confusion_matrix, etc.\n",
        "        output_table_path (Optional[str]): Path to save the metrics table.\n",
        "                                           If None, prints to console.\n",
        "                                           Supports .md and .csv extensions.\n",
        "        output_cm_plot_path (Optional[str]): Path to save the confusion matrix plot.\n",
        "                                             If None, displays the plot.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    required_metrics = [\"accuracy\", \"precision\", \"recall\", \"f1_score\", \"confusion_matrix\",\n",
        "                        \"num_mapped_for_evaluation\", \"true_positives\", \"false_positives\",\n",
        "                        \"true_negatives\", \"false_negatives\"]\n",
        "    if not isinstance(evaluation_metrics, dict) or \\\n",
        "       not all(metric in evaluation_metrics for metric in required_metrics):\n",
        "        raise ValueError(f\"evaluation_metrics' dict is missing one or more required keys: {required_metrics}\")\n",
        "\n",
        "    # --- Sub-step 14.b: Generate Metrics Table ---\n",
        "    # Create a pandas DataFrame for a nicely formatted table.\n",
        "    metrics_data = {\n",
        "        \"Metric\": [\"Number of Mapped Cases for Evaluation\", \"Accuracy\", \"Precision (for Narrative Shift)\",\n",
        "                   \"Recall (for Narrative Shift)\", \"F1-score (for Narrative Shift)\",\n",
        "                   \"True Positives (Narrative Shift)\", \"False Positives (Narrative Shift)\",\n",
        "                   \"True Negatives (Content Shift)\", \"False Negatives (Content Shift)\"],\n",
        "        \"Value\": [\n",
        "            evaluation_metrics[\"num_mapped_for_evaluation\"],\n",
        "            f\"{evaluation_metrics['accuracy']:.4f}\" if not np.isnan(evaluation_metrics['accuracy']) else \"N/A\",\n",
        "            f\"{evaluation_metrics['precision']:.4f}\" if not np.isnan(evaluation_metrics['precision']) else \"N/A\",\n",
        "            f\"{evaluation_metrics['recall']:.4f}\" if not np.isnan(evaluation_metrics['recall']) else \"N/A\",\n",
        "            f\"{evaluation_metrics['f1_score']:.4f}\" if not np.isnan(evaluation_metrics['f1_score']) else \"N/A\",\n",
        "            evaluation_metrics[\"true_positives\"],\n",
        "            evaluation_metrics[\"false_positives\"],\n",
        "            evaluation_metrics[\"true_negatives\"],\n",
        "            evaluation_metrics[\"false_negatives\"]\n",
        "        ]\n",
        "    }\n",
        "    metrics_df = pd.DataFrame(metrics_data)\n",
        "\n",
        "    # Display or save the table\n",
        "    if output_table_path:\n",
        "        try:\n",
        "            if output_table_path.endswith(\".csv\"):\n",
        "                metrics_df.to_csv(output_table_path, index=False)\n",
        "                print(f\"LLM performance metrics table saved to {output_table_path}\")\n",
        "            elif output_table_path.endswith(\".md\"):\n",
        "                metrics_df.to_markdown(output_table_path, index=False)\n",
        "                print(f\"LLM performance metrics table saved to {output_table_path}\")\n",
        "            else: # Default to CSV if extension is unknown/unsupported for direct save\n",
        "                output_table_path_csv = output_table_path + \".csv\"\n",
        "                metrics_df.to_csv(output_table_path_csv, index=False)\n",
        "                warnings.warn(f\"Unsupported table format for {output_table_path}. Saved as CSV: {output_table_path_csv}\", UserWarning)\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Could not save metrics table to {output_table_path}. Error: {e}. Printing to console.\", UserWarning)\n",
        "            print(\"\\nLLM Performance Metrics:\\n\", metrics_df.to_string(index=False))\n",
        "    else:\n",
        "        print(\"\\nLLM Performance Metrics:\\n\", metrics_df.to_string(index=False))\n",
        "\n",
        "    # --- Sub-step 14.b: Generate Confusion Matrix Plot ---\n",
        "    cm = evaluation_metrics.get(\"confusion_matrix\")\n",
        "    # Ensure cm is a 2x2 numpy array.\n",
        "    if isinstance(cm, np.ndarray) and cm.shape == (2,2) and not np.isnan(cm).any():\n",
        "        # Define display labels for the confusion matrix axes.\n",
        "        # Assuming class 0 = \"Content Shift\", class 1 = \"Narrative Shift\"\n",
        "        display_labels = [\"Content Shift (0)\", \"Narrative Shift (1)\"]\n",
        "\n",
        "        # Create the ConfusionMatrixDisplay object.\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
        "\n",
        "        # Plot the confusion matrix.\n",
        "        fig_cm, ax_cm = plt.subplots(figsize=(6, 5))\n",
        "        disp.plot(ax=ax_cm, cmap=plt.cm.Blues, values_format='d') # 'd' for integer format\n",
        "        ax_cm.set_title(\"LLM Classification Confusion Matrix\\n(True Label vs. Predicted Label)\", fontsize=10)\n",
        "        plt.xticks(rotation=0, ha=\"center\") # Adjust tick rotation if needed\n",
        "        plt.yticks(rotation=90, va=\"center\")\n",
        "        fig_cm.tight_layout()\n",
        "\n",
        "        # Save or display the plot\n",
        "        if output_cm_plot_path:\n",
        "            try:\n",
        "                plt.savefig(output_cm_plot_path, dpi=300, bbox_inches='tight')\n",
        "                print(f\"Confusion matrix plot saved to {output_cm_plot_path}\")\n",
        "            except Exception as e:\n",
        "                warnings.warn(f\"Could not save confusion matrix plot to {output_cm_plot_path}. Error: {e}\", UserWarning)\n",
        "                plt.show() # Fallback to displaying\n",
        "        else:\n",
        "            plt.show()\n",
        "        # Close the plot to free memory\n",
        "        plt.close(fig_cm)\n",
        "    elif cm is not None: # cm exists but is not valid for plotting\n",
        "         warnings.warn(f\"Confusion matrix data is invalid or not 2x2: {cm}. Cannot generate plot.\", UserWarning)\n",
        "    else: # cm is None\n",
        "         warnings.warn(\"Confusion matrix data not found in evaluation_metrics. Cannot generate plot.\", UserWarning)\n"
      ],
      "metadata": {
        "id": "HmsotXnaJMqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15: Documentation\n",
        "\n",
        "def generate_pipeline_run_documentation(\n",
        "    all_input_parameters: Dict[str, Dict[str, Any]], # Dict of all param dicts\n",
        "    # Specific configurations that might not be in top-level params but set in functions\n",
        "    spacy_model_name_used: Optional[str] = None,\n",
        "    lda_iterations_prototype: Optional[int] = None,\n",
        "    lda_alpha_prototype: Optional[Any] = None,\n",
        "    lda_eta_prototype: Optional[Any] = None,\n",
        "    rolling_lda_iterations_warmup: Optional[int] = None,\n",
        "    rolling_lda_iterations_update: Optional[int] = None,\n",
        "    llm_model_identifier_used: Optional[str] = None,\n",
        "    llm_quantization_config_used: Optional[Dict[str, Any]] = None,\n",
        "    # User-provided notes\n",
        "    run_notes_and_observations: Optional[List[str]] = None,\n",
        "    # Key outputs for reference in documentation (optional)\n",
        "    num_system_detected_changes: Optional[int] = None,\n",
        "    evaluation_results: Optional[Dict[str, Any]] = None,\n",
        "    output_format: str = \"json\" # \"json\" or \"markdown\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generates a structured documentation string for a pipeline run,\n",
        "    recording hyperparameters, model configurations, library versions,\n",
        "    and user notes.\n",
        "\n",
        "    Args:\n",
        "        all_input_parameters (Dict[str, Dict[str, Any]]):\n",
        "            A dictionary where keys are descriptive names (e.g.,\n",
        "            \"lda_prototype_params\", \"general_study_params\") and values are\n",
        "            the actual parameter dictionaries used for the run.\n",
        "        spacy_model_name_used (Optional[str]): Specific spaCy model used.\n",
        "        lda_iterations_prototype (Optional[int]): Iterations for LDAPrototype's LDA.\n",
        "        lda_alpha_prototype (Optional[Any]): Alpha for LDAPrototype's LDA.\n",
        "        lda_eta_prototype (Optional[Any]): Eta for LDAPrototype's LDA.\n",
        "        rolling_lda_iterations_warmup (Optional[int]): Iterations for RollingLDA warm-up.\n",
        "        rolling_lda_iterations_update (Optional[int]): Iterations for RollingLDA updates.\n",
        "        llm_model_identifier_used (Optional[str]): Specific LLM identifier used.\n",
        "        llm_quantization_config_used (Optional[Dict[str, Any]]): Quantization config for LLM.\n",
        "        run_notes_and_observations (Optional[List[str]]):\n",
        "            A list of strings representing user notes, challenges, or adjustments\n",
        "            made during this specific run.\n",
        "        num_system_detected_changes (Optional[int]): Number of changes detected by system.\n",
        "        evaluation_results (Optional[Dict[str, Any]]): Key evaluation metrics.\n",
        "        output_format (str): Desired output format: \"json\" or \"markdown\".\n",
        "\n",
        "    Returns:\n",
        "        str: A string containing the structured documentation in the specified format.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If `output_format` is not supported.\n",
        "    \"\"\"\n",
        "    # --- Timestamp and System Info ---\n",
        "    # Record the time of documentation generation.\n",
        "    generation_timestamp = datetime.datetime.now().isoformat()\n",
        "    # Get Python version.\n",
        "    python_version = sys.version # More detailed than platform.python_version()\n",
        "    # Get OS platform information.\n",
        "    os_platform = platform.platform()\n",
        "\n",
        "    # --- Collate Documentation Data ---\n",
        "    doc_data: Dict[str, Any] = {\n",
        "        \"run_documentation_generated_at\": generation_timestamp,\n",
        "        \"pipeline_execution_environment\": {\n",
        "            \"python_version\": python_version,\n",
        "            \"operating_system\": os_platform,\n",
        "        },\n",
        "        \"library_versions\": {\n",
        "            \"pandas\": PANDAS_VERSION,\n",
        "            \"numpy\": NUMPY_VERSION,\n",
        "            \"scikit-learn\": SKLEARN_VERSION,\n",
        "            \"gensim\": GENSIM_VERSION,\n",
        "            \"spacy\": SPACY_VERSION,\n",
        "            \"torch\": TORCH_VERSION,\n",
        "            \"transformers\": TRANSFORMERS_VERSION,\n",
        "            \"matplotlib\": MATPLOTLIB_VERSION\n",
        "        },\n",
        "        \"input_parameters\": all_input_parameters,\n",
        "        \"key_model_configurations_and_settings\": {},\n",
        "        \"run_specific_notes_and_observations\": run_notes_and_observations if run_notes_and_observations else \"None provided.\",\n",
        "        \"summary_of_key_outputs\": {}\n",
        "    }\n",
        "\n",
        "    # Populate key model configurations\n",
        "    # This section captures parameters that might be defaulted within functions if not in `all_input_parameters`\n",
        "    # or specific model names/versions used.\n",
        "    key_configs = doc_data[\"key_model_configurations_and_settings\"]\n",
        "    if spacy_model_name_used: key_configs[\"spacy_model_name\"] = spacy_model_name_used\n",
        "    if lda_iterations_prototype: key_configs[\"lda_prototype_iterations\"] = lda_iterations_prototype\n",
        "    if lda_alpha_prototype: key_configs[\"lda_prototype_alpha\"] = str(lda_alpha_prototype) # str for complex types\n",
        "    if lda_eta_prototype: key_configs[\"lda_prototype_eta\"] = str(lda_eta_prototype) # str for complex types\n",
        "    if rolling_lda_iterations_warmup: key_configs[\"rolling_lda_warmup_iterations\"] = rolling_lda_iterations_warmup\n",
        "    if rolling_lda_iterations_update: key_configs[\"rolling_lda_update_iterations\"] = rolling_lda_iterations_update\n",
        "    if llm_model_identifier_used: key_configs[\"llm_model_identifier\"] = llm_model_identifier_used\n",
        "    if llm_quantization_config_used: key_configs[\"llm_quantization_config\"] = llm_quantization_config_used\n",
        "\n",
        "    # Populate summary of key outputs\n",
        "    key_outputs = doc_data[\"summary_of_key_outputs\"]\n",
        "    if num_system_detected_changes is not None:\n",
        "        key_outputs[\"number_of_system_detected_changes\"] = num_system_detected_changes\n",
        "    if evaluation_results is not None:\n",
        "        # Convert numpy arrays in confusion matrix to lists for JSON serialization\n",
        "        if \"confusion_matrix\" in evaluation_results and isinstance(evaluation_results[\"confusion_matrix\"], np.ndarray):\n",
        "            eval_results_copy = evaluation_results.copy() # Avoid modifying original\n",
        "            eval_results_copy[\"confusion_matrix\"] = eval_results_copy[\"confusion_matrix\"].tolist()\n",
        "            key_outputs[\"llm_classification_evaluation\"] = eval_results_copy\n",
        "        else:\n",
        "            key_outputs[\"llm_classification_evaluation\"] = evaluation_results\n",
        "\n",
        "\n",
        "    # --- Format Output ---\n",
        "    # Based on the requested output_format.\n",
        "    if output_format.lower() == \"json\":\n",
        "        try:\n",
        "            # Serialize the documentation data dictionary to a JSON formatted string.\n",
        "            # `indent=4` for pretty printing.\n",
        "            return json.dumps(doc_data, indent=4, default=str) # default=str to handle non-serializable\n",
        "        except TypeError as e:\n",
        "            # Fallback if complex objects are not serializable even with default=str\n",
        "            warnings.warn(f\"Error serializing documentation to JSON: {e}. Some data might be lost.\", UserWarning)\n",
        "            # Attempt to serialize with problematic parts converted to string more aggressively\n",
        "            return json.dumps({k: str(v) if isinstance(v, (dict, list)) else v for k,v in doc_data.items()}, indent=4, default=str)\n",
        "\n",
        "\n",
        "    elif output_format.lower() == \"markdown\":\n",
        "        # Construct a Markdown formatted string.\n",
        "        md_parts: List[str] = []\n",
        "        # Add main title and timestamp.\n",
        "        md_parts.append(f\"# Pipeline Run Documentation\")\n",
        "        md_parts.append(f\"**Generated At:** {doc_data['run_documentation_generated_at']}\\n\")\n",
        "\n",
        "        # Add execution environment details.\n",
        "        md_parts.append(f\"## Execution Environment\")\n",
        "        md_parts.append(f\"- **Python Version:** {doc_data['pipeline_execution_environment']['python_version']}\")\n",
        "        md_parts.append(f\"- **Operating System:** {doc_data['pipeline_execution_environment']['operating_system']}\\n\")\n",
        "\n",
        "        # Add library versions.\n",
        "        md_parts.append(f\"## Library Versions\")\n",
        "        for lib, ver in doc_data['library_versions'].items():\n",
        "            md_parts.append(f\"- **{lib.capitalize()}:** {ver}\")\n",
        "        md_parts.append(\"\\n\")\n",
        "\n",
        "        # Add input parameters.\n",
        "        md_parts.append(f\"## Input Parameters\")\n",
        "        for param_group_name, params in doc_data['input_parameters'].items():\n",
        "            md_parts.append(f\"### {param_group_name.replace('_', ' ').title()}\")\n",
        "            if isinstance(params, dict):\n",
        "                for key, value in params.items():\n",
        "                    md_parts.append(f\"- **{key}:** {value}\")\n",
        "            else: # Should be a dict, but handle if not\n",
        "                 md_parts.append(f\"- {params}\") # Print as is\n",
        "            md_parts.append(\"\") # Add a newline for spacing\n",
        "        md_parts.append(\"\\n\")\n",
        "\n",
        "        # Add key model configurations.\n",
        "        md_parts.append(f\"## Key Model Configurations & Settings\")\n",
        "        if doc_data['key_model_configurations_and_settings']:\n",
        "            for key, value in doc_data['key_model_configurations_and_settings'].items():\n",
        "                md_parts.append(f\"- **{key.replace('_', ' ').title()}:** {value}\")\n",
        "        else:\n",
        "            md_parts.append(\"No specific model configurations provided beyond input parameters.\")\n",
        "        md_parts.append(\"\\n\")\n",
        "\n",
        "        # Add run-specific notes.\n",
        "        md_parts.append(f\"## Run-Specific Notes and Observations\")\n",
        "        if isinstance(doc_data['run_specific_notes_and_observations'], list):\n",
        "            if doc_data['run_specific_notes_and_observations']:\n",
        "                for note in doc_data['run_specific_notes_and_observations']:\n",
        "                    md_parts.append(f\"- {note}\")\n",
        "            else:\n",
        "                md_parts.append(\"None provided.\")\n",
        "        else: # Should be a list or None, but handle if it's a string\n",
        "            md_parts.append(str(doc_data['run_specific_notes_and_observations']))\n",
        "        md_parts.append(\"\\n\")\n",
        "\n",
        "        # Add summary of key outputs.\n",
        "        md_parts.append(f\"## Summary of Key Outputs\")\n",
        "        if doc_data['summary_of_key_outputs']:\n",
        "            num_changes = doc_data['summary_of_key_outputs'].get('number_of_system_detected_changes')\n",
        "            if num_changes is not None:\n",
        "                md_parts.append(f\"- **Number of System-Detected Changes:** {num_changes}\")\n",
        "\n",
        "            eval_res = doc_data['summary_of_key_outputs'].get('llm_classification_evaluation')\n",
        "            if eval_res and isinstance(eval_res, dict):\n",
        "                md_parts.append(f\"### LLM Classification Evaluation:\")\n",
        "                for metric, value in eval_res.items():\n",
        "                    if metric == \"confusion_matrix\" and isinstance(value, list): # Already converted to list for JSON\n",
        "                        md_parts.append(f\"- **Confusion Matrix (TN, FP, FN, TP):** \"\n",
        "                                        f\"[[{value[0][0]}, {value[0][1]}], [{value[1][0]}, {value[1][1]}]]\")\n",
        "                    else:\n",
        "                        md_parts.append(f\"- **{metric.replace('_', ' ').title()}:** {value}\")\n",
        "        else:\n",
        "            md_parts.append(\"No key outputs summarized.\")\n",
        "        md_parts.append(\"\\n\")\n",
        "\n",
        "        # Join all Markdown parts into a single string.\n",
        "        return \"\\n\".join(md_parts)\n",
        "    else:\n",
        "        # Raise ValueError for unsupported output format.\n",
        "        raise ValueError(f\"Unsupported 'output_format': {output_format}. Choose 'json' or 'markdown'.\")\n"
      ],
      "metadata": {
        "id": "Pn9ikOQnLxoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline\n",
        "\n",
        "def run_narrative_shift_detection_pipeline(\n",
        "    # Parameters (i) to (vii) from the main problem description\n",
        "    news_article_data_frame_input: pd.DataFrame,\n",
        "    lda_prototype_params_input: Dict[str, Any],\n",
        "    rolling_lda_params_input: Dict[str, Any],\n",
        "    topical_changes_params_input: Dict[str, Any],\n",
        "    llm_interpretation_params_input: Dict[str, Any],\n",
        "    general_study_params_input: Dict[str, Any],\n",
        "    human_annotations_input_data: Dict[str, Dict[str, Any]],\n",
        "\n",
        "    # Detailed configuration parameters for individual pipeline steps\n",
        "    spacy_model_name_cfg: str = \"en_core_web_sm\",\n",
        "    custom_stopwords_cfg: Optional[List[str]] = None,\n",
        "    countvectorizer_min_df_cfg: int = 5,\n",
        "    countvectorizer_max_df_cfg: float = 0.95,\n",
        "\n",
        "    lda_iterations_prototype_cfg: int = 1000,\n",
        "    lda_alpha_prototype_cfg: str = 'symmetric',\n",
        "    lda_eta_prototype_cfg: Optional[Any] = None, # Gensim default (symmetric based on num_topics if None)\n",
        "    lda_passes_prototype_cfg: int = 10, # Added for completeness for train_lda_prototype\n",
        "\n",
        "    rolling_lda_iterations_warmup_cfg: int = 50,\n",
        "    rolling_lda_iterations_update_cfg: int = 20,\n",
        "    rolling_lda_passes_warmup_cfg: int = 10,\n",
        "    rolling_lda_passes_update_cfg: int = 1,\n",
        "    rolling_lda_alpha_cfg: str = 'symmetric',\n",
        "    rolling_lda_epsilon_eta_cfg: float = 1e-9,\n",
        "\n",
        "    tc_num_tokens_bootstrap_cfg: int = 10000,\n",
        "    tc_num_significant_loo_cfg: int = 10,\n",
        "    tc_epsilon_cfg: float = 1e-9,\n",
        "\n",
        "    llm_quantization_cfg: Optional[Dict[str, Any]] = None,\n",
        "    llm_auth_token_cfg: Optional[str] = None,\n",
        "    llm_trust_remote_code_cfg: bool = True, # Often needed for newer models\n",
        "    llm_use_cache_cfg: bool = True, # For setup_llm_model_and_tokenizer cache\n",
        "\n",
        "    llm_max_new_tokens_cfg: int = 3072,\n",
        "\n",
        "    eval_topic_matching_threshold_cfg: float = 0.1,\n",
        "    analysis_num_top_words_display_cfg: int = 5,\n",
        "    analysis_mapping_num_top_words_cfg: int = 10, # For compile_analysis_results mapping helper\n",
        "\n",
        "    viz_plots_per_row_cfg: int = 5,\n",
        "    viz_figure_title_cfg: str = \"Topic Evolution and Detected Narrative Shifts\",\n",
        "\n",
        "    output_directory_cfg: Optional[str] = None,\n",
        "\n",
        "    doc_run_notes_cfg: Optional[List[str]] = None,\n",
        "    doc_output_format_cfg: str = \"json\"\n",
        "\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire end-to-end narrative shift detection pipeline,\n",
        "    integrating all defined tasks from data validation to documentation.\n",
        "\n",
        "    This function manages the flow of data between modular components,\n",
        "    handles configuration, and implements saving of large artifacts to disk\n",
        "    if an output directory is specified.\n",
        "\n",
        "    Args:\n",
        "        news_article_data_frame_input: Raw news articles DataFrame (param i).\n",
        "        lda_prototype_params_input: Params for LDAPrototype (param ii).\n",
        "        rolling_lda_params_input: Params for RollingLDA (param iii).\n",
        "        topical_changes_params_input: Params for Topical Changes (param iv).\n",
        "        llm_interpretation_params_input: Params for LLM interpretation (param v).\n",
        "        general_study_params_input: General study parameters (param vi).\n",
        "        human_annotations_input_data: Pre-existing human annotations (param vii).\n",
        "        spacy_model_name_cfg: Name of spaCy model for preprocessing.\n",
        "        custom_stopwords_cfg: Custom stopwords for preprocessing.\n",
        "        countvectorizer_min_df_cfg: Min document frequency for CountVectorizer.\n",
        "        countvectorizer_max_df_cfg: Max document frequency for CountVectorizer.\n",
        "        lda_iterations_prototype_cfg: Iterations for LDA in LDAPrototype.\n",
        "        lda_alpha_prototype_cfg: Alpha for LDA in LDAPrototype.\n",
        "        lda_eta_prototype_cfg: Eta for LDA in LDAPrototype.\n",
        "        lda_passes_prototype_cfg: Passes for LDA in LDAPrototype.\n",
        "        rolling_lda_iterations_warmup_cfg: Iterations for RollingLDA warm-up.\n",
        "        rolling_lda_iterations_update_cfg: Iterations for RollingLDA updates.\n",
        "        rolling_lda_passes_warmup_cfg: Passes for RollingLDA warm-up.\n",
        "        rolling_lda_passes_update_cfg: Passes for RollingLDA updates.\n",
        "        rolling_lda_alpha_cfg: Alpha for RollingLDA.\n",
        "        rolling_lda_epsilon_eta_cfg: Epsilon for RollingLDA eta.\n",
        "        tc_num_tokens_bootstrap_cfg: N tokens for Topical Changes bootstrap.\n",
        "        tc_num_significant_loo_cfg: N LOO words for Topical Changes.\n",
        "        tc_epsilon_cfg: Epsilon for Topical Changes numerical stability.\n",
        "        llm_quantization_cfg: Quantization config for LLM setup.\n",
        "        llm_auth_token_cfg: Auth token for LLM setup.\n",
        "        llm_trust_remote_code_cfg: Trust remote code for LLM.\n",
        "        llm_use_cache_cfg: Whether to use internal cache in LLM setup.\n",
        "        llm_max_new_tokens_cfg: Max new tokens for LLM generation.\n",
        "        eval_topic_matching_threshold_cfg: Threshold for mapping system to human topics.\n",
        "        analysis_num_top_words_display_cfg: Num top words for topic display in analysis DF.\n",
        "        analysis_mapping_num_top_words_cfg: Num top words for topic matching in analysis mapping.\n",
        "        viz_plots_per_row_cfg: Plots per row in topic evolution visualization.\n",
        "        viz_figure_title_cfg: Title for the topic evolution figure.\n",
        "        output_directory_cfg (Optional[str]): Base directory to save all generated\n",
        "                                             artifacts. If None, artifacts are not saved.\n",
        "        doc_run_notes_cfg (Optional[List[str]]): User notes for the documentation.\n",
        "        doc_output_format_cfg (str): Format for run documentation ('json' or 'markdown').\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive dictionary containing key outputs from each major\n",
        "                        step of the pipeline. If `output_directory_cfg` is provided,\n",
        "                        this dictionary will contain paths to saved artifacts.\n",
        "    \"\"\"\n",
        "    # --- Record Pipeline Start Time ---\n",
        "    pipeline_start_time = datetime.datetime.now()\n",
        "    # Log the start of the pipeline execution.\n",
        "    print(f\"Narrative Shift Detection Pipeline started at: {pipeline_start_time.isoformat()}\")\n",
        "\n",
        "    # --- Initialize Main Output Dictionary (Task 16 Structure) ---\n",
        "    # This dictionary will store key results and paths to artifacts.\n",
        "    pipeline_outputs: Dict[str, Any] = {}\n",
        "\n",
        "    # --- Prepare Output Directories if Specified ---\n",
        "    # Initialize paths dictionary; values will be None if output_directory_cfg is None.\n",
        "    paths: Dict[str, Optional[str]] = {\n",
        "        \"base\": output_directory_cfg,\n",
        "        \"data_artifacts\": None, \"model_artifacts\": None, \"results_artifacts\": None,\n",
        "        \"visualizations\": None, \"documentation\": None\n",
        "    }\n",
        "    # If an output directory is specified, create it and its subdirectories.\n",
        "    if output_directory_cfg:\n",
        "        # Create the base output directory.\n",
        "        os.makedirs(output_directory_cfg, exist_ok=True)\n",
        "        # Define and create subdirectories for organized artifact storage.\n",
        "        paths[\"data_artifacts\"] = os.path.join(output_directory_cfg, \"data_artifacts\")\n",
        "        paths[\"model_artifacts\"] = os.path.join(output_directory_cfg, \"model_artifacts\")\n",
        "        paths[\"results_artifacts\"] = os.path.join(output_directory_cfg, \"results_artifacts\") # For tables, compiled DFs, JSON results\n",
        "        paths[\"visualizations\"] = os.path.join(output_directory_cfg, \"visualizations\")\n",
        "        paths[\"documentation\"] = os.path.join(output_directory_cfg, \"documentation\")\n",
        "        # Create each subdirectory.\n",
        "        for path_key, path_val in paths.items():\n",
        "            if path_key != \"base\" and path_val is not None: # 'base' is already created or is None\n",
        "                os.makedirs(path_val, exist_ok=True)\n",
        "\n",
        "    # --- Store All Input Parameters and Configurations for Documentation ---\n",
        "    # Create a comprehensive dictionary of all parameters passed to the orchestrator.\n",
        "    # This will be part of the final returned dictionary and used by Task 15.\n",
        "    all_orchestrator_input_parameters = {\n",
        "        \"input_data_summary\": { # Summaries of large data inputs\n",
        "            \"news_article_data_frame_input_shape\": str(news_article_data_frame_input.shape),\n",
        "            \"human_annotations_input_num_entries\": len(human_annotations_input_data),\n",
        "        },\n",
        "        \"lda_prototype_params_input\": lda_prototype_params_input.copy(),\n",
        "        \"rolling_lda_params_input\": rolling_lda_params_input.copy(),\n",
        "        \"topical_changes_params_input\": topical_changes_params_input.copy(),\n",
        "        \"llm_interpretation_params_input\": llm_interpretation_params_input.copy(),\n",
        "        \"general_study_params_input\": general_study_params_input.copy(),\n",
        "        \"pipeline_step_configurations\": { # Detailed configurations for each step\n",
        "            \"spacy_model_name\": spacy_model_name_cfg,\n",
        "            \"custom_stopwords_present\": bool(custom_stopwords_cfg),\n",
        "            \"countvectorizer_min_df\": countvectorizer_min_df_cfg,\n",
        "            \"countvectorizer_max_df\": countvectorizer_max_df_cfg,\n",
        "            \"lda_prototype_iterations\": lda_iterations_prototype_cfg,\n",
        "            \"lda_prototype_alpha\": str(lda_alpha_prototype_cfg),\n",
        "            \"lda_prototype_eta\": str(lda_eta_prototype_cfg),\n",
        "            \"lda_prototype_passes\": lda_passes_prototype_cfg,\n",
        "            \"rolling_lda_warmup_iterations\": rolling_lda_iterations_warmup_cfg,\n",
        "            \"rolling_lda_update_iterations\": rolling_lda_iterations_update_cfg,\n",
        "            \"rolling_lda_warmup_passes\": rolling_lda_passes_warmup_cfg,\n",
        "            \"rolling_lda_update_passes\": rolling_lda_passes_update_cfg,\n",
        "            \"rolling_lda_alpha\": str(rolling_lda_alpha_cfg),\n",
        "            \"rolling_lda_epsilon_eta\": rolling_lda_epsilon_eta_cfg,\n",
        "            \"topical_changes_bootstrap_tokens\": tc_num_tokens_bootstrap_cfg,\n",
        "            \"topical_changes_num_loo_words\": tc_num_significant_loo_cfg,\n",
        "            \"topical_changes_epsilon\": tc_epsilon_cfg,\n",
        "            \"llm_quantization_config_used\": llm_quantization_cfg, # Store the dict itself\n",
        "            \"llm_auth_token_provided\": bool(llm_auth_token_cfg),\n",
        "            \"llm_trust_remote_code\": llm_trust_remote_code_cfg,\n",
        "            \"llm_use_setup_cache\": llm_use_cache_cfg,\n",
        "            \"llm_generation_max_new_tokens\": llm_max_new_tokens_cfg,\n",
        "            \"evaluation_topic_matching_threshold\": eval_topic_matching_threshold_cfg,\n",
        "            \"analysis_topic_display_num_top_words\": analysis_num_top_words_display_cfg,\n",
        "            \"analysis_mapping_num_top_words\": analysis_mapping_num_top_words_cfg,\n",
        "            \"visualization_plots_per_row\": viz_plots_per_row_cfg,\n",
        "            \"visualization_figure_title\": viz_figure_title_cfg,\n",
        "            \"documentation_run_notes_provided\": bool(doc_run_notes_cfg),\n",
        "            \"documentation_output_format\": doc_output_format_cfg\n",
        "        },\n",
        "        \"output_directory_configuration\": output_directory_cfg\n",
        "    }\n",
        "    # Add these collected parameters to the main output dictionary.\n",
        "    pipeline_outputs[\"parameters_and_configurations\"] = all_orchestrator_input_parameters\n",
        "\n",
        "    # --- Main Pipeline Execution with Error Handling ---\n",
        "    try:\n",
        "        # --- Task 0: Parameter Validation ---\n",
        "        print(\"\\n--- Starting Task 0: Parameter Validation ---\")\n",
        "        # Call the validation function (assumed defined elsewhere).\n",
        "        validate_input_parameters(\n",
        "            news_article_data_frame=news_article_data_frame_input,\n",
        "            lda_prototype_params=lda_prototype_params_input,\n",
        "            rolling_lda_params=rolling_lda_params_input,\n",
        "            topical_changes_params=topical_changes_params_input,\n",
        "            llm_interpretation_params=llm_interpretation_params_input,\n",
        "            general_study_params=general_study_params_input,\n",
        "            human_detected_change_points=human_annotations_input_data\n",
        "        )\n",
        "        # Log successful validation.\n",
        "        print(\"Task 0: Parameter Validation COMPLETED successfully.\")\n",
        "        pipeline_outputs[\"parameter_validation_status\"] = \"Passed\"\n",
        "\n",
        "        # --- Task 1: Data Cleansing ---\n",
        "        print(\"\\n--- Starting Task 1: Data Cleansing ---\")\n",
        "        # Call the cleansing function (assumed defined elsewhere).\n",
        "        cleansed_df = cleanse_news_data(news_article_data_frame_input)\n",
        "        # Log completion and store info in outputs.\n",
        "        print(f\"Task 1: Data Cleansing COMPLETED. Shape after cleansing: {cleansed_df.shape}\")\n",
        "        pipeline_outputs[\"cleansed_data_shape\"] = str(cleansed_df.shape)\n",
        "        pipeline_outputs[\"num_rows_after_cleansing\"] = len(cleansed_df)\n",
        "\n",
        "        # --- Task 2: Data Preprocessing ---\n",
        "        print(\"\\n--- Starting Task 2: Data Preprocessing ---\")\n",
        "        # Call the preprocessing function (assumed defined elsewhere).\n",
        "        processed_df_obj, count_vectorizer_obj, vocabulary_list_obj = preprocess_text_data(\n",
        "            cleansed_df=cleansed_df,\n",
        "            general_study_params=general_study_params_input,\n",
        "            rolling_lda_params=rolling_lda_params_input,\n",
        "            spacy_model_name=spacy_model_name_cfg,\n",
        "            countvectorizer_min_df=countvectorizer_min_df_cfg,\n",
        "            countvectorizer_max_df=countvectorizer_max_df_cfg,\n",
        "            custom_stopwords=custom_stopwords_cfg\n",
        "        )\n",
        "        # Extract sklearn feature names for use in later BoW conversions.\n",
        "        sklearn_feature_names_list_obj = count_vectorizer_obj.get_feature_names_out().tolist()\n",
        "        # Log completion and store info/paths in outputs.\n",
        "        print(f\"Task 2: Data Preprocessing COMPLETED. Vocab size: {len(vocabulary_list_obj)}. BoW column added to DataFrame.\")\n",
        "        pipeline_outputs[\"vocabulary_size\"] = len(vocabulary_list_obj)\n",
        "        if paths[\"data_artifacts\"]: # If output directory is specified for data artifacts\n",
        "            processed_df_path = os.path.join(paths[\"data_artifacts\"], \"processed_dataframe.pkl\")\n",
        "            processed_df_obj.to_pickle(processed_df_path)\n",
        "            pipeline_outputs[\"processed_dataframe_path\"] = processed_df_path\n",
        "\n",
        "            cv_path = os.path.join(paths[\"model_artifacts\"], \"count_vectorizer.pkl\") # Save vectorizer as a model\n",
        "            with open(cv_path, \"wb\") as f_cv: pickle.dump(count_vectorizer_obj, f_cv)\n",
        "            pipeline_outputs[\"count_vectorizer_path\"] = cv_path\n",
        "\n",
        "            vocab_path = os.path.join(paths[\"data_artifacts\"], \"vocabulary_list.json\")\n",
        "            with open(vocab_path, \"w\", encoding=\"utf-8\") as f_vocab: json.dump(vocabulary_list_obj, f_vocab, indent=4)\n",
        "            pipeline_outputs[\"vocabulary_list_path\"] = vocab_path\n",
        "        else: # Store summaries if not saving full objects\n",
        "            pipeline_outputs[\"processed_data_sample_head_json\"] = processed_df_obj.head().to_json(orient=\"records\", lines=False, date_format=\"iso\")\n",
        "\n",
        "\n",
        "        # --- Task 3: Time Chunking ---\n",
        "        print(\"\\n--- Starting Task 3: Time Chunking ---\")\n",
        "        # Call the time chunking function (assumed defined elsewhere).\n",
        "        chunked_corpus_sklearn_bow, ordered_chunk_keys = chunk_data_by_time(\n",
        "            processed_df=processed_df_obj,\n",
        "            general_study_params=general_study_params_input,\n",
        "            min_articles_per_chunk=10 # Example, make configurable via _cfg if needed\n",
        "        )\n",
        "        # Log completion and store info/paths in outputs.\n",
        "        print(f\"Task 3: Time Chunking COMPLETED. Number of chunks: {len(ordered_chunk_keys)}.\")\n",
        "        pipeline_outputs[\"num_time_chunks\"] = len(ordered_chunk_keys)\n",
        "        pipeline_outputs[\"ordered_chunk_keys_sample\"] = ordered_chunk_keys[:min(5, len(ordered_chunk_keys))] + \\\n",
        "                                                       ([\"...\"] if len(ordered_chunk_keys) > 5 else [])\n",
        "        if paths[\"data_artifacts\"]:\n",
        "            chunked_bow_path = os.path.join(paths[\"data_artifacts\"], \"chunked_sklearn_bow.pkl\")\n",
        "            with open(chunked_bow_path, \"wb\") as f_cb: pickle.dump(chunked_corpus_sklearn_bow, f_cb)\n",
        "            pipeline_outputs[\"chunked_sklearn_bow_path\"] = chunked_bow_path\n",
        "\n",
        "\n",
        "        # --- Task 4: LDAPrototype Implementation ---\n",
        "        print(\"\\n--- Starting Task 4: LDAPrototype Implementation ---\")\n",
        "        # Call the LDAPrototype training function (assumed defined elsewhere).\n",
        "        lda_prototype_model_obj = train_lda_prototype(\n",
        "            processed_df=processed_df_obj,\n",
        "            count_vectorizer=count_vectorizer_obj,\n",
        "            general_study_params=general_study_params_input,\n",
        "            rolling_lda_params=rolling_lda_params_input,\n",
        "            lda_prototype_params=lda_prototype_params_input,\n",
        "            lda_iterations=lda_iterations_prototype_cfg,\n",
        "            lda_alpha=lda_alpha_prototype_cfg,\n",
        "            lda_eta=lda_eta_prototype_cfg,\n",
        "            lda_passes=lda_passes_prototype_cfg # Added passes to signature\n",
        "        )\n",
        "        # Explicitly capture the GensimDictionary from the prototype model.\n",
        "        gensim_dictionary_global_obj: GensimDictionary = lda_prototype_model_obj.id2word\n",
        "        # Log completion and store info/paths in outputs.\n",
        "        print(\"Task 4: LDAPrototype Implementation COMPLETED. Prototype model selected.\")\n",
        "        if paths[\"model_artifacts\"]:\n",
        "            proto_model_path = os.path.join(paths[\"model_artifacts\"], \"lda_prototype_model.gensim\")\n",
        "            lda_prototype_model_obj.save(proto_model_path) # Use Gensim's save method\n",
        "            pipeline_outputs[\"lda_prototype_model_path\"] = proto_model_path\n",
        "\n",
        "            gensim_dict_path = os.path.join(paths[\"model_artifacts\"], \"gensim_dictionary.gensimdict\")\n",
        "            gensim_dictionary_global_obj.save(gensim_dict_path) # Use Gensim's save method\n",
        "            pipeline_outputs[\"gensim_dictionary_path\"] = gensim_dict_path\n",
        "        else: # Store summary if not saving model\n",
        "            prototype_summary_topics = {}\n",
        "            for i in range(min(5, lda_prototype_params_input[\"K_topics\"])):\n",
        "                prototype_summary_topics[f\"Topic_{i}\"] = [word for word, prob in lda_prototype_model_obj.show_topic(i, topn=5)]\n",
        "            pipeline_outputs[\"lda_prototype_model_sample_topics\"] = prototype_summary_topics\n",
        "\n",
        "\n",
        "        # --- Task 5: RollingLDA Implementation ---\n",
        "        print(\"\\n--- Starting Task 5: RollingLDA Implementation ---\")\n",
        "        # Call the RollingLDA application function (assumed defined elsewhere).\n",
        "        time_series_topic_word_dist_obj = apply_rolling_lda(\n",
        "            lda_prototype_model=lda_prototype_model_obj,\n",
        "            chunked_corpus_sklearn_bow=chunked_corpus_sklearn_bow,\n",
        "            ordered_chunk_keys=ordered_chunk_keys,\n",
        "            gensim_dictionary=gensim_dictionary_global_obj,\n",
        "            sklearn_feature_names=sklearn_feature_names_list_obj,\n",
        "            rolling_lda_params=rolling_lda_params_input,\n",
        "            lda_iterations_warmup=rolling_lda_iterations_warmup_cfg,\n",
        "            lda_iterations_update=rolling_lda_iterations_update_cfg,\n",
        "            lda_passes_warmup=rolling_lda_passes_warmup_cfg,\n",
        "            lda_passes_update=rolling_lda_passes_update_cfg,\n",
        "            lda_alpha_rolling=rolling_lda_alpha_cfg,\n",
        "            epsilon_eta=rolling_lda_epsilon_eta_cfg\n",
        "        )\n",
        "        # Log completion and store info/paths in outputs.\n",
        "        print(f\"Task 5: RollingLDA Implementation COMPLETED. Time series of {len(time_series_topic_word_dist_obj)} topic distributions generated.\")\n",
        "        if paths[\"results_artifacts\"]: # Save to results artifacts\n",
        "            ts_topic_dist_path = os.path.join(paths[\"results_artifacts\"], \"time_series_topic_word_dist.pkl\")\n",
        "            with open(ts_topic_dist_path, \"wb\") as f_ts: pickle.dump(time_series_topic_word_dist_obj, f_ts)\n",
        "            pipeline_outputs[\"time_series_topic_word_dist_path\"] = ts_topic_dist_path\n",
        "        pipeline_outputs[\"time_series_topic_word_dist_num_entries\"] = len(time_series_topic_word_dist_obj)\n",
        "\n",
        "\n",
        "        # --- Task 6: Topical Changes Implementation ---\n",
        "        print(\"\\n--- Starting Task 6: Topical Changes Implementation ---\")\n",
        "        # Call the topical changes detection function (assumed defined elsewhere).\n",
        "        detected_change_points_with_loo, visualization_data_for_changes = detect_topical_changes(\n",
        "            time_series_topic_word_dist=time_series_topic_word_dist_obj,\n",
        "            ordered_chunk_keys=ordered_chunk_keys,\n",
        "            gensim_dictionary=gensim_dictionary_global_obj,\n",
        "            topical_changes_params=topical_changes_params_input,\n",
        "            k_topics=rolling_lda_params_input[\"K_topics\"],\n",
        "            num_tokens_for_bootstrap_resampling_per_topic=tc_num_tokens_bootstrap_cfg,\n",
        "            num_significant_words_loo=tc_num_significant_loo_cfg,\n",
        "            epsilon=tc_epsilon_cfg\n",
        "        )\n",
        "        num_detected_changes = len(detected_change_points_with_loo)\n",
        "        # Log completion and store info/paths in outputs.\n",
        "        print(f\"Task 6: Topical Changes Implementation COMPLETED. Found {num_detected_changes} potential changes.\")\n",
        "        pipeline_outputs[\"num_system_detected_changes\"] = num_detected_changes\n",
        "        pipeline_outputs[\"system_detected_change_points_sample\"] = [\n",
        "            {\"chunk\": cp[0], \"topic_id\": cp[1], \"num_loo_words\": len(cp[2]),\n",
        "             \"d_obs\": round(cp[3], 4) if isinstance(cp[3], float) else cp[3], # Handle potential NaN\n",
        "             \"d_crit\": round(cp[4], 4) if isinstance(cp[4], float) else cp[4]} # Handle potential NaN\n",
        "            for cp in detected_change_points_with_loo[:min(5, num_detected_changes)]\n",
        "        ]\n",
        "        if paths[\"results_artifacts\"]:\n",
        "            changes_path = os.path.join(paths[\"results_artifacts\"], \"system_detected_changes_with_loo.json\")\n",
        "            # Convert tuples to dicts for better JSON readability\n",
        "            changes_to_save = [\n",
        "                {\"chunk_key\":c[0], \"topic_id\":c[1], \"loo_words\":c[2], \"d_obs\":c[3], \"d_crit\":c[4]}\n",
        "                for c in detected_change_points_with_loo\n",
        "            ]\n",
        "            with open(changes_path, \"w\", encoding=\"utf-8\") as f_chg: json.dump(changes_to_save, f_chg, indent=2)\n",
        "            pipeline_outputs[\"system_detected_changes_with_loo_path\"] = changes_path\n",
        "\n",
        "            viz_data_path = os.path.join(paths[\"results_artifacts\"], \"visualization_data_for_changes.json\")\n",
        "            viz_data_to_save = [\n",
        "                 {\"chunk_key\":v[0], \"topic_id\":v[1], \"d_obs\":v[2], \"d_crit\":v[3]}\n",
        "                 for v in visualization_data_for_changes\n",
        "            ]\n",
        "            with open(viz_data_path, \"w\", encoding=\"utf-8\") as f_viz: json.dump(viz_data_to_save, f_viz, indent=2)\n",
        "            pipeline_outputs[\"visualization_data_for_changes_path\"] = viz_data_path\n",
        "\n",
        "\n",
        "        # --- LLM Processing Stage (Tasks 7, 8, 9, 10) ---\n",
        "        llm_parsed_responses_map: Dict[Tuple[str, int], Optional[Dict[str, Any]]] = {}\n",
        "        # Only proceed if there are changes to analyze.\n",
        "        if num_detected_changes > 0:\n",
        "            print(\"\\n--- Starting LLM Processing Stage (Tasks 7-10) ---\")\n",
        "            # Task 8: LLM Setup (once)\n",
        "            llm_model, llm_tokenizer = setup_llm_model_and_tokenizer(\n",
        "                llm_model_identifier=llm_interpretation_params_input[\"llm_model_name\"],\n",
        "                quantization_config=llm_quantization_cfg,\n",
        "                auth_token=llm_auth_token_cfg,\n",
        "                trust_remote_code=llm_trust_remote_code_cfg,\n",
        "                use_cache=llm_use_cache_cfg\n",
        "            )\n",
        "            print(\"LLM Model and Tokenizer loaded for analysis.\")\n",
        "\n",
        "            num_articles_for_llm = llm_interpretation_params_input[\"N_docs_filter\"]\n",
        "\n",
        "            # Loop through each detected change point for LLM analysis.\n",
        "            for idx, change_point_detail_tuple in enumerate(detected_change_points_with_loo):\n",
        "                current_chunk_key, current_topic_id, current_loo_words, _, _ = change_point_detail_tuple\n",
        "                print(f\"  LLM Processing change {idx+1}/{num_detected_changes}: Chunk {current_chunk_key}, Topic {current_topic_id}\")\n",
        "\n",
        "                # Task 7: Document Filtering\n",
        "                filtered_texts_for_llm = filter_documents_for_llm(\n",
        "                    detected_change_point_info=change_point_detail_tuple,\n",
        "                    processed_df=processed_df_obj,\n",
        "                    llm_interpretation_params=llm_interpretation_params_input,\n",
        "                    text_column_for_counting=\"full_text_lemmatized_list\"\n",
        "                )\n",
        "\n",
        "                if not filtered_texts_for_llm:\n",
        "                    warnings.warn(f\"No documents filtered for LLM analysis of change ({current_chunk_key}, Topic {current_topic_id}). Skipping LLM.\", UserWarning)\n",
        "                    llm_parsed_responses_map[(current_chunk_key, current_topic_id)] = None\n",
        "                    continue\n",
        "\n",
        "                # Task 9: LLM Prompt Engineering\n",
        "                idx_current_chunk_in_ordered_keys = ordered_chunk_keys.index(current_chunk_key)\n",
        "                top_words_before_list: List[str] = []\n",
        "                if idx_current_chunk_in_ordered_keys > 0:\n",
        "                    key_before_change = ordered_chunk_keys[idx_current_chunk_in_ordered_keys - 1]\n",
        "                    phi_matrix_before = time_series_topic_word_dist_obj.get(key_before_change)\n",
        "                    if phi_matrix_before is not None:\n",
        "                        top_words_before_list = _get_top_n_words_for_topic(\n",
        "                            phi_matrix_before, current_topic_id, gensim_dictionary_global_obj, top_n=10\n",
        "                        )\n",
        "\n",
        "                phi_matrix_after = time_series_topic_word_dist_obj.get(current_chunk_key)\n",
        "                top_words_after_list: List[str] = []\n",
        "                if phi_matrix_after is not None:\n",
        "                    top_words_after_list = _get_top_n_words_for_topic(\n",
        "                        phi_matrix_after, current_topic_id, gensim_dictionary_global_obj, top_n=10\n",
        "                    )\n",
        "\n",
        "                full_llm_prompt = construct_llm_prompt_for_narrative_analysis(\n",
        "                    date_of_change=current_chunk_key,\n",
        "                    top_words_before_change=top_words_before_list,\n",
        "                    top_words_after_change=top_words_after_list,\n",
        "                    significant_loo_words=current_loo_words,\n",
        "                    filtered_article_texts=filtered_texts_for_llm\n",
        "                )\n",
        "\n",
        "                # Task 10: LLM Analysis\n",
        "                parsed_json, raw_llm_output = perform_llm_analysis_on_change_point(\n",
        "                    full_prompt_string=full_llm_prompt,\n",
        "                    llm_model=llm_model,\n",
        "                    llm_tokenizer=llm_tokenizer,\n",
        "                    llm_interpretation_params=llm_interpretation_params_input,\n",
        "                    max_new_tokens_generation=llm_max_new_tokens_cfg,\n",
        "                    num_articles_in_prompt=len(filtered_texts_for_llm)\n",
        "                )\n",
        "                llm_parsed_responses_map[(current_chunk_key, current_topic_id)] = parsed_json\n",
        "                # Optionally save raw_llm_output, e.g., if paths[\"results_artifacts\"] is set\n",
        "                if paths[\"results_artifacts\"]:\n",
        "                    raw_output_filename = f\"llm_raw_output_{current_chunk_key}_T{current_topic_id}.txt\"\n",
        "                    with open(os.path.join(paths[\"results_artifacts\"], raw_output_filename), \"w\", encoding=\"utf-8\") as f_raw:\n",
        "                        f_raw.write(raw_llm_output)\n",
        "            print(f\"LLM Processing Stage COMPLETED for {len(llm_parsed_responses_map)} successfully processed changes.\")\n",
        "        else: # No system-detected changes\n",
        "            print(\"No system-detected changes found. Skipping LLM Processing, Evaluation, and related parts of Analysis/Visualization.\")\n",
        "\n",
        "        # Store LLM responses (path or summary)\n",
        "        if paths[\"results_artifacts\"] and llm_parsed_responses_map:\n",
        "            llm_resp_path = os.path.join(paths[\"results_artifacts\"], \"llm_parsed_responses.json\")\n",
        "            llm_resp_serializable = {f\"{k[0]}_T{k[1]}\": v for k,v in llm_parsed_responses_map.items()} # Convert tuple keys\n",
        "            with open(llm_resp_path, \"w\", encoding=\"utf-8\") as f_llm_resp: json.dump(llm_resp_serializable, f_llm_resp, indent=2, default=str)\n",
        "            pipeline_outputs[\"llm_parsed_responses_path\"] = llm_resp_path\n",
        "        else: # Store summary if not saving full object\n",
        "             pipeline_outputs[\"llm_parsed_responses_summary\"] = {\n",
        "                f\"{k[0]}_T{k[1]}\": (v is not None)\n",
        "                for k,v in list(llm_parsed_responses_map.items())[:min(5, len(llm_parsed_responses_map))]\n",
        "            }\n",
        "\n",
        "\n",
        "        # --- Task 11: Human Annotation (Data is input, validated in Task 0) ---\n",
        "        print(\"\\n--- Task 11: Human Annotation data acknowledged as input ---\")\n",
        "        pipeline_outputs[\"human_annotations_input_num_entries\"] = len(human_annotations_input_data)\n",
        "\n",
        "\n",
        "        # --- Task 12: Evaluation ---\n",
        "        evaluation_results_dict: Optional[Dict[str, Any]] = None\n",
        "        # Only run if LLM analyses were performed and there are system changes.\n",
        "        if llm_parsed_responses_map and detected_change_points_with_loo:\n",
        "            print(\"\\n--- Starting Task 12: Evaluation ---\")\n",
        "            evaluation_results_dict = evaluate_llm_classification_performance(\n",
        "                system_detected_change_points=detected_change_points_with_loo,\n",
        "                llm_parsed_responses=llm_parsed_responses_map,\n",
        "                human_annotations_input=human_annotations_input_data,\n",
        "                time_series_topic_word_dist=time_series_topic_word_dist_obj,\n",
        "                gensim_dictionary=gensim_dictionary_global_obj,\n",
        "                topic_matching_threshold_for_mapping=eval_topic_matching_threshold_cfg\n",
        "            )\n",
        "            print(f\"Task 12: Evaluation COMPLETED. Mapped cases: {evaluation_results_dict.get('num_mapped_for_evaluation', 0)}. \"\n",
        "                  f\"Accuracy: {evaluation_results_dict.get('accuracy', 'N/A')}\")\n",
        "            pipeline_outputs[\"evaluation_metrics\"] = evaluation_results_dict\n",
        "            if paths[\"results_artifacts\"]:\n",
        "                eval_metrics_path = os.path.join(paths[\"results_artifacts\"], \"evaluation_metrics.json\")\n",
        "                # Convert numpy arrays/objects in metrics to lists/native types for JSON\n",
        "                serializable_eval_metrics = evaluation_results_dict.copy()\n",
        "                if \"confusion_matrix\" in serializable_eval_metrics and isinstance(serializable_eval_metrics[\"confusion_matrix\"], np.ndarray):\n",
        "                    serializable_eval_metrics[\"confusion_matrix\"] = serializable_eval_metrics[\"confusion_matrix\"].tolist()\n",
        "                with open(eval_metrics_path, \"w\", encoding=\"utf-8\") as f_eval: json.dump(serializable_eval_metrics, f_eval, indent=4)\n",
        "                pipeline_outputs[\"evaluation_metrics_path\"] = eval_metrics_path\n",
        "        else:\n",
        "            print(\"Skipping Task 12: Evaluation as no LLM analyses were performed or no system changes detected.\")\n",
        "            pipeline_outputs[\"evaluation_metrics\"] = {\"status\": \"Skipped - No LLM analyses or no system changes.\"}\n",
        "\n",
        "\n",
        "        # --- Task 13: Result Analysis (Compile DataFrame) ---\n",
        "        print(\"\\n--- Starting Task 13: Result Analysis (Compile DataFrame) ---\")\n",
        "        compiled_analysis_df_obj = compile_analysis_results(\n",
        "             system_detected_change_points_with_loo=detected_change_points_with_loo,\n",
        "             llm_parsed_responses=llm_parsed_responses_map,\n",
        "             human_annotations_input=human_annotations_input_data,\n",
        "             time_series_topic_word_dist=time_series_topic_word_dist_obj,\n",
        "             gensim_dictionary=gensim_dictionary_global_obj,\n",
        "             k_topics=rolling_lda_params_input[\"K_topics\"],\n",
        "             topic_matching_threshold_for_mapping=eval_topic_matching_threshold_cfg,\n",
        "             num_top_words_for_topic_display=analysis_num_top_words_display_cfg,\n",
        "             num_top_words_for_mapping_match=analysis_mapping_num_top_words_cfg\n",
        "        )\n",
        "        print(f\"Task 13: Result Analysis COMPLETED. Compiled DataFrame shape: {compiled_analysis_df_obj.shape}\")\n",
        "        if paths[\"results_artifacts\"]:\n",
        "            analysis_df_path = os.path.join(paths[\"results_artifacts\"], \"compiled_analysis_results.csv\")\n",
        "            compiled_analysis_df_obj.to_csv(analysis_df_path, index=False, encoding=\"utf-8\")\n",
        "            pipeline_outputs[\"compiled_analysis_dataframe_path\"] = analysis_df_path\n",
        "        else:\n",
        "            pipeline_outputs[\"compiled_analysis_dataframe_shape\"] = str(compiled_analysis_df_obj.shape)\n",
        "\n",
        "\n",
        "        # --- Task 14: Visualization ---\n",
        "        print(\"\\n--- Starting Task 14: Visualization ---\")\n",
        "        topic_evo_plot_path = os.path.join(paths[\"visualizations\"], \"topic_evolution_plots.png\") if paths[\"visualizations\"] else None\n",
        "        metrics_table_path_md = os.path.join(paths[\"results_artifacts\"], \"llm_performance_metrics.md\") if paths[\"results_artifacts\"] else None\n",
        "        cm_plot_path = os.path.join(paths[\"visualizations\"], \"llm_confusion_matrix.png\") if paths[\"visualizations\"] else None\n",
        "\n",
        "        plot_topic_evolution_and_changes(\n",
        "            visualization_data=visualization_data_for_changes,\n",
        "            detected_change_points=detected_change_points_with_loo,\n",
        "            ordered_chunk_keys=ordered_chunk_keys,\n",
        "            k_topics=rolling_lda_params_input[\"K_topics\"],\n",
        "            time_series_topic_word_dist=time_series_topic_word_dist_obj,\n",
        "            gensim_dictionary=gensim_dictionary_global_obj,\n",
        "            output_figure_path=topic_evo_plot_path,\n",
        "            plots_per_row=viz_plots_per_row_cfg,\n",
        "            figure_title=viz_figure_title_cfg\n",
        "        )\n",
        "        if evaluation_results_dict and evaluation_results_dict.get(\"num_mapped_for_evaluation\", 0) > 0 :\n",
        "            display_llm_performance_summary(\n",
        "                evaluation_metrics=evaluation_results_dict,\n",
        "                output_table_path=metrics_table_path_md,\n",
        "                output_cm_plot_path=cm_plot_path\n",
        "            )\n",
        "        else:\n",
        "            print(\"Skipping LLM performance summary visualization as evaluation was not performed or had no mapped cases.\")\n",
        "        print(\"Task 14: Visualization COMPLETED.\")\n",
        "        if paths[\"visualizations\"] and topic_evo_plot_path and os.path.exists(topic_evo_plot_path): pipeline_outputs[\"topic_evolution_plot_path\"] = topic_evo_plot_path\n",
        "        if paths[\"results_artifacts\"] and metrics_table_path_md and os.path.exists(metrics_table_path_md) : pipeline_outputs[\"metrics_table_path\"] = metrics_table_path_md\n",
        "        if paths[\"visualizations\"] and cm_plot_path and os.path.exists(cm_plot_path) : pipeline_outputs[\"confusion_matrix_plot_path\"] = cm_plot_path\n",
        "\n",
        "\n",
        "        # --- Task 15: Documentation ---\n",
        "        print(\"\\n--- Starting Task 15: Documentation ---\")\n",
        "        run_documentation_str = generate_pipeline_run_documentation(\n",
        "            all_input_parameters=all_orchestrator_input_parameters,\n",
        "            spacy_model_name_used=spacy_model_name_cfg,\n",
        "            lda_iterations_prototype=lda_iterations_prototype_cfg,\n",
        "            lda_alpha_prototype=lda_alpha_prototype_cfg, # Pass actual value\n",
        "            lda_eta_prototype=lda_eta_prototype_cfg,     # Pass actual value\n",
        "            rolling_lda_iterations_warmup=rolling_lda_iterations_warmup_cfg,\n",
        "            rolling_lda_iterations_update=rolling_lda_iterations_update_cfg,\n",
        "            llm_model_identifier_used=llm_interpretation_params_input[\"llm_model_name\"],\n",
        "            llm_quantization_config_used=llm_quantization_cfg,\n",
        "            run_notes_and_observations=doc_run_notes_cfg,\n",
        "            num_system_detected_changes=num_detected_changes,\n",
        "            evaluation_results=evaluation_results_dict,\n",
        "            output_format=doc_output_format_cfg\n",
        "        )\n",
        "\n",
        "        doc_filename = f\"pipeline_run_documentation.{doc_output_format_cfg.lower()}\"\n",
        "        doc_path = os.path.join(paths[\"documentation\"], doc_filename) if paths[\"documentation\"] else doc_filename\n",
        "        try:\n",
        "            with open(doc_path, \"w\", encoding=\"utf-8\") as f_doc:\n",
        "                f_doc.write(run_documentation_str)\n",
        "            print(f\"Task 15: Documentation COMPLETED. Saved to {doc_path}\")\n",
        "            pipeline_outputs[\"documentation_file_path\"] = doc_path\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Failed to save documentation to {doc_path}. Error: {e}\", UserWarning)\n",
        "            pipeline_outputs[\"documentation_content_str\"] = run_documentation_str\n",
        "\n",
        "    except Exception as pipeline_error:\n",
        "        # Catch any unhandled error from the tasks.\n",
        "        print(f\"\\nPIPELINE EXECUTION FAILED: {pipeline_error}\")\n",
        "        # Populate error information in the output dictionary.\n",
        "        pipeline_outputs[\"pipeline_status\"] = \"Failed\"\n",
        "        pipeline_outputs[\"error_message\"] = str(pipeline_error)\n",
        "        import traceback\n",
        "        pipeline_outputs[\"error_traceback\"] = traceback.format_exc()\n",
        "        # Do not re-raise; allow function to return the pipeline_outputs dict with error info.\n",
        "    else:\n",
        "        # If no exceptions throughout the try block, mark pipeline as successful.\n",
        "        pipeline_outputs[\"pipeline_status\"] = \"Completed Successfully\"\n",
        "\n",
        "    # --- End of Pipeline ---\n",
        "    # Record end time and calculate duration.\n",
        "    pipeline_end_time = datetime.datetime.now()\n",
        "    pipeline_duration = pipeline_end_time - pipeline_start_time\n",
        "    # Log pipeline completion and duration.\n",
        "    print(f\"\\nNarrative Shift Detection Pipeline finished at: {pipeline_end_time.isoformat()}\")\n",
        "    print(f\"Total pipeline duration: {pipeline_duration}\")\n",
        "    # Store duration and end time in outputs.\n",
        "    pipeline_outputs[\"pipeline_duration_seconds\"] = pipeline_duration.total_seconds()\n",
        "    pipeline_outputs[\"pipeline_end_timestamp_iso\"] = pipeline_end_time.isoformat()\n",
        "\n",
        "    # Return the comprehensive dictionary of outputs (Task 16).\n",
        "    return pipeline_outputs\n"
      ],
      "metadata": {
        "id": "N8aUkwX_gogD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}